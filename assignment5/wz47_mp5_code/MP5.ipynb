{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: pyvirtualdisplay in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.18.1)\n",
      "[sudo] password for wxzhang: \n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /home/wxzhang/.local/lib/python3.8/site-packages (69.5.1)\n",
      "Requirement already satisfied: ez_setup in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[atari]) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[atari]) (7.1.0)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: typing-extensions in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from ale-py~=0.8.0->gym[atari]) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.18.1)\n",
      "Requirement already satisfied: gym[accept-rom-license] in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[accept-rom-license]) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[accept-rom-license]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[accept-rom-license]) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from gym[accept-rom-license]) (7.1.0)\n",
      "Requirement already satisfied: autorom~=0.4.2 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.66.2)\n",
      "Requirement already satisfied: importlib-resources in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (6.4.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (3.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.0   memory length: 123   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.0\n",
      "episode: 1   score: 2.0   memory length: 320   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 2   score: 3.0   memory length: 545   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 6.0   memory length: 910   epsilon: 1.0    steps: 365    lr: 0.0001     evaluation reward: 2.75\n",
      "episode: 4   score: 0.0   memory length: 1032   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 5   score: 1.0   memory length: 1202   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 6   score: 0.0   memory length: 1325   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 7   score: 1.0   memory length: 1493   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 8   score: 1.0   memory length: 1644   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 9   score: 3.0   memory length: 1875   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 10   score: 2.0   memory length: 2076   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.7272727272727273\n",
      "episode: 11   score: 3.0   memory length: 2302   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 12   score: 0.0   memory length: 2425   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
      "episode: 13   score: 1.0   memory length: 2594   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 14   score: 2.0   memory length: 2813   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 15   score: 2.0   memory length: 2995   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.6875\n",
      "episode: 16   score: 0.0   memory length: 3118   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 17   score: 1.0   memory length: 3287   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 18   score: 0.0   memory length: 3410   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4736842105263157\n",
      "episode: 19   score: 2.0   memory length: 3607   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 20   score: 3.0   memory length: 3852   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 21   score: 0.0   memory length: 3975   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 22   score: 0.0   memory length: 4098   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.434782608695652\n",
      "episode: 23   score: 1.0   memory length: 4268   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 24   score: 3.0   memory length: 4515   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 25   score: 1.0   memory length: 4684   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 26   score: 3.0   memory length: 4915   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.5185185185185186\n",
      "episode: 27   score: 0.0   memory length: 5038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4642857142857142\n",
      "episode: 28   score: 1.0   memory length: 5189   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4482758620689655\n",
      "episode: 29   score: 2.0   memory length: 5387   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4666666666666666\n",
      "episode: 30   score: 2.0   memory length: 5584   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4838709677419355\n",
      "episode: 31   score: 0.0   memory length: 5706   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4375\n",
      "episode: 32   score: 0.0   memory length: 5829   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.393939393939394\n",
      "episode: 33   score: 0.0   memory length: 5952   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3529411764705883\n",
      "episode: 34   score: 0.0   memory length: 6075   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3142857142857143\n",
      "episode: 35   score: 2.0   memory length: 6273   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 36   score: 0.0   memory length: 6396   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2972972972972974\n",
      "episode: 37   score: 2.0   memory length: 6593   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3157894736842106\n",
      "episode: 38   score: 2.0   memory length: 6791   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 39   score: 3.0   memory length: 7037   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 40   score: 2.0   memory length: 7220   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.3902439024390243\n",
      "episode: 41   score: 3.0   memory length: 7446   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 42   score: 1.0   memory length: 7615   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4186046511627908\n",
      "episode: 43   score: 3.0   memory length: 7861   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 44   score: 1.0   memory length: 8033   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 45   score: 4.0   memory length: 8309   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 46   score: 0.0   memory length: 8432   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4680851063829787\n",
      "episode: 47   score: 3.0   memory length: 8679   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 48   score: 2.0   memory length: 8877   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.510204081632653\n",
      "episode: 49   score: 2.0   memory length: 9098   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 50   score: 1.0   memory length: 9270   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5098039215686274\n",
      "episode: 51   score: 2.0   memory length: 9467   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5192307692307692\n",
      "episode: 52   score: 1.0   memory length: 9639   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.509433962264151\n",
      "episode: 53   score: 3.0   memory length: 9850   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.537037037037037\n",
      "episode: 54   score: 0.0   memory length: 9973   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.509090909090909\n",
      "episode: 55   score: 2.0   memory length: 10152   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.5178571428571428\n",
      "episode: 56   score: 3.0   memory length: 10399   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.543859649122807\n",
      "episode: 57   score: 3.0   memory length: 10662   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.5689655172413792\n",
      "episode: 58   score: 7.0   memory length: 11048   epsilon: 1.0    steps: 386    lr: 0.0001     evaluation reward: 1.6610169491525424\n",
      "episode: 59   score: 2.0   memory length: 11246   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 60   score: 1.0   memory length: 11416   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6557377049180328\n",
      "episode: 61   score: 1.0   memory length: 11567   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6451612903225807\n",
      "episode: 62   score: 0.0   memory length: 11690   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 63   score: 3.0   memory length: 11938   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.640625\n",
      "episode: 64   score: 2.0   memory length: 12156   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.646153846153846\n",
      "episode: 65   score: 1.0   memory length: 12327   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 66   score: 1.0   memory length: 12498   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.626865671641791\n",
      "episode: 67   score: 2.0   memory length: 12718   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6323529411764706\n",
      "episode: 68   score: 6.0   memory length: 13108   epsilon: 1.0    steps: 390    lr: 0.0001     evaluation reward: 1.6956521739130435\n",
      "episode: 69   score: 0.0   memory length: 13231   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6714285714285715\n",
      "episode: 70   score: 0.0   memory length: 13353   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.647887323943662\n",
      "episode: 71   score: 1.0   memory length: 13523   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6388888888888888\n",
      "episode: 72   score: 2.0   memory length: 13720   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.643835616438356\n",
      "episode: 73   score: 4.0   memory length: 13999   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.6756756756756757\n",
      "episode: 74   score: 2.0   memory length: 14198   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 75   score: 2.0   memory length: 14417   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
      "episode: 76   score: 0.0   memory length: 14540   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6623376623376624\n",
      "episode: 77   score: 1.0   memory length: 14709   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6538461538461537\n",
      "episode: 78   score: 1.0   memory length: 14860   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6455696202531647\n",
      "episode: 79   score: 1.0   memory length: 15030   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6375\n",
      "episode: 80   score: 0.0   memory length: 15152   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.617283950617284\n",
      "episode: 81   score: 3.0   memory length: 15378   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.6341463414634145\n",
      "episode: 82   score: 1.0   memory length: 15547   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6265060240963856\n",
      "episode: 83   score: 2.0   memory length: 15728   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.630952380952381\n",
      "episode: 84   score: 2.0   memory length: 15928   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6352941176470588\n",
      "episode: 85   score: 3.0   memory length: 16157   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.6511627906976745\n",
      "episode: 86   score: 0.0   memory length: 16280   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.632183908045977\n",
      "episode: 87   score: 2.0   memory length: 16478   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 88   score: 0.0   memory length: 16600   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6179775280898876\n",
      "episode: 89   score: 2.0   memory length: 16800   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6222222222222222\n",
      "episode: 90   score: 1.0   memory length: 16971   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 91   score: 1.0   memory length: 17141   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 92   score: 3.0   memory length: 17390   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.6236559139784945\n",
      "episode: 93   score: 1.0   memory length: 17541   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6170212765957446\n",
      "episode: 94   score: 3.0   memory length: 17767   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.631578947368421\n",
      "episode: 95   score: 3.0   memory length: 18019   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.6458333333333333\n",
      "episode: 96   score: 3.0   memory length: 18265   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6597938144329898\n",
      "episode: 97   score: 3.0   memory length: 18480   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.6734693877551021\n",
      "episode: 98   score: 0.0   memory length: 18603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6565656565656566\n",
      "episode: 99   score: 0.0   memory length: 18726   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 100   score: 0.0   memory length: 18849   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 101   score: 0.0   memory length: 18971   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 102   score: 0.0   memory length: 19093   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 103   score: 0.0   memory length: 19215   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 104   score: 4.0   memory length: 19491   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 105   score: 12.0   memory length: 20001   epsilon: 1.0    steps: 510    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 106   score: 2.0   memory length: 20200   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 107   score: 2.0   memory length: 20419   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 108   score: 2.0   memory length: 20636   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 109   score: 1.0   memory length: 20805   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 110   score: 0.0   memory length: 20927   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 111   score: 3.0   memory length: 21175   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 112   score: 2.0   memory length: 21373   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 113   score: 2.0   memory length: 21591   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 114   score: 4.0   memory length: 21907   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 115   score: 0.0   memory length: 22029   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 116   score: 1.0   memory length: 22199   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 117   score: 2.0   memory length: 22397   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 118   score: 1.0   memory length: 22565   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 119   score: 1.0   memory length: 22736   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 120   score: 1.0   memory length: 22904   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 121   score: 0.0   memory length: 23027   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 122   score: 5.0   memory length: 23344   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 123   score: 4.0   memory length: 23661   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 124   score: 3.0   memory length: 23926   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 125   score: 3.0   memory length: 24172   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 126   score: 2.0   memory length: 24389   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 127   score: 2.0   memory length: 24587   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 128   score: 3.0   memory length: 24812   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 129   score: 2.0   memory length: 25009   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 130   score: 1.0   memory length: 25178   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 131   score: 2.0   memory length: 25375   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 132   score: 3.0   memory length: 25622   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 133   score: 1.0   memory length: 25791   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 134   score: 3.0   memory length: 26039   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 135   score: 0.0   memory length: 26162   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 136   score: 1.0   memory length: 26330   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 137   score: 5.0   memory length: 26623   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 138   score: 1.0   memory length: 26792   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 139   score: 1.0   memory length: 26942   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 140   score: 0.0   memory length: 27064   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 141   score: 1.0   memory length: 27214   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 142   score: 4.0   memory length: 27488   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 143   score: 0.0   memory length: 27611   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 144   score: 1.0   memory length: 27762   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 145   score: 1.0   memory length: 27933   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 146   score: 3.0   memory length: 28185   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 147   score: 3.0   memory length: 28411   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 148   score: 2.0   memory length: 28596   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 149   score: 0.0   memory length: 28719   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 150   score: 1.0   memory length: 28870   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 151   score: 2.0   memory length: 29068   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 152   score: 3.0   memory length: 29317   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 153   score: 2.0   memory length: 29515   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 154   score: 2.0   memory length: 29713   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 155   score: 2.0   memory length: 29897   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 156   score: 0.0   memory length: 30019   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 157   score: 3.0   memory length: 30266   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 158   score: 0.0   memory length: 30389   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 159   score: 0.0   memory length: 30511   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 160   score: 2.0   memory length: 30729   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 161   score: 3.0   memory length: 30975   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 162   score: 3.0   memory length: 31222   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 163   score: 3.0   memory length: 31467   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 164   score: 1.0   memory length: 31618   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 165   score: 3.0   memory length: 31863   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 166   score: 2.0   memory length: 32083   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 167   score: 3.0   memory length: 32330   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 168   score: 2.0   memory length: 32531   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 169   score: 4.0   memory length: 32808   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 170   score: 2.0   memory length: 33026   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 171   score: 1.0   memory length: 33177   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 172   score: 1.0   memory length: 33347   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 173   score: 4.0   memory length: 33641   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 174   score: 0.0   memory length: 33764   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 175   score: 2.0   memory length: 33961   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 176   score: 3.0   memory length: 34225   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 177   score: 3.0   memory length: 34436   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 178   score: 1.0   memory length: 34586   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 179   score: 3.0   memory length: 34829   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 180   score: 3.0   memory length: 35057   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 181   score: 0.0   memory length: 35180   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 182   score: 0.0   memory length: 35303   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 183   score: 3.0   memory length: 35530   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 184   score: 1.0   memory length: 35699   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 185   score: 5.0   memory length: 36025   epsilon: 1.0    steps: 326    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 186   score: 1.0   memory length: 36176   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 187   score: 0.0   memory length: 36298   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 188   score: 0.0   memory length: 36421   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 189   score: 3.0   memory length: 36647   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 190   score: 2.0   memory length: 36845   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 191   score: 1.0   memory length: 37013   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 192   score: 1.0   memory length: 37184   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 193   score: 0.0   memory length: 37306   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 194   score: 2.0   memory length: 37525   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 195   score: 2.0   memory length: 37743   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 196   score: 1.0   memory length: 37911   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 197   score: 1.0   memory length: 38062   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 198   score: 0.0   memory length: 38185   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 199   score: 2.0   memory length: 38382   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 200   score: 2.0   memory length: 38600   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 201   score: 0.0   memory length: 38723   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 202   score: 2.0   memory length: 38923   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 203   score: 1.0   memory length: 39075   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 204   score: 0.0   memory length: 39198   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 205   score: 2.0   memory length: 39416   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 206   score: 4.0   memory length: 39712   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 207   score: 0.0   memory length: 39834   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 208   score: 2.0   memory length: 40049   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 209   score: 1.0   memory length: 40218   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 210   score: 0.0   memory length: 40341   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 211   score: 0.0   memory length: 40463   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 212   score: 2.0   memory length: 40663   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 213   score: 4.0   memory length: 40980   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 214   score: 1.0   memory length: 41131   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 215   score: 0.0   memory length: 41254   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 216   score: 1.0   memory length: 41406   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 217   score: 0.0   memory length: 41529   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 218   score: 0.0   memory length: 41651   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 219   score: 0.0   memory length: 41773   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 220   score: 3.0   memory length: 42016   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 221   score: 0.0   memory length: 42139   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 222   score: 2.0   memory length: 42357   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 223   score: 1.0   memory length: 42526   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 224   score: 1.0   memory length: 42696   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 225   score: 6.0   memory length: 43082   epsilon: 1.0    steps: 386    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 226   score: 0.0   memory length: 43204   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 227   score: 0.0   memory length: 43326   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 228   score: 2.0   memory length: 43524   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 229   score: 0.0   memory length: 43646   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 230   score: 2.0   memory length: 43861   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 231   score: 1.0   memory length: 44011   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 232   score: 1.0   memory length: 44180   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 233   score: 4.0   memory length: 44457   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 234   score: 1.0   memory length: 44628   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 235   score: 2.0   memory length: 44844   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 236   score: 2.0   memory length: 45059   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 237   score: 2.0   memory length: 45277   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 238   score: 0.0   memory length: 45399   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 239   score: 3.0   memory length: 45646   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 240   score: 7.0   memory length: 46060   epsilon: 1.0    steps: 414    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 241   score: 2.0   memory length: 46278   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 242   score: 2.0   memory length: 46476   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 243   score: 3.0   memory length: 46741   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 244   score: 2.0   memory length: 46942   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 245   score: 1.0   memory length: 47114   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 246   score: 4.0   memory length: 47405   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 247   score: 1.0   memory length: 47573   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 248   score: 0.0   memory length: 47696   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 249   score: 3.0   memory length: 47943   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 250   score: 0.0   memory length: 48065   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 251   score: 1.0   memory length: 48236   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 252   score: 1.0   memory length: 48406   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 253   score: 1.0   memory length: 48575   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 254   score: 1.0   memory length: 48745   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 255   score: 1.0   memory length: 48917   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 256   score: 1.0   memory length: 49085   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 257   score: 1.0   memory length: 49236   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 258   score: 3.0   memory length: 49462   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 259   score: 1.0   memory length: 49632   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 260   score: 1.0   memory length: 49801   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 261   score: 0.0   memory length: 49923   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 262   score: 0.0   memory length: 50045   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 263   score: 3.0   memory length: 50289   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 264   score: 2.0   memory length: 50486   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 265   score: 2.0   memory length: 50705   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 266   score: 1.0   memory length: 50876   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 267   score: 0.0   memory length: 50999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 268   score: 1.0   memory length: 51149   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 269   score: 1.0   memory length: 51300   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 270   score: 4.0   memory length: 51576   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 271   score: 0.0   memory length: 51699   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 272   score: 2.0   memory length: 51918   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 273   score: 2.0   memory length: 52116   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 274   score: 2.0   memory length: 52334   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 275   score: 0.0   memory length: 52456   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 276   score: 2.0   memory length: 52674   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 277   score: 0.0   memory length: 52797   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 278   score: 1.0   memory length: 52968   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 279   score: 2.0   memory length: 53166   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 280   score: 4.0   memory length: 53459   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 281   score: 1.0   memory length: 53610   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 282   score: 0.0   memory length: 53732   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 283   score: 0.0   memory length: 53854   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 284   score: 4.0   memory length: 54109   epsilon: 1.0    steps: 255    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 285   score: 2.0   memory length: 54306   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 286   score: 2.0   memory length: 54504   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 287   score: 2.0   memory length: 54702   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 288   score: 3.0   memory length: 54948   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 289   score: 0.0   memory length: 55071   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 290   score: 1.0   memory length: 55240   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 291   score: 1.0   memory length: 55409   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 292   score: 4.0   memory length: 55705   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 293   score: 3.0   memory length: 55968   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 294   score: 2.0   memory length: 56187   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 295   score: 2.0   memory length: 56385   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 296   score: 3.0   memory length: 56632   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 297   score: 2.0   memory length: 56850   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 298   score: 5.0   memory length: 57176   epsilon: 1.0    steps: 326    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 299   score: 3.0   memory length: 57421   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 300   score: 0.0   memory length: 57543   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 301   score: 1.0   memory length: 57712   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 302   score: 1.0   memory length: 57881   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 303   score: 0.0   memory length: 58003   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 304   score: 1.0   memory length: 58154   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 305   score: 2.0   memory length: 58352   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 306   score: 1.0   memory length: 58505   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 307   score: 1.0   memory length: 58656   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 308   score: 2.0   memory length: 58874   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 309   score: 0.0   memory length: 58997   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 310   score: 0.0   memory length: 59120   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 311   score: 3.0   memory length: 59367   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 312   score: 0.0   memory length: 59490   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 313   score: 1.0   memory length: 59640   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 314   score: 4.0   memory length: 59937   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 315   score: 1.0   memory length: 60087   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 316   score: 0.0   memory length: 60210   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 317   score: 0.0   memory length: 60333   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 318   score: 3.0   memory length: 60580   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 319   score: 5.0   memory length: 60892   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 320   score: 3.0   memory length: 61137   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 321   score: 2.0   memory length: 61335   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 322   score: 2.0   memory length: 61533   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 323   score: 3.0   memory length: 61778   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 324   score: 1.0   memory length: 61929   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 325   score: 2.0   memory length: 62126   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 326   score: 0.0   memory length: 62249   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 327   score: 1.0   memory length: 62421   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 328   score: 3.0   memory length: 62649   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 329   score: 2.0   memory length: 62847   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 330   score: 0.0   memory length: 62969   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 331   score: 2.0   memory length: 63150   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 332   score: 0.0   memory length: 63273   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 333   score: 1.0   memory length: 63442   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 334   score: 3.0   memory length: 63689   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 335   score: 6.0   memory length: 64063   epsilon: 1.0    steps: 374    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 336   score: 1.0   memory length: 64213   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 337   score: 0.0   memory length: 64335   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 338   score: 1.0   memory length: 64503   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 339   score: 2.0   memory length: 64700   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 340   score: 0.0   memory length: 64823   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 341   score: 0.0   memory length: 64946   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 342   score: 3.0   memory length: 65215   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 343   score: 2.0   memory length: 65416   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 344   score: 0.0   memory length: 65539   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 345   score: 2.0   memory length: 65737   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 346   score: 1.0   memory length: 65906   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 347   score: 1.0   memory length: 66056   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 348   score: 0.0   memory length: 66179   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 349   score: 2.0   memory length: 66381   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 350   score: 1.0   memory length: 66550   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 351   score: 0.0   memory length: 66672   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 352   score: 2.0   memory length: 66872   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 353   score: 1.0   memory length: 67022   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 354   score: 0.0   memory length: 67145   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 355   score: 1.0   memory length: 67313   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 356   score: 0.0   memory length: 67436   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 357   score: 0.0   memory length: 67559   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 358   score: 0.0   memory length: 67682   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 359   score: 0.0   memory length: 67804   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 360   score: 0.0   memory length: 67926   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 361   score: 3.0   memory length: 68174   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 362   score: 0.0   memory length: 68297   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 363   score: 1.0   memory length: 68448   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 364   score: 3.0   memory length: 68674   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 365   score: 0.0   memory length: 68797   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 366   score: 2.0   memory length: 69013   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 367   score: 7.0   memory length: 69431   epsilon: 1.0    steps: 418    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 368   score: 0.0   memory length: 69553   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 369   score: 0.0   memory length: 69676   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 370   score: 0.0   memory length: 69799   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 371   score: 0.0   memory length: 69922   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 372   score: 3.0   memory length: 70188   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 373   score: 3.0   memory length: 70436   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 374   score: 0.0   memory length: 70558   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 375   score: 1.0   memory length: 70708   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 376   score: 2.0   memory length: 70905   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 377   score: 1.0   memory length: 71056   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 378   score: 0.0   memory length: 71179   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 379   score: 2.0   memory length: 71402   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 380   score: 1.0   memory length: 71571   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 381   score: 2.0   memory length: 71789   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 382   score: 6.0   memory length: 72162   epsilon: 1.0    steps: 373    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 383   score: 2.0   memory length: 72360   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 384   score: 0.0   memory length: 72483   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 385   score: 4.0   memory length: 72778   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 386   score: 3.0   memory length: 73048   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 387   score: 4.0   memory length: 73365   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 388   score: 4.0   memory length: 73641   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 389   score: 3.0   memory length: 73872   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 390   score: 0.0   memory length: 73995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 391   score: 1.0   memory length: 74165   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 392   score: 1.0   memory length: 74334   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 393   score: 2.0   memory length: 74554   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 394   score: 0.0   memory length: 74677   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 395   score: 3.0   memory length: 74905   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 396   score: 3.0   memory length: 75172   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 397   score: 2.0   memory length: 75370   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 398   score: 0.0   memory length: 75492   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 399   score: 0.0   memory length: 75615   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 400   score: 1.0   memory length: 75783   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 401   score: 1.0   memory length: 75936   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 402   score: 1.0   memory length: 76104   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 403   score: 1.0   memory length: 76273   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 404   score: 4.0   memory length: 76566   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 405   score: 2.0   memory length: 76783   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 406   score: 0.0   memory length: 76905   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 407   score: 0.0   memory length: 77028   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 408   score: 3.0   memory length: 77253   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 409   score: 2.0   memory length: 77450   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 410   score: 0.0   memory length: 77573   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 411   score: 2.0   memory length: 77770   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 412   score: 1.0   memory length: 77921   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 413   score: 0.0   memory length: 78044   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 414   score: 3.0   memory length: 78292   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 415   score: 3.0   memory length: 78538   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 416   score: 5.0   memory length: 78862   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 417   score: 0.0   memory length: 78984   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 418   score: 2.0   memory length: 79181   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 419   score: 2.0   memory length: 79398   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 420   score: 0.0   memory length: 79520   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 421   score: 2.0   memory length: 79717   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 422   score: 1.0   memory length: 79889   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 423   score: 1.0   memory length: 80061   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 424   score: 0.0   memory length: 80183   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 425   score: 0.0   memory length: 80306   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 426   score: 0.0   memory length: 80429   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 427   score: 3.0   memory length: 80678   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 428   score: 1.0   memory length: 80828   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 429   score: 0.0   memory length: 80951   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 430   score: 0.0   memory length: 81074   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 431   score: 2.0   memory length: 81289   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 432   score: 2.0   memory length: 81487   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 433   score: 1.0   memory length: 81638   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 434   score: 0.0   memory length: 81761   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 435   score: 0.0   memory length: 81884   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 436   score: 2.0   memory length: 82081   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 437   score: 0.0   memory length: 82204   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 438   score: 0.0   memory length: 82326   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 439   score: 2.0   memory length: 82523   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 440   score: 0.0   memory length: 82646   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 441   score: 2.0   memory length: 82844   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 442   score: 1.0   memory length: 82997   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 443   score: 0.0   memory length: 83119   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 444   score: 0.0   memory length: 83242   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 445   score: 2.0   memory length: 83439   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 446   score: 5.0   memory length: 83763   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 447   score: 2.0   memory length: 83943   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 448   score: 2.0   memory length: 84141   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 449   score: 0.0   memory length: 84263   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 450   score: 2.0   memory length: 84465   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 451   score: 1.0   memory length: 84637   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 452   score: 2.0   memory length: 84818   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 453   score: 3.0   memory length: 85044   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 454   score: 0.0   memory length: 85167   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 455   score: 0.0   memory length: 85290   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 456   score: 0.0   memory length: 85413   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 457   score: 2.0   memory length: 85610   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 458   score: 0.0   memory length: 85733   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 459   score: 0.0   memory length: 85855   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 460   score: 2.0   memory length: 86073   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 461   score: 3.0   memory length: 86299   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 462   score: 0.0   memory length: 86421   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 463   score: 3.0   memory length: 86668   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 464   score: 0.0   memory length: 86791   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 465   score: 3.0   memory length: 87038   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 466   score: 1.0   memory length: 87189   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 467   score: 3.0   memory length: 87436   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 468   score: 1.0   memory length: 87605   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 469   score: 7.0   memory length: 87904   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 470   score: 0.0   memory length: 88026   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 471   score: 1.0   memory length: 88197   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 472   score: 1.0   memory length: 88366   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 473   score: 0.0   memory length: 88488   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 474   score: 0.0   memory length: 88610   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 475   score: 2.0   memory length: 88811   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 476   score: 1.0   memory length: 88980   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 477   score: 2.0   memory length: 89178   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 478   score: 0.0   memory length: 89301   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 479   score: 4.0   memory length: 89577   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 480   score: 3.0   memory length: 89823   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 481   score: 2.0   memory length: 90007   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 482   score: 1.0   memory length: 90178   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 483   score: 0.0   memory length: 90301   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 484   score: 1.0   memory length: 90470   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 485   score: 2.0   memory length: 90667   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 486   score: 1.0   memory length: 90839   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 487   score: 1.0   memory length: 90990   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 488   score: 0.0   memory length: 91113   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 489   score: 0.0   memory length: 91236   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 490   score: 2.0   memory length: 91454   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 491   score: 0.0   memory length: 91577   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 492   score: 0.0   memory length: 91700   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 493   score: 5.0   memory length: 92011   epsilon: 1.0    steps: 311    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 494   score: 2.0   memory length: 92229   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 495   score: 2.0   memory length: 92409   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 496   score: 2.0   memory length: 92629   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 497   score: 1.0   memory length: 92797   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 498   score: 1.0   memory length: 92965   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 499   score: 2.0   memory length: 93185   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 500   score: 3.0   memory length: 93412   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 501   score: 2.0   memory length: 93630   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 502   score: 2.0   memory length: 93828   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 503   score: 2.0   memory length: 94026   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 504   score: 0.0   memory length: 94149   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 505   score: 0.0   memory length: 94271   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 506   score: 0.0   memory length: 94394   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 507   score: 3.0   memory length: 94640   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 508   score: 2.0   memory length: 94855   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 509   score: 4.0   memory length: 95147   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 510   score: 1.0   memory length: 95297   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 511   score: 1.0   memory length: 95465   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 512   score: 0.0   memory length: 95587   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 513   score: 0.0   memory length: 95709   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 514   score: 2.0   memory length: 95927   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 515   score: 3.0   memory length: 96176   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 516   score: 1.0   memory length: 96344   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 517   score: 0.0   memory length: 96467   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 518   score: 2.0   memory length: 96667   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 519   score: 3.0   memory length: 96878   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 520   score: 0.0   memory length: 97001   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 521   score: 1.0   memory length: 97171   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 522   score: 3.0   memory length: 97398   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 523   score: 3.0   memory length: 97646   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 524   score: 1.0   memory length: 97815   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 525   score: 2.0   memory length: 98015   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 526   score: 6.0   memory length: 98388   epsilon: 1.0    steps: 373    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 527   score: 2.0   memory length: 98586   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 528   score: 0.0   memory length: 98709   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 529   score: 2.0   memory length: 98906   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 530   score: 2.0   memory length: 99087   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 531   score: 2.0   memory length: 99303   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 532   score: 2.0   memory length: 99501   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 533   score: 3.0   memory length: 99727   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 534   score: 0.0   memory length: 99850   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 535   score: 0.0   memory length: 99973   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/assignment5/memory.py:30: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample = np.array(sample, dtype=object)\n",
      "/home/wxzhang/assignment5/agent.py:67: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  mini_batch = np.array(mini_batch, dtype=object).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 536   score: 0.0   memory length: 100096   epsilon: 0.9998079400000042    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 537   score: 2.0   memory length: 100316   epsilon: 0.9993723400000136    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 538   score: 2.0   memory length: 100533   epsilon: 0.998942680000023    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 539   score: 1.0   memory length: 100702   epsilon: 0.9986080600000302    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 540   score: 0.0   memory length: 100825   epsilon: 0.9983645200000355    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 541   score: 1.0   memory length: 100975   epsilon: 0.998067520000042    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 542   score: 2.0   memory length: 101173   epsilon: 0.9976754800000505    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 543   score: 1.0   memory length: 101343   epsilon: 0.9973388800000578    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 544   score: 3.0   memory length: 101608   epsilon: 0.9968141800000692    steps: 265    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 545   score: 3.0   memory length: 101853   epsilon: 0.9963290800000797    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 546   score: 2.0   memory length: 102074   epsilon: 0.9958915000000892    steps: 221    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 547   score: 3.0   memory length: 102303   epsilon: 0.995438080000099    steps: 229    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 548   score: 0.0   memory length: 102426   epsilon: 0.9951945400001043    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 549   score: 3.0   memory length: 102672   epsilon: 0.9947074600001149    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 550   score: 1.0   memory length: 102823   epsilon: 0.9944084800001214    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 551   score: 2.0   memory length: 103042   epsilon: 0.9939748600001308    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 552   score: 2.0   memory length: 103262   epsilon: 0.9935392600001403    steps: 220    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 553   score: 4.0   memory length: 103578   epsilon: 0.9929135800001538    steps: 316    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 554   score: 2.0   memory length: 103778   epsilon: 0.9925175800001624    steps: 200    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 555   score: 0.0   memory length: 103901   epsilon: 0.9922740400001677    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 556   score: 3.0   memory length: 104145   epsilon: 0.9917909200001782    steps: 244    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 557   score: 0.0   memory length: 104268   epsilon: 0.9915473800001835    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 558   score: 1.0   memory length: 104437   epsilon: 0.9912127600001908    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 559   score: 2.0   memory length: 104635   epsilon: 0.9908207200001993    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 560   score: 0.0   memory length: 104757   epsilon: 0.9905791600002045    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 561   score: 4.0   memory length: 105032   epsilon: 0.9900346600002163    steps: 275    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 562   score: 1.0   memory length: 105201   epsilon: 0.9897000400002236    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 563   score: 4.0   memory length: 105475   epsilon: 0.9891575200002354    steps: 274    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 564   score: 1.0   memory length: 105647   epsilon: 0.9888169600002428    steps: 172    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 565   score: 0.0   memory length: 105770   epsilon: 0.9885734200002481    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 566   score: 0.0   memory length: 105893   epsilon: 0.9883298800002533    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 567   score: 4.0   memory length: 106187   epsilon: 0.987747760000266    steps: 294    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 568   score: 0.0   memory length: 106310   epsilon: 0.9875042200002713    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 569   score: 2.0   memory length: 106528   epsilon: 0.9870725800002806    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 570   score: 1.0   memory length: 106679   epsilon: 0.9867736000002871    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 571   score: 1.0   memory length: 106830   epsilon: 0.9864746200002936    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 572   score: 2.0   memory length: 107028   epsilon: 0.9860825800003021    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 573   score: 2.0   memory length: 107209   epsilon: 0.9857242000003099    steps: 181    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 574   score: 1.0   memory length: 107379   epsilon: 0.9853876000003172    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 575   score: 3.0   memory length: 107644   epsilon: 0.9848629000003286    steps: 265    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 576   score: 4.0   memory length: 107939   epsilon: 0.9842788000003413    steps: 295    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 577   score: 4.0   memory length: 108216   epsilon: 0.9837303400003532    steps: 277    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 578   score: 2.0   memory length: 108414   epsilon: 0.9833383000003617    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 579   score: 1.0   memory length: 108582   epsilon: 0.9830056600003689    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 580   score: 0.0   memory length: 108705   epsilon: 0.9827621200003742    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 581   score: 4.0   memory length: 108998   epsilon: 0.9821819800003868    steps: 293    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 582   score: 3.0   memory length: 109243   epsilon: 0.9816968800003973    steps: 245    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 583   score: 0.0   memory length: 109366   epsilon: 0.9814533400004026    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 584   score: 0.0   memory length: 109489   epsilon: 0.9812098000004079    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 585   score: 0.0   memory length: 109611   epsilon: 0.9809682400004132    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 586   score: 1.0   memory length: 109779   epsilon: 0.9806356000004204    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 587   score: 2.0   memory length: 109999   epsilon: 0.9802000000004298    steps: 220    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 588   score: 0.0   memory length: 110122   epsilon: 0.9799564600004351    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 589   score: 3.0   memory length: 110388   epsilon: 0.9794297800004466    steps: 266    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 590   score: 1.0   memory length: 110557   epsilon: 0.9790951600004538    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 591   score: 2.0   memory length: 110754   epsilon: 0.9787051000004623    steps: 197    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 592   score: 1.0   memory length: 110924   epsilon: 0.9783685000004696    steps: 170    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 593   score: 2.0   memory length: 111143   epsilon: 0.977934880000479    steps: 219    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 594   score: 0.0   memory length: 111266   epsilon: 0.9776913400004843    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 595   score: 3.0   memory length: 111511   epsilon: 0.9772062400004948    steps: 245    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 596   score: 1.0   memory length: 111680   epsilon: 0.9768716200005021    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 597   score: 1.0   memory length: 111848   epsilon: 0.9765389800005093    steps: 168    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 598   score: 0.0   memory length: 111971   epsilon: 0.9762954400005146    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 599   score: 0.0   memory length: 112093   epsilon: 0.9760538800005198    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 600   score: 0.0   memory length: 112216   epsilon: 0.9758103400005251    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 601   score: 4.0   memory length: 112491   epsilon: 0.975265840000537    steps: 275    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 602   score: 3.0   memory length: 112702   epsilon: 0.974848060000546    steps: 211    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 603   score: 2.0   memory length: 112920   epsilon: 0.9744164200005554    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 604   score: 0.0   memory length: 113043   epsilon: 0.9741728800005607    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 605   score: 1.0   memory length: 113212   epsilon: 0.973838260000568    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 606   score: 2.0   memory length: 113409   epsilon: 0.9734482000005764    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 607   score: 2.0   memory length: 113627   epsilon: 0.9730165600005858    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 608   score: 2.0   memory length: 113846   epsilon: 0.9725829400005952    steps: 219    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 609   score: 3.0   memory length: 114072   epsilon: 0.9721354600006049    steps: 226    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 610   score: 0.0   memory length: 114195   epsilon: 0.9718919200006102    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 611   score: 1.0   memory length: 114363   epsilon: 0.9715592800006174    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 612   score: 3.0   memory length: 114611   epsilon: 0.9710682400006281    steps: 248    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 613   score: 0.0   memory length: 114733   epsilon: 0.9708266800006333    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 614   score: 3.0   memory length: 114962   epsilon: 0.9703732600006432    steps: 229    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 615   score: 0.0   memory length: 115085   epsilon: 0.9701297200006485    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 616   score: 1.0   memory length: 115236   epsilon: 0.969830740000655    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 617   score: 2.0   memory length: 115436   epsilon: 0.9694347400006635    steps: 200    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 618   score: 2.0   memory length: 115634   epsilon: 0.969042700000672    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 619   score: 3.0   memory length: 115901   epsilon: 0.9685140400006835    steps: 267    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 620   score: 2.0   memory length: 116101   epsilon: 0.9681180400006921    steps: 200    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 621   score: 0.0   memory length: 116224   epsilon: 0.9678745000006974    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 622   score: 1.0   memory length: 116392   epsilon: 0.9675418600007046    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 623   score: 0.0   memory length: 116515   epsilon: 0.9672983200007099    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 624   score: 0.0   memory length: 116637   epsilon: 0.9670567600007152    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 625   score: 1.0   memory length: 116787   epsilon: 0.9667597600007216    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 626   score: 1.0   memory length: 116938   epsilon: 0.9664607800007281    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 627   score: 2.0   memory length: 117157   epsilon: 0.9660271600007375    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 628   score: 2.0   memory length: 117375   epsilon: 0.9655955200007469    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 629   score: 0.0   memory length: 117497   epsilon: 0.9653539600007521    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 630   score: 2.0   memory length: 117694   epsilon: 0.9649639000007606    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 631   score: 0.0   memory length: 117817   epsilon: 0.9647203600007659    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 632   score: 0.0   memory length: 117939   epsilon: 0.9644788000007711    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 633   score: 2.0   memory length: 118155   epsilon: 0.9640511200007804    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 634   score: 0.0   memory length: 118278   epsilon: 0.9638075800007857    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 635   score: 2.0   memory length: 118496   epsilon: 0.9633759400007951    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 636   score: 0.0   memory length: 118619   epsilon: 0.9631324000008004    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 637   score: 2.0   memory length: 118835   epsilon: 0.9627047200008096    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 638   score: 3.0   memory length: 119063   epsilon: 0.9622532800008194    steps: 228    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 639   score: 2.0   memory length: 119283   epsilon: 0.9618176800008289    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 640   score: 1.0   memory length: 119453   epsilon: 0.9614810800008362    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 641   score: 0.0   memory length: 119575   epsilon: 0.9612395200008415    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 642   score: 1.0   memory length: 119746   epsilon: 0.9609009400008488    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 643   score: 2.0   memory length: 119943   epsilon: 0.9605108800008573    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 644   score: 6.0   memory length: 120319   epsilon: 0.9597664000008734    steps: 376    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 645   score: 1.0   memory length: 120470   epsilon: 0.9594674200008799    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 646   score: 3.0   memory length: 120695   epsilon: 0.9590219200008896    steps: 225    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 647   score: 0.0   memory length: 120817   epsilon: 0.9587803600008948    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 648   score: 0.0   memory length: 120940   epsilon: 0.9585368200009001    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 649   score: 3.0   memory length: 121152   epsilon: 0.9581170600009092    steps: 212    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 650   score: 0.0   memory length: 121274   epsilon: 0.9578755000009145    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 651   score: 2.0   memory length: 121472   epsilon: 0.957483460000923    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 652   score: 2.0   memory length: 121670   epsilon: 0.9570914200009315    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 653   score: 0.0   memory length: 121792   epsilon: 0.9568498600009367    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 654   score: 2.0   memory length: 122011   epsilon: 0.9564162400009462    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 655   score: 4.0   memory length: 122306   epsilon: 0.9558321400009588    steps: 295    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 656   score: 1.0   memory length: 122478   epsilon: 0.9554915800009662    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 657   score: 4.0   memory length: 122791   epsilon: 0.9548718400009797    steps: 313    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 658   score: 4.0   memory length: 123083   epsilon: 0.9542936800009922    steps: 292    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 659   score: 1.0   memory length: 123254   epsilon: 0.9539551000009996    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 660   score: 5.0   memory length: 123619   epsilon: 0.9532324000010153    steps: 365    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 661   score: 3.0   memory length: 123845   epsilon: 0.952784920001025    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 662   score: 0.0   memory length: 123967   epsilon: 0.9525433600010302    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 663   score: 2.0   memory length: 124165   epsilon: 0.9521513200010387    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 664   score: 0.0   memory length: 124287   epsilon: 0.951909760001044    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 665   score: 2.0   memory length: 124504   epsilon: 0.9514801000010533    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 666   score: 2.0   memory length: 124722   epsilon: 0.9510484600010627    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 667   score: 3.0   memory length: 124988   epsilon: 0.9505217800010741    steps: 266    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 668   score: 1.0   memory length: 125156   epsilon: 0.9501891400010813    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 669   score: 1.0   memory length: 125325   epsilon: 0.9498545200010886    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 670   score: 1.0   memory length: 125497   epsilon: 0.949513960001096    steps: 172    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 671   score: 3.0   memory length: 125746   epsilon: 0.9490209400011067    steps: 249    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 672   score: 1.0   memory length: 125917   epsilon: 0.9486823600011141    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 673   score: 0.0   memory length: 126040   epsilon: 0.9484388200011193    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 674   score: 1.0   memory length: 126190   epsilon: 0.9481418200011258    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 675   score: 0.0   memory length: 126313   epsilon: 0.9478982800011311    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 676   score: 2.0   memory length: 126511   epsilon: 0.9475062400011396    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 677   score: 0.0   memory length: 126634   epsilon: 0.9472627000011449    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 678   score: 1.0   memory length: 126803   epsilon: 0.9469280800011521    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 679   score: 8.0   memory length: 127263   epsilon: 0.9460172800011719    steps: 460    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 680   score: 2.0   memory length: 127461   epsilon: 0.9456252400011804    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 681   score: 1.0   memory length: 127629   epsilon: 0.9452926000011876    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 682   score: 1.0   memory length: 127798   epsilon: 0.9449579800011949    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 683   score: 2.0   memory length: 127977   epsilon: 0.9446035600012026    steps: 179    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 684   score: 0.0   memory length: 128100   epsilon: 0.9443600200012079    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 685   score: 0.0   memory length: 128223   epsilon: 0.9441164800012132    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 686   score: 1.0   memory length: 128373   epsilon: 0.9438194800012196    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 687   score: 0.0   memory length: 128496   epsilon: 0.9435759400012249    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 688   score: 0.0   memory length: 128619   epsilon: 0.9433324000012302    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 689   score: 2.0   memory length: 128836   epsilon: 0.9429027400012395    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 690   score: 0.0   memory length: 128958   epsilon: 0.9426611800012448    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 691   score: 1.0   memory length: 129109   epsilon: 0.9423622000012513    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 692   score: 4.0   memory length: 129408   epsilon: 0.9417701800012641    steps: 299    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 693   score: 1.0   memory length: 129559   epsilon: 0.9414712000012706    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 694   score: 0.0   memory length: 129681   epsilon: 0.9412296400012758    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 695   score: 2.0   memory length: 129878   epsilon: 0.9408395800012843    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 696   score: 0.0   memory length: 130001   epsilon: 0.9405960400012896    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 697   score: 1.0   memory length: 130169   epsilon: 0.9402634000012968    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 698   score: 0.0   memory length: 130292   epsilon: 0.9400198600013021    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 699   score: 2.0   memory length: 130490   epsilon: 0.9396278200013106    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 700   score: 2.0   memory length: 130709   epsilon: 0.93919420000132    steps: 219    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 701   score: 3.0   memory length: 130956   epsilon: 0.9387051400013307    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 702   score: 0.0   memory length: 131078   epsilon: 0.9384635800013359    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 703   score: 1.0   memory length: 131246   epsilon: 0.9381309400013431    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 704   score: 2.0   memory length: 131443   epsilon: 0.9377408800013516    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 705   score: 1.0   memory length: 131612   epsilon: 0.9374062600013588    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 706   score: 1.0   memory length: 131783   epsilon: 0.9370676800013662    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 707   score: 2.0   memory length: 132002   epsilon: 0.9366340600013756    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 708   score: 2.0   memory length: 132200   epsilon: 0.9362420200013841    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 709   score: 0.0   memory length: 132323   epsilon: 0.9359984800013894    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 710   score: 1.0   memory length: 132492   epsilon: 0.9356638600013967    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 711   score: 0.0   memory length: 132615   epsilon: 0.935420320001402    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 712   score: 0.0   memory length: 132738   epsilon: 0.9351767800014072    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 713   score: 4.0   memory length: 133004   epsilon: 0.9346501000014187    steps: 266    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 714   score: 0.0   memory length: 133127   epsilon: 0.934406560001424    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 715   score: 1.0   memory length: 133295   epsilon: 0.9340739200014312    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 716   score: 4.0   memory length: 133572   epsilon: 0.9335254600014431    steps: 277    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 717   score: 0.0   memory length: 133695   epsilon: 0.9332819200014484    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 718   score: 1.0   memory length: 133864   epsilon: 0.9329473000014556    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 719   score: 3.0   memory length: 134114   epsilon: 0.9324523000014664    steps: 250    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 720   score: 0.0   memory length: 134237   epsilon: 0.9322087600014717    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 721   score: 4.0   memory length: 134532   epsilon: 0.9316246600014844    steps: 295    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 722   score: 1.0   memory length: 134701   epsilon: 0.9312900400014916    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 723   score: 2.0   memory length: 134920   epsilon: 0.930856420001501    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 724   score: 1.0   memory length: 135071   epsilon: 0.9305574400015075    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 725   score: 2.0   memory length: 135269   epsilon: 0.930165400001516    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 726   score: 2.0   memory length: 135467   epsilon: 0.9297733600015246    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 727   score: 2.0   memory length: 135684   epsilon: 0.9293437000015339    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 728   score: 3.0   memory length: 135931   epsilon: 0.9288546400015445    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 729   score: 2.0   memory length: 136111   epsilon: 0.9284982400015522    steps: 180    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 730   score: 1.0   memory length: 136280   epsilon: 0.9281636200015595    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 731   score: 1.0   memory length: 136449   epsilon: 0.9278290000015668    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 732   score: 0.0   memory length: 136572   epsilon: 0.927585460001572    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 733   score: 4.0   memory length: 136867   epsilon: 0.9270013600015847    steps: 295    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 734   score: 0.0   memory length: 136990   epsilon: 0.92675782000159    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 735   score: 1.0   memory length: 137160   epsilon: 0.9264212200015973    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 736   score: 1.0   memory length: 137311   epsilon: 0.9261222400016038    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 737   score: 0.0   memory length: 137434   epsilon: 0.9258787000016091    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 738   score: 1.0   memory length: 137606   epsilon: 0.9255381400016165    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 739   score: 4.0   memory length: 137919   epsilon: 0.92491840000163    steps: 313    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 740   score: 0.0   memory length: 138042   epsilon: 0.9246748600016352    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 741   score: 1.0   memory length: 138193   epsilon: 0.9243758800016417    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 742   score: 1.0   memory length: 138362   epsilon: 0.924041260001649    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 743   score: 2.0   memory length: 138579   epsilon: 0.9236116000016583    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 744   score: 0.0   memory length: 138701   epsilon: 0.9233700400016636    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 745   score: 8.0   memory length: 139030   epsilon: 0.9227186200016777    steps: 329    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 746   score: 1.0   memory length: 139181   epsilon: 0.9224196400016842    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 747   score: 1.0   memory length: 139333   epsilon: 0.9221186800016907    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 748   score: 0.0   memory length: 139455   epsilon: 0.921877120001696    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 749   score: 1.0   memory length: 139627   epsilon: 0.9215365600017034    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 750   score: 3.0   memory length: 139852   epsilon: 0.921091060001713    steps: 225    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 751   score: 1.0   memory length: 140023   epsilon: 0.9207524800017204    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 752   score: 1.0   memory length: 140173   epsilon: 0.9204554800017268    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 753   score: 0.0   memory length: 140296   epsilon: 0.9202119400017321    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 754   score: 1.0   memory length: 140464   epsilon: 0.9198793000017393    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 755   score: 3.0   memory length: 140709   epsilon: 0.9193942000017499    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 756   score: 1.0   memory length: 140860   epsilon: 0.9190952200017564    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 757   score: 3.0   memory length: 141124   epsilon: 0.9185725000017677    steps: 264    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 758   score: 3.0   memory length: 141370   epsilon: 0.9180854200017783    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 759   score: 1.0   memory length: 141538   epsilon: 0.9177527800017855    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 760   score: 0.0   memory length: 141661   epsilon: 0.9175092400017908    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 761   score: 0.0   memory length: 141784   epsilon: 0.9172657000017961    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 762   score: 3.0   memory length: 142048   epsilon: 0.9167429800018074    steps: 264    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 763   score: 2.0   memory length: 142265   epsilon: 0.9163133200018168    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 764   score: 2.0   memory length: 142483   epsilon: 0.9158816800018261    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 765   score: 3.0   memory length: 142709   epsilon: 0.9154342000018358    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 766   score: 0.0   memory length: 142832   epsilon: 0.9151906600018411    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 767   score: 0.0   memory length: 142954   epsilon: 0.9149491000018464    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 768   score: 0.0   memory length: 143077   epsilon: 0.9147055600018517    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 769   score: 1.0   memory length: 143249   epsilon: 0.914365000001859    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 770   score: 1.0   memory length: 143418   epsilon: 0.9140303800018663    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 771   score: 2.0   memory length: 143599   epsilon: 0.9136720000018741    steps: 181    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 772   score: 1.0   memory length: 143750   epsilon: 0.9133730200018806    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 773   score: 3.0   memory length: 144016   epsilon: 0.912846340001892    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 774   score: 2.0   memory length: 144213   epsilon: 0.9124562800019005    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 775   score: 0.0   memory length: 144335   epsilon: 0.9122147200019057    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 776   score: 0.0   memory length: 144458   epsilon: 0.911971180001911    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 777   score: 0.0   memory length: 144581   epsilon: 0.9117276400019163    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 778   score: 2.0   memory length: 144799   epsilon: 0.9112960000019257    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 779   score: 0.0   memory length: 144922   epsilon: 0.911052460001931    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 780   score: 0.0   memory length: 145045   epsilon: 0.9108089200019363    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 781   score: 3.0   memory length: 145275   epsilon: 0.9103535200019461    steps: 230    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 782   score: 2.0   memory length: 145472   epsilon: 0.9099634600019546    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 783   score: 0.0   memory length: 145594   epsilon: 0.9097219000019598    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 784   score: 2.0   memory length: 145810   epsilon: 0.9092942200019691    steps: 216    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 785   score: 0.0   memory length: 145933   epsilon: 0.9090506800019744    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 786   score: 2.0   memory length: 146130   epsilon: 0.9086606200019829    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 787   score: 2.0   memory length: 146328   epsilon: 0.9082685800019914    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 788   score: 2.0   memory length: 146525   epsilon: 0.9078785200019999    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 789   score: 0.0   memory length: 146648   epsilon: 0.9076349800020052    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 790   score: 2.0   memory length: 146866   epsilon: 0.9072033400020145    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 791   score: 2.0   memory length: 147085   epsilon: 0.9067697200020239    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 792   score: 3.0   memory length: 147317   epsilon: 0.9063103600020339    steps: 232    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 793   score: 0.0   memory length: 147439   epsilon: 0.9060688000020392    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 794   score: 4.0   memory length: 147757   epsilon: 0.9054391600020528    steps: 318    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 795   score: 3.0   memory length: 148005   epsilon: 0.9049481200020635    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 796   score: 2.0   memory length: 148203   epsilon: 0.904556080002072    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 797   score: 0.0   memory length: 148326   epsilon: 0.9043125400020773    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 798   score: 2.0   memory length: 148524   epsilon: 0.9039205000020858    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 799   score: 7.0   memory length: 148770   epsilon: 0.9034334200020964    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 800   score: 2.0   memory length: 148968   epsilon: 0.9030413800021049    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 801   score: 0.0   memory length: 149091   epsilon: 0.9027978400021102    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 802   score: 3.0   memory length: 149338   epsilon: 0.9023087800021208    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 803   score: 0.0   memory length: 149461   epsilon: 0.9020652400021261    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 804   score: 1.0   memory length: 149630   epsilon: 0.9017306200021333    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 805   score: 1.0   memory length: 149781   epsilon: 0.9014316400021398    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 806   score: 3.0   memory length: 150025   epsilon: 0.9009485200021503    steps: 244    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 807   score: 1.0   memory length: 150176   epsilon: 0.9006495400021568    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 808   score: 3.0   memory length: 150443   epsilon: 0.9001208800021683    steps: 267    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 809   score: 3.0   memory length: 150689   epsilon: 0.8996338000021789    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 810   score: 1.0   memory length: 150840   epsilon: 0.8993348200021853    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 811   score: 1.0   memory length: 150991   epsilon: 0.8990358400021918    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 812   score: 0.0   memory length: 151114   epsilon: 0.8987923000021971    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 813   score: 2.0   memory length: 151311   epsilon: 0.8984022400022056    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 814   score: 2.0   memory length: 151509   epsilon: 0.8980102000022141    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 815   score: 3.0   memory length: 151735   epsilon: 0.8975627200022238    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 816   score: 0.0   memory length: 151857   epsilon: 0.8973211600022291    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 817   score: 4.0   memory length: 152172   epsilon: 0.8966974600022426    steps: 315    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 818   score: 0.0   memory length: 152295   epsilon: 0.8964539200022479    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 819   score: 3.0   memory length: 152543   epsilon: 0.8959628800022585    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 820   score: 2.0   memory length: 152741   epsilon: 0.895570840002267    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 821   score: 1.0   memory length: 152892   epsilon: 0.8952718600022735    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 822   score: 1.0   memory length: 153043   epsilon: 0.89497288000228    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 823   score: 1.0   memory length: 153194   epsilon: 0.8946739000022865    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 824   score: 3.0   memory length: 153458   epsilon: 0.8941511800022979    steps: 264    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 825   score: 2.0   memory length: 153656   epsilon: 0.8937591400023064    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 826   score: 2.0   memory length: 153837   epsilon: 0.8934007600023142    steps: 181    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 827   score: 2.0   memory length: 154019   epsilon: 0.893040400002322    steps: 182    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 828   score: 0.0   memory length: 154141   epsilon: 0.8927988400023272    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 829   score: 1.0   memory length: 154311   epsilon: 0.8924622400023345    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 830   score: 1.0   memory length: 154481   epsilon: 0.8921256400023418    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 831   score: 2.0   memory length: 154699   epsilon: 0.8916940000023512    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 832   score: 1.0   memory length: 154871   epsilon: 0.8913534400023586    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 833   score: 2.0   memory length: 155069   epsilon: 0.8909614000023671    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 834   score: 1.0   memory length: 155221   epsilon: 0.8906604400023737    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 835   score: 1.0   memory length: 155390   epsilon: 0.8903258200023809    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 836   score: 5.0   memory length: 155719   epsilon: 0.8896744000023951    steps: 329    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 837   score: 2.0   memory length: 155917   epsilon: 0.8892823600024036    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 838   score: 6.0   memory length: 156304   epsilon: 0.8885161000024202    steps: 387    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 839   score: 1.0   memory length: 156473   epsilon: 0.8881814800024275    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 840   score: 4.0   memory length: 156730   epsilon: 0.8876726200024385    steps: 257    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 841   score: 2.0   memory length: 156948   epsilon: 0.8872409800024479    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 842   score: 8.0   memory length: 157283   epsilon: 0.8865776800024623    steps: 335    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 843   score: 0.0   memory length: 157406   epsilon: 0.8863341400024676    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 844   score: 2.0   memory length: 157624   epsilon: 0.885902500002477    steps: 218    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 845   score: 2.0   memory length: 157822   epsilon: 0.8855104600024855    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 846   score: 4.0   memory length: 158098   epsilon: 0.8849639800024973    steps: 276    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 847   score: 2.0   memory length: 158298   epsilon: 0.8845679800025059    steps: 200    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 848   score: 1.0   memory length: 158449   epsilon: 0.8842690000025124    steps: 151    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 849   score: 2.0   memory length: 158647   epsilon: 0.8838769600025209    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 850   score: 1.0   memory length: 158815   epsilon: 0.8835443200025281    steps: 168    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 851   score: 4.0   memory length: 159090   epsilon: 0.88299982000254    steps: 275    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 852   score: 1.0   memory length: 159258   epsilon: 0.8826671800025472    steps: 168    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 853   score: 0.0   memory length: 159381   epsilon: 0.8824236400025525    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 854   score: 3.0   memory length: 159626   epsilon: 0.881938540002563    steps: 245    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 855   score: 3.0   memory length: 159852   epsilon: 0.8814910600025727    steps: 226    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 856   score: 2.0   memory length: 160050   epsilon: 0.8810990200025812    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 857   score: 3.0   memory length: 160315   epsilon: 0.8805743200025926    steps: 265    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 858   score: 0.0   memory length: 160438   epsilon: 0.8803307800025979    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 859   score: 1.0   memory length: 160607   epsilon: 0.8799961600026052    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 860   score: 1.0   memory length: 160759   epsilon: 0.8796952000026117    steps: 152    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 861   score: 0.0   memory length: 160882   epsilon: 0.879451660002617    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 862   score: 0.0   memory length: 161005   epsilon: 0.8792081200026223    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 863   score: 1.0   memory length: 161155   epsilon: 0.8789111200026287    steps: 150    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 864   score: 2.0   memory length: 161373   epsilon: 0.8784794800026381    steps: 218    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 865   score: 1.0   memory length: 161543   epsilon: 0.8781428800026454    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 866   score: 2.0   memory length: 161761   epsilon: 0.8777112400026548    steps: 218    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 867   score: 1.0   memory length: 161932   epsilon: 0.8773726600026621    steps: 171    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 868   score: 2.0   memory length: 162129   epsilon: 0.8769826000026706    steps: 197    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 869   score: 3.0   memory length: 162377   epsilon: 0.8764915600026812    steps: 248    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 870   score: 3.0   memory length: 162642   epsilon: 0.8759668600026926    steps: 265    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 871   score: 0.0   memory length: 162765   epsilon: 0.8757233200026979    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 872   score: 5.0   memory length: 163096   epsilon: 0.8750679400027122    steps: 331    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 873   score: 3.0   memory length: 163342   epsilon: 0.8745808600027227    steps: 246    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 874   score: 2.0   memory length: 163558   epsilon: 0.874153180002732    steps: 216    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 875   score: 1.0   memory length: 163708   epsilon: 0.8738561800027385    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 876   score: 4.0   memory length: 163974   epsilon: 0.8733295000027499    steps: 266    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 877   score: 2.0   memory length: 164191   epsilon: 0.8728998400027592    steps: 217    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 878   score: 0.0   memory length: 164314   epsilon: 0.8726563000027645    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 879   score: 2.0   memory length: 164494   epsilon: 0.8722999000027722    steps: 180    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 880   score: 2.0   memory length: 164715   epsilon: 0.8718623200027817    steps: 221    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 881   score: 1.0   memory length: 164886   epsilon: 0.8715237400027891    steps: 171    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 882   score: 3.0   memory length: 165098   epsilon: 0.8711039800027982    steps: 212    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 883   score: 0.0   memory length: 165221   epsilon: 0.8708604400028035    steps: 123    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 884   score: 0.0   memory length: 165343   epsilon: 0.8706188800028087    steps: 122    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 885   score: 1.0   memory length: 165515   epsilon: 0.8702783200028161    steps: 172    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 886   score: 0.0   memory length: 165638   epsilon: 0.8700347800028214    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 887   score: 0.0   memory length: 165761   epsilon: 0.8697912400028267    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 888   score: 3.0   memory length: 166008   epsilon: 0.8693021800028373    steps: 247    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 889   score: 1.0   memory length: 166159   epsilon: 0.8690032000028438    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 890   score: 0.0   memory length: 166282   epsilon: 0.8687596600028491    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 891   score: 2.0   memory length: 166500   epsilon: 0.8683280200028585    steps: 218    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 892   score: 1.0   memory length: 166650   epsilon: 0.8680310200028649    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 893   score: 3.0   memory length: 166875   epsilon: 0.8675855200028746    steps: 225    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 894   score: 1.0   memory length: 167046   epsilon: 0.8672469400028819    steps: 171    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 895   score: 2.0   memory length: 167244   epsilon: 0.8668549000028904    steps: 198    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 896   score: 2.0   memory length: 167460   epsilon: 0.8664272200028997    steps: 216    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 897   score: 1.0   memory length: 167630   epsilon: 0.866090620002907    steps: 170    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 898   score: 4.0   memory length: 167924   epsilon: 0.8655085000029197    steps: 294    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 899   score: 2.0   memory length: 168142   epsilon: 0.865076860002929    steps: 218    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 900   score: 3.0   memory length: 168409   epsilon: 0.8645482000029405    steps: 267    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 901   score: 1.0   memory length: 168560   epsilon: 0.864249220002947    steps: 151    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 902   score: 0.0   memory length: 168683   epsilon: 0.8640056800029523    steps: 123    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 903   score: 2.0   memory length: 168883   epsilon: 0.8636096800029609    steps: 200    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 904   score: 0.0   memory length: 169005   epsilon: 0.8633681200029661    steps: 122    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 905   score: 1.0   memory length: 169156   epsilon: 0.8630691400029726    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 906   score: 0.0   memory length: 169279   epsilon: 0.8628256000029779    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 907   score: 3.0   memory length: 169524   epsilon: 0.8623405000029885    steps: 245    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 908   score: 5.0   memory length: 169849   epsilon: 0.8616970000030024    steps: 325    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 909   score: 1.0   memory length: 170018   epsilon: 0.8613623800030097    steps: 169    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 910   score: 2.0   memory length: 170234   epsilon: 0.860934700003019    steps: 216    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 911   score: 0.0   memory length: 170357   epsilon: 0.8606911600030243    steps: 123    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 912   score: 2.0   memory length: 170555   epsilon: 0.8602991200030328    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 913   score: 2.0   memory length: 170774   epsilon: 0.8598655000030422    steps: 219    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 914   score: 4.0   memory length: 171049   epsilon: 0.859321000003054    steps: 275    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 915   score: 1.0   memory length: 171218   epsilon: 0.8589863800030613    steps: 169    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 916   score: 1.0   memory length: 171369   epsilon: 0.8586874000030678    steps: 151    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 917   score: 2.0   memory length: 171588   epsilon: 0.8582537800030772    steps: 219    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 918   score: 1.0   memory length: 171738   epsilon: 0.8579567800030836    steps: 150    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 919   score: 3.0   memory length: 171985   epsilon: 0.8574677200030942    steps: 247    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 920   score: 2.0   memory length: 172183   epsilon: 0.8570756800031027    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 921   score: 3.0   memory length: 172429   epsilon: 0.8565886000031133    steps: 246    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 922   score: 1.0   memory length: 172580   epsilon: 0.8562896200031198    steps: 151    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 923   score: 0.0   memory length: 172703   epsilon: 0.8560460800031251    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 924   score: 2.0   memory length: 172902   epsilon: 0.8556520600031337    steps: 199    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 925   score: 4.0   memory length: 173197   epsilon: 0.8550679600031463    steps: 295    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 926   score: 3.0   memory length: 173443   epsilon: 0.8545808800031569    steps: 246    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 927   score: 3.0   memory length: 173670   epsilon: 0.8541314200031667    steps: 227    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 928   score: 1.0   memory length: 173821   epsilon: 0.8538324400031732    steps: 151    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 929   score: 3.0   memory length: 174065   epsilon: 0.8533493200031836    steps: 244    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 930   score: 0.0   memory length: 174187   epsilon: 0.8531077600031889    steps: 122    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 931   score: 0.0   memory length: 174310   epsilon: 0.8528642200031942    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 932   score: 1.0   memory length: 174482   epsilon: 0.8525236600032016    steps: 172    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 933   score: 6.0   memory length: 174846   epsilon: 0.8518029400032172    steps: 364    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 934   score: 1.0   memory length: 175014   epsilon: 0.8514703000032244    steps: 168    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 935   score: 3.0   memory length: 175262   epsilon: 0.8509792600032351    steps: 248    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 936   score: 2.0   memory length: 175480   epsilon: 0.8505476200032445    steps: 218    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 937   score: 2.0   memory length: 175698   epsilon: 0.8501159800032538    steps: 218    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 938   score: 2.0   memory length: 175896   epsilon: 0.8497239400032623    steps: 198    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 939   score: 2.0   memory length: 176094   epsilon: 0.8493319000032709    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 940   score: 3.0   memory length: 176343   epsilon: 0.8488388800032816    steps: 249    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 941   score: 3.0   memory length: 176591   epsilon: 0.8483478400032922    steps: 248    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 942   score: 5.0   memory length: 176933   epsilon: 0.8476706800033069    steps: 342    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 943   score: 2.0   memory length: 177151   epsilon: 0.8472390400033163    steps: 218    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 944   score: 2.0   memory length: 177348   epsilon: 0.8468489800033248    steps: 197    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 945   score: 3.0   memory length: 177595   epsilon: 0.8463599200033354    steps: 247    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 946   score: 1.0   memory length: 177746   epsilon: 0.8460609400033419    steps: 151    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 947   score: 1.0   memory length: 177897   epsilon: 0.8457619600033484    steps: 151    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 948   score: 2.0   memory length: 178117   epsilon: 0.8453263600033578    steps: 220    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 949   score: 0.0   memory length: 178240   epsilon: 0.8450828200033631    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 950   score: 2.0   memory length: 178458   epsilon: 0.8446511800033725    steps: 218    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 951   score: 2.0   memory length: 178656   epsilon: 0.844259140003381    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 952   score: 0.0   memory length: 178779   epsilon: 0.8440156000033863    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 953   score: 0.0   memory length: 178902   epsilon: 0.8437720600033916    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 954   score: 1.0   memory length: 179073   epsilon: 0.8434334800033989    steps: 171    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 955   score: 2.0   memory length: 179270   epsilon: 0.8430434200034074    steps: 197    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 956   score: 1.0   memory length: 179420   epsilon: 0.8427464200034138    steps: 150    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 957   score: 1.0   memory length: 179570   epsilon: 0.8424494200034203    steps: 150    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 958   score: 2.0   memory length: 179770   epsilon: 0.8420534200034289    steps: 200    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 959   score: 2.0   memory length: 179950   epsilon: 0.8416970200034366    steps: 180    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 960   score: 2.0   memory length: 180148   epsilon: 0.8413049800034451    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 961   score: 2.0   memory length: 180366   epsilon: 0.8408733400034545    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 962   score: 2.0   memory length: 180546   epsilon: 0.8405169400034622    steps: 180    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 963   score: 3.0   memory length: 180774   epsilon: 0.840065500003472    steps: 228    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 964   score: 0.0   memory length: 180897   epsilon: 0.8398219600034773    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 965   score: 3.0   memory length: 181142   epsilon: 0.8393368600034878    steps: 245    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 966   score: 0.0   memory length: 181265   epsilon: 0.8390933200034931    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 967   score: 3.0   memory length: 181491   epsilon: 0.8386458400035028    steps: 226    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 968   score: 5.0   memory length: 181799   epsilon: 0.8380360000035161    steps: 308    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 969   score: 2.0   memory length: 182017   epsilon: 0.8376043600035254    steps: 218    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 970   score: 3.0   memory length: 182265   epsilon: 0.8371133200035361    steps: 248    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 971   score: 0.0   memory length: 182388   epsilon: 0.8368697800035414    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 972   score: 4.0   memory length: 182680   epsilon: 0.836291620003554    steps: 292    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 973   score: 5.0   memory length: 183014   epsilon: 0.8356303000035683    steps: 334    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 974   score: 3.0   memory length: 183260   epsilon: 0.8351432200035789    steps: 246    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 975   score: 2.0   memory length: 183458   epsilon: 0.8347511800035874    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 976   score: 3.0   memory length: 183684   epsilon: 0.8343037000035971    steps: 226    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 977   score: 3.0   memory length: 183928   epsilon: 0.8338205800036076    steps: 244    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 978   score: 3.0   memory length: 184173   epsilon: 0.8333354800036181    steps: 245    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 979   score: 4.0   memory length: 184449   epsilon: 0.83278900000363    steps: 276    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 980   score: 3.0   memory length: 184675   epsilon: 0.8323415200036397    steps: 226    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 981   score: 4.0   memory length: 184971   epsilon: 0.8317554400036524    steps: 296    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 982   score: 6.0   memory length: 185348   epsilon: 0.8310089800036686    steps: 377    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 983   score: 3.0   memory length: 185596   epsilon: 0.8305179400036793    steps: 248    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 984   score: 3.0   memory length: 185826   epsilon: 0.8300625400036892    steps: 230    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 985   score: 3.0   memory length: 186071   epsilon: 0.8295774400036997    steps: 245    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 986   score: 4.0   memory length: 186387   epsilon: 0.8289517600037133    steps: 316    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 987   score: 1.0   memory length: 186538   epsilon: 0.8286527800037198    steps: 151    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 988   score: 2.0   memory length: 186756   epsilon: 0.8282211400037292    steps: 218    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 989   score: 4.0   memory length: 187030   epsilon: 0.8276786200037409    steps: 274    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 990   score: 3.0   memory length: 187277   epsilon: 0.8271895600037515    steps: 247    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 991   score: 2.0   memory length: 187460   epsilon: 0.8268272200037594    steps: 183    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 992   score: 3.0   memory length: 187685   epsilon: 0.8263817200037691    steps: 225    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 993   score: 1.0   memory length: 187854   epsilon: 0.8260471000037763    steps: 169    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 994   score: 6.0   memory length: 188208   epsilon: 0.8253461800037916    steps: 354    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 995   score: 0.0   memory length: 188331   epsilon: 0.8251026400037969    steps: 123    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 996   score: 0.0   memory length: 188453   epsilon: 0.8248610800038021    steps: 122    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 997   score: 5.0   memory length: 188782   epsilon: 0.8242096600038162    steps: 329    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 998   score: 2.0   memory length: 189002   epsilon: 0.8237740600038257    steps: 220    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 999   score: 3.0   memory length: 189213   epsilon: 0.8233562800038348    steps: 211    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1000   score: 5.0   memory length: 189555   epsilon: 0.8226791200038495    steps: 342    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1001   score: 2.0   memory length: 189773   epsilon: 0.8222474800038588    steps: 218    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1002   score: 3.0   memory length: 190001   epsilon: 0.8217960400038686    steps: 228    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1003   score: 2.0   memory length: 190181   epsilon: 0.8214396400038764    steps: 180    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1004   score: 1.0   memory length: 190350   epsilon: 0.8211050200038836    steps: 169    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 1005   score: 2.0   memory length: 190550   epsilon: 0.8207090200038922    steps: 200    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1006   score: 2.0   memory length: 190768   epsilon: 0.8202773800039016    steps: 218    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1007   score: 0.0   memory length: 190891   epsilon: 0.8200338400039069    steps: 123    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 1008   score: 2.0   memory length: 191109   epsilon: 0.8196022000039163    steps: 218    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 1009   score: 3.0   memory length: 191338   epsilon: 0.8191487800039261    steps: 229    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1010   score: 7.0   memory length: 191772   epsilon: 0.8182894600039448    steps: 434    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1011   score: 1.0   memory length: 191923   epsilon: 0.8179904800039512    steps: 151    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 1012   score: 3.0   memory length: 192172   epsilon: 0.817497460003962    steps: 249    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1013   score: 3.0   memory length: 192397   epsilon: 0.8170519600039716    steps: 225    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1014   score: 4.0   memory length: 192692   epsilon: 0.8164678600039843    steps: 295    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1015   score: 1.0   memory length: 192843   epsilon: 0.8161688800039908    steps: 151    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1016   score: 5.0   memory length: 193149   epsilon: 0.815563000004004    steps: 306    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1017   score: 2.0   memory length: 193367   epsilon: 0.8151313600040133    steps: 218    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1018   score: 1.0   memory length: 193518   epsilon: 0.8148323800040198    steps: 151    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1019   score: 3.0   memory length: 193748   epsilon: 0.8143769800040297    steps: 230    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1020   score: 0.0   memory length: 193871   epsilon: 0.814133440004035    steps: 123    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1021   score: 2.0   memory length: 194071   epsilon: 0.8137374400040436    steps: 200    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1022   score: 1.0   memory length: 194222   epsilon: 0.8134384600040501    steps: 151    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1023   score: 3.0   memory length: 194490   epsilon: 0.8129078200040616    steps: 268    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1024   score: 0.0   memory length: 194613   epsilon: 0.8126642800040669    steps: 123    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1025   score: 2.0   memory length: 194832   epsilon: 0.8122306600040763    steps: 219    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1026   score: 3.0   memory length: 195058   epsilon: 0.811783180004086    steps: 226    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1027   score: 3.0   memory length: 195285   epsilon: 0.8113337200040958    steps: 227    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1028   score: 0.0   memory length: 195408   epsilon: 0.811090180004101    steps: 123    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1029   score: 1.0   memory length: 195576   epsilon: 0.8107575400041083    steps: 168    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1030   score: 1.0   memory length: 195745   epsilon: 0.8104229200041155    steps: 169    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 1031   score: 2.0   memory length: 195943   epsilon: 0.810030880004124    steps: 198    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1032   score: 3.0   memory length: 196191   epsilon: 0.8095398400041347    steps: 248    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1033   score: 4.0   memory length: 196487   epsilon: 0.8089537600041474    steps: 296    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1034   score: 3.0   memory length: 196712   epsilon: 0.8085082600041571    steps: 225    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1035   score: 4.0   memory length: 196989   epsilon: 0.807959800004169    steps: 277    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1036   score: 1.0   memory length: 197140   epsilon: 0.8076608200041755    steps: 151    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1037   score: 1.0   memory length: 197309   epsilon: 0.8073262000041828    steps: 169    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1038   score: 2.0   memory length: 197506   epsilon: 0.8069361400041912    steps: 197    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1039   score: 4.0   memory length: 197765   epsilon: 0.8064233200042024    steps: 259    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1040   score: 4.0   memory length: 198019   epsilon: 0.8059204000042133    steps: 254    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1041   score: 3.0   memory length: 198282   epsilon: 0.8053996600042246    steps: 263    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1042   score: 1.0   memory length: 198451   epsilon: 0.8050650400042318    steps: 169    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1043   score: 2.0   memory length: 198649   epsilon: 0.8046730000042404    steps: 198    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1044   score: 7.0   memory length: 199040   epsilon: 0.8038988200042572    steps: 391    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1045   score: 2.0   memory length: 199240   epsilon: 0.8035028200042658    steps: 200    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1046   score: 4.0   memory length: 199521   epsilon: 0.8029464400042778    steps: 281    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 1047   score: 2.0   memory length: 199720   epsilon: 0.8025524200042864    steps: 199    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 1048   score: 4.0   memory length: 199998   epsilon: 0.8020019800042983    steps: 278    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1049   score: 3.0   memory length: 200244   epsilon: 0.8015149000043089    steps: 246    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1050   score: 5.0   memory length: 200569   epsilon: 0.8008714000043229    steps: 325    lr: 4e-05     evaluation reward: 2.5\n",
      "episode: 1051   score: 5.0   memory length: 200936   epsilon: 0.8001447400043387    steps: 367    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1052   score: 2.0   memory length: 201134   epsilon: 0.7997527000043472    steps: 198    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1053   score: 2.0   memory length: 201353   epsilon: 0.7993190800043566    steps: 219    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1054   score: 2.0   memory length: 201552   epsilon: 0.7989250600043651    steps: 199    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1055   score: 7.0   memory length: 201923   epsilon: 0.7981904800043811    steps: 371    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1056   score: 3.0   memory length: 202148   epsilon: 0.7977449800043908    steps: 225    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1057   score: 3.0   memory length: 202379   epsilon: 0.7972876000044007    steps: 231    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1058   score: 5.0   memory length: 202673   epsilon: 0.7967054800044133    steps: 294    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1059   score: 0.0   memory length: 202795   epsilon: 0.7964639200044186    steps: 122    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1060   score: 2.0   memory length: 203011   epsilon: 0.7960362400044279    steps: 216    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1061   score: 2.0   memory length: 203228   epsilon: 0.7956065800044372    steps: 217    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1062   score: 3.0   memory length: 203453   epsilon: 0.7951610800044469    steps: 225    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1063   score: 3.0   memory length: 203683   epsilon: 0.7947056800044567    steps: 230    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1064   score: 1.0   memory length: 203834   epsilon: 0.7944067000044632    steps: 151    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1065   score: 1.0   memory length: 203985   epsilon: 0.7941077200044697    steps: 151    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1066   score: 2.0   memory length: 204183   epsilon: 0.7937156800044782    steps: 198    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1067   score: 4.0   memory length: 204459   epsilon: 0.7931692000044901    steps: 276    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1068   score: 4.0   memory length: 204775   epsilon: 0.7925435200045037    steps: 316    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1069   score: 2.0   memory length: 204975   epsilon: 0.7921475200045123    steps: 200    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1070   score: 5.0   memory length: 205315   epsilon: 0.7914743200045269    steps: 340    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1071   score: 1.0   memory length: 205484   epsilon: 0.7911397000045342    steps: 169    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1072   score: 2.0   memory length: 205664   epsilon: 0.7907833000045419    steps: 180    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1073   score: 3.0   memory length: 205927   epsilon: 0.7902625600045532    steps: 263    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1074   score: 2.0   memory length: 206145   epsilon: 0.7898309200045626    steps: 218    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1075   score: 2.0   memory length: 206325   epsilon: 0.7894745200045703    steps: 180    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1076   score: 1.0   memory length: 206495   epsilon: 0.7891379200045776    steps: 170    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1077   score: 4.0   memory length: 206770   epsilon: 0.7885934200045894    steps: 275    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1078   score: 2.0   memory length: 206968   epsilon: 0.7882013800045979    steps: 198    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1079   score: 1.0   memory length: 207139   epsilon: 0.7878628000046053    steps: 171    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1080   score: 2.0   memory length: 207319   epsilon: 0.787506400004613    steps: 180    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1081   score: 2.0   memory length: 207537   epsilon: 0.7870747600046224    steps: 218    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1082   score: 5.0   memory length: 207857   epsilon: 0.7864411600046362    steps: 320    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1083   score: 3.0   memory length: 208085   epsilon: 0.785989720004646    steps: 228    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1084   score: 3.0   memory length: 208295   epsilon: 0.785573920004655    steps: 210    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1085   score: 2.0   memory length: 208494   epsilon: 0.7851799000046635    steps: 199    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1086   score: 1.0   memory length: 208645   epsilon: 0.78488092000467    steps: 151    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1087   score: 1.0   memory length: 208795   epsilon: 0.7845839200046765    steps: 150    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1088   score: 5.0   memory length: 209069   epsilon: 0.7840414000046882    steps: 274    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1089   score: 6.0   memory length: 209410   epsilon: 0.7833662200047029    steps: 341    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1090   score: 7.0   memory length: 209851   epsilon: 0.7824930400047219    steps: 441    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1091   score: 5.0   memory length: 210175   epsilon: 0.7818515200047358    steps: 324    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1092   score: 0.0   memory length: 210298   epsilon: 0.7816079800047411    steps: 123    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1093   score: 7.0   memory length: 210698   epsilon: 0.7808159800047583    steps: 400    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1094   score: 1.0   memory length: 210868   epsilon: 0.7804793800047656    steps: 170    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1095   score: 6.0   memory length: 211218   epsilon: 0.7797863800047806    steps: 350    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1096   score: 4.0   memory length: 211535   epsilon: 0.7791587200047942    steps: 317    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1097   score: 3.0   memory length: 211781   epsilon: 0.7786716400048048    steps: 246    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1098   score: 3.0   memory length: 212009   epsilon: 0.7782202000048146    steps: 228    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1099   score: 2.0   memory length: 212190   epsilon: 0.7778618200048224    steps: 181    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1100   score: 2.0   memory length: 212392   epsilon: 0.7774618600048311    steps: 202    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1101   score: 4.0   memory length: 212650   epsilon: 0.7769510200048422    steps: 258    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1102   score: 4.0   memory length: 212908   epsilon: 0.7764401800048533    steps: 258    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1103   score: 2.0   memory length: 213105   epsilon: 0.7760501200048617    steps: 197    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1104   score: 3.0   memory length: 213370   epsilon: 0.7755254200048731    steps: 265    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1105   score: 1.0   memory length: 213520   epsilon: 0.7752284200048796    steps: 150    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1106   score: 1.0   memory length: 213689   epsilon: 0.7748938000048868    steps: 169    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1107   score: 2.0   memory length: 213868   epsilon: 0.7745393800048945    steps: 179    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1108   score: 5.0   memory length: 214217   epsilon: 0.7738483600049095    steps: 349    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1109   score: 5.0   memory length: 214534   epsilon: 0.7732207000049232    steps: 317    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1110   score: 4.0   memory length: 214792   epsilon: 0.7727098600049342    steps: 258    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1111   score: 5.0   memory length: 215117   epsilon: 0.7720663600049482    steps: 325    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1112   score: 0.0   memory length: 215239   epsilon: 0.7718248000049535    steps: 122    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1113   score: 7.0   memory length: 215655   epsilon: 0.7710011200049713    steps: 416    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1114   score: 3.0   memory length: 215902   epsilon: 0.770512060004982    steps: 247    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1115   score: 2.0   memory length: 216082   epsilon: 0.7701556600049897    steps: 180    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1116   score: 2.0   memory length: 216280   epsilon: 0.7697636200049982    steps: 198    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1117   score: 9.0   memory length: 216773   epsilon: 0.7687874800050194    steps: 493    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1118   score: 11.0   memory length: 217223   epsilon: 0.7678964800050387    steps: 450    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1119   score: 3.0   memory length: 217469   epsilon: 0.7674094000050493    steps: 246    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1120   score: 1.0   memory length: 217619   epsilon: 0.7671124000050558    steps: 150    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1121   score: 3.0   memory length: 217845   epsilon: 0.7666649200050655    steps: 226    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1122   score: 4.0   memory length: 218122   epsilon: 0.7661164600050774    steps: 277    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1123   score: 3.0   memory length: 218349   epsilon: 0.7656670000050871    steps: 227    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1124   score: 3.0   memory length: 218612   epsilon: 0.7651462600050984    steps: 263    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1125   score: 2.0   memory length: 218810   epsilon: 0.764754220005107    steps: 198    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1126   score: 2.0   memory length: 219028   epsilon: 0.7643225800051163    steps: 218    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1127   score: 3.0   memory length: 219280   epsilon: 0.7638236200051272    steps: 252    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1128   score: 2.0   memory length: 219461   epsilon: 0.7634652400051349    steps: 181    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1129   score: 5.0   memory length: 219767   epsilon: 0.7628593600051481    steps: 306    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1130   score: 5.0   memory length: 220113   epsilon: 0.762174280005163    steps: 346    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1131   score: 1.0   memory length: 220284   epsilon: 0.7618357000051703    steps: 171    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1132   score: 2.0   memory length: 220466   epsilon: 0.7614753400051781    steps: 182    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1133   score: 4.0   memory length: 220763   epsilon: 0.7608872800051909    steps: 297    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1134   score: 1.0   memory length: 220932   epsilon: 0.7605526600051982    steps: 169    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1135   score: 3.0   memory length: 221200   epsilon: 0.7600220200052097    steps: 268    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1136   score: 5.0   memory length: 221546   epsilon: 0.7593369400052246    steps: 346    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1137   score: 0.0   memory length: 221669   epsilon: 0.7590934000052298    steps: 123    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1138   score: 5.0   memory length: 222014   epsilon: 0.7584103000052447    steps: 345    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1139   score: 3.0   memory length: 222259   epsilon: 0.7579252000052552    steps: 245    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1140   score: 0.0   memory length: 222382   epsilon: 0.7576816600052605    steps: 123    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1141   score: 6.0   memory length: 222750   epsilon: 0.7569530200052763    steps: 368    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1142   score: 4.0   memory length: 223026   epsilon: 0.7564065400052882    steps: 276    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1143   score: 3.0   memory length: 223270   epsilon: 0.7559234200052987    steps: 244    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1144   score: 2.0   memory length: 223471   epsilon: 0.7555254400053073    steps: 201    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1145   score: 3.0   memory length: 223718   epsilon: 0.7550363800053179    steps: 247    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1146   score: 4.0   memory length: 224020   epsilon: 0.7544384200053309    steps: 302    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1147   score: 3.0   memory length: 224288   epsilon: 0.7539077800053424    steps: 268    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1148   score: 1.0   memory length: 224440   epsilon: 0.753606820005349    steps: 152    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1149   score: 5.0   memory length: 224733   epsilon: 0.7530266800053615    steps: 293    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1150   score: 4.0   memory length: 225021   epsilon: 0.7524564400053739    steps: 288    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1151   score: 3.0   memory length: 225288   epsilon: 0.7519277800053854    steps: 267    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1152   score: 3.0   memory length: 225516   epsilon: 0.7514763400053952    steps: 228    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1153   score: 4.0   memory length: 225816   epsilon: 0.7508823400054081    steps: 300    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1154   score: 2.0   memory length: 226034   epsilon: 0.7504507000054175    steps: 218    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1155   score: 2.0   memory length: 226234   epsilon: 0.7500547000054261    steps: 200    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1156   score: 3.0   memory length: 226480   epsilon: 0.7495676200054366    steps: 246    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1157   score: 4.0   memory length: 226757   epsilon: 0.7490191600054485    steps: 277    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1158   score: 2.0   memory length: 226955   epsilon: 0.7486271200054571    steps: 198    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1159   score: 5.0   memory length: 227277   epsilon: 0.7479895600054709    steps: 322    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1160   score: 3.0   memory length: 227522   epsilon: 0.7475044600054814    steps: 245    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1161   score: 5.0   memory length: 227848   epsilon: 0.7468589800054954    steps: 326    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1162   score: 3.0   memory length: 228095   epsilon: 0.7463699200055061    steps: 247    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1163   score: 1.0   memory length: 228248   epsilon: 0.7460669800055126    steps: 153    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1164   score: 6.0   memory length: 228563   epsilon: 0.7454432800055262    steps: 315    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1165   score: 4.0   memory length: 228820   epsilon: 0.7449344200055372    steps: 257    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1166   score: 5.0   memory length: 229144   epsilon: 0.7442929000055512    steps: 324    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1167   score: 3.0   memory length: 229373   epsilon: 0.743839480005561    steps: 229    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1168   score: 2.0   memory length: 229591   epsilon: 0.7434078400055704    steps: 218    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1169   score: 3.0   memory length: 229837   epsilon: 0.7429207600055809    steps: 246    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1170   score: 5.0   memory length: 230143   epsilon: 0.7423148800055941    steps: 306    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1171   score: 1.0   memory length: 230315   epsilon: 0.7419743200056015    steps: 172    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1172   score: 2.0   memory length: 230497   epsilon: 0.7416139600056093    steps: 182    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1173   score: 6.0   memory length: 230827   epsilon: 0.7409605600056235    steps: 330    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1174   score: 3.0   memory length: 231071   epsilon: 0.740477440005634    steps: 244    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1175   score: 4.0   memory length: 231367   epsilon: 0.7398913600056467    steps: 296    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1176   score: 4.0   memory length: 231661   epsilon: 0.7393092400056593    steps: 294    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1177   score: 3.0   memory length: 231889   epsilon: 0.7388578000056691    steps: 228    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1178   score: 4.0   memory length: 232184   epsilon: 0.7382737000056818    steps: 295    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1179   score: 0.0   memory length: 232306   epsilon: 0.7380321400056871    steps: 122    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1180   score: 3.0   memory length: 232572   epsilon: 0.7375054600056985    steps: 266    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1181   score: 1.0   memory length: 232723   epsilon: 0.737206480005705    steps: 151    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1182   score: 2.0   memory length: 232905   epsilon: 0.7368461200057128    steps: 182    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1183   score: 3.0   memory length: 233151   epsilon: 0.7363590400057234    steps: 246    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1184   score: 3.0   memory length: 233394   epsilon: 0.7358779000057338    steps: 243    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1185   score: 0.0   memory length: 233517   epsilon: 0.7356343600057391    steps: 123    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1186   score: 0.0   memory length: 233640   epsilon: 0.7353908200057444    steps: 123    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1187   score: 2.0   memory length: 233860   epsilon: 0.7349552200057539    steps: 220    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1188   score: 1.0   memory length: 234029   epsilon: 0.7346206000057611    steps: 169    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1189   score: 2.0   memory length: 234227   epsilon: 0.7342285600057696    steps: 198    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1190   score: 2.0   memory length: 234425   epsilon: 0.7338365200057781    steps: 198    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1191   score: 2.0   memory length: 234607   epsilon: 0.733476160005786    steps: 182    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1192   score: 14.0   memory length: 235087   epsilon: 0.7325257600058066    steps: 480    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1193   score: 1.0   memory length: 235259   epsilon: 0.732185200005814    steps: 172    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1194   score: 3.0   memory length: 235505   epsilon: 0.7316981200058246    steps: 246    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1195   score: 3.0   memory length: 235752   epsilon: 0.7312090600058352    steps: 247    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1196   score: 6.0   memory length: 236100   epsilon: 0.7305200200058501    steps: 348    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1197   score: 4.0   memory length: 236378   epsilon: 0.7299695800058621    steps: 278    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1198   score: 2.0   memory length: 236578   epsilon: 0.7295735800058707    steps: 200    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1199   score: 5.0   memory length: 236906   epsilon: 0.7289241400058848    steps: 328    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1200   score: 0.0   memory length: 237029   epsilon: 0.7286806000058901    steps: 123    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1201   score: 3.0   memory length: 237275   epsilon: 0.7281935200059007    steps: 246    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1202   score: 2.0   memory length: 237473   epsilon: 0.7278014800059092    steps: 198    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1203   score: 4.0   memory length: 237768   epsilon: 0.7272173800059218    steps: 295    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1204   score: 3.0   memory length: 237999   epsilon: 0.7267600000059318    steps: 231    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1205   score: 3.0   memory length: 238225   epsilon: 0.7263125200059415    steps: 226    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1206   score: 1.0   memory length: 238396   epsilon: 0.7259739400059488    steps: 171    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1207   score: 3.0   memory length: 238622   epsilon: 0.7255264600059586    steps: 226    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1208   score: 3.0   memory length: 238869   epsilon: 0.7250374000059692    steps: 247    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1209   score: 3.0   memory length: 239094   epsilon: 0.7245919000059788    steps: 225    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1210   score: 5.0   memory length: 239441   epsilon: 0.7239048400059938    steps: 347    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1211   score: 3.0   memory length: 239707   epsilon: 0.7233781600060052    steps: 266    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1212   score: 1.0   memory length: 239858   epsilon: 0.7230791800060117    steps: 151    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1213   score: 3.0   memory length: 240104   epsilon: 0.7225921000060223    steps: 246    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1214   score: 4.0   memory length: 240383   epsilon: 0.7220396800060342    steps: 279    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1215   score: 7.0   memory length: 240780   epsilon: 0.7212536200060513    steps: 397    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1216   score: 4.0   memory length: 241039   epsilon: 0.7207408000060624    steps: 259    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1217   score: 4.0   memory length: 241333   epsilon: 0.7201586800060751    steps: 294    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1218   score: 7.0   memory length: 241775   epsilon: 0.7192835200060941    steps: 442    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1219   score: 3.0   memory length: 242021   epsilon: 0.7187964400061047    steps: 246    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1220   score: 5.0   memory length: 242345   epsilon: 0.7181549200061186    steps: 324    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1221   score: 5.0   memory length: 242667   epsilon: 0.7175173600061324    steps: 322    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1222   score: 0.0   memory length: 242789   epsilon: 0.7172758000061377    steps: 122    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1223   score: 6.0   memory length: 243161   epsilon: 0.7165392400061537    steps: 372    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1224   score: 3.0   memory length: 243390   epsilon: 0.7160858200061635    steps: 229    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1225   score: 0.0   memory length: 243513   epsilon: 0.7158422800061688    steps: 123    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1226   score: 2.0   memory length: 243711   epsilon: 0.7154502400061773    steps: 198    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1227   score: 4.0   memory length: 243988   epsilon: 0.7149017800061892    steps: 277    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1228   score: 4.0   memory length: 244260   epsilon: 0.7143632200062009    steps: 272    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1229   score: 2.0   memory length: 244476   epsilon: 0.7139355400062102    steps: 216    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1230   score: 1.0   memory length: 244645   epsilon: 0.7136009200062174    steps: 169    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1231   score: 3.0   memory length: 244874   epsilon: 0.7131475000062273    steps: 229    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1232   score: 8.0   memory length: 245273   epsilon: 0.7123574800062444    steps: 399    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1233   score: 4.0   memory length: 245535   epsilon: 0.7118387200062557    steps: 262    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1234   score: 1.0   memory length: 245706   epsilon: 0.711500140006263    steps: 171    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1235   score: 3.0   memory length: 245916   epsilon: 0.7110843400062721    steps: 210    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1236   score: 3.0   memory length: 246164   epsilon: 0.7105933000062827    steps: 248    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1237   score: 5.0   memory length: 246467   epsilon: 0.7099933600062958    steps: 303    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1238   score: 6.0   memory length: 246800   epsilon: 0.7093340200063101    steps: 333    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1239   score: 0.0   memory length: 246922   epsilon: 0.7090924600063153    steps: 122    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1240   score: 1.0   memory length: 247091   epsilon: 0.7087578400063226    steps: 169    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1241   score: 2.0   memory length: 247289   epsilon: 0.7083658000063311    steps: 198    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1242   score: 2.0   memory length: 247486   epsilon: 0.7079757400063396    steps: 197    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1243   score: 3.0   memory length: 247732   epsilon: 0.7074886600063501    steps: 246    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1244   score: 3.0   memory length: 247961   epsilon: 0.70703524000636    steps: 229    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1245   score: 0.0   memory length: 248084   epsilon: 0.7067917000063653    steps: 123    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1246   score: 2.0   memory length: 248281   epsilon: 0.7064016400063737    steps: 197    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1247   score: 1.0   memory length: 248452   epsilon: 0.7060630600063811    steps: 171    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1248   score: 4.0   memory length: 248732   epsilon: 0.7055086600063931    steps: 280    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1249   score: 2.0   memory length: 248929   epsilon: 0.7051186000064016    steps: 197    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1250   score: 4.0   memory length: 249208   epsilon: 0.7045661800064136    steps: 279    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1251   score: 5.0   memory length: 249552   epsilon: 0.7038850600064284    steps: 344    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1252   score: 4.0   memory length: 249803   epsilon: 0.7033880800064392    steps: 251    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1253   score: 4.0   memory length: 250077   epsilon: 0.7028455600064509    steps: 274    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1254   score: 0.0   memory length: 250200   epsilon: 0.7026020200064562    steps: 123    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1255   score: 0.0   memory length: 250323   epsilon: 0.7023584800064615    steps: 123    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1256   score: 3.0   memory length: 250532   epsilon: 0.7019446600064705    steps: 209    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1257   score: 3.0   memory length: 250761   epsilon: 0.7014912400064803    steps: 229    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1258   score: 5.0   memory length: 251042   epsilon: 0.7009348600064924    steps: 281    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1259   score: 5.0   memory length: 251335   epsilon: 0.700354720006505    steps: 293    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1260   score: 1.0   memory length: 251504   epsilon: 0.7000201000065123    steps: 169    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1261   score: 3.0   memory length: 251733   epsilon: 0.6995666800065221    steps: 229    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1262   score: 2.0   memory length: 251931   epsilon: 0.6991746400065306    steps: 198    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1263   score: 3.0   memory length: 252178   epsilon: 0.6986855800065412    steps: 247    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1264   score: 0.0   memory length: 252301   epsilon: 0.6984420400065465    steps: 123    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1265   score: 2.0   memory length: 252519   epsilon: 0.6980104000065559    steps: 218    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1266   score: 1.0   memory length: 252670   epsilon: 0.6977114200065624    steps: 151    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1267   score: 7.0   memory length: 253038   epsilon: 0.6969827800065782    steps: 368    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1268   score: 3.0   memory length: 253269   epsilon: 0.6965254000065881    steps: 231    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1269   score: 5.0   memory length: 253575   epsilon: 0.6959195200066013    steps: 306    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1270   score: 4.0   memory length: 253869   epsilon: 0.6953374000066139    steps: 294    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1271   score: 2.0   memory length: 254050   epsilon: 0.6949790200066217    steps: 181    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1272   score: 2.0   memory length: 254266   epsilon: 0.694551340006631    steps: 216    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1273   score: 4.0   memory length: 254522   epsilon: 0.694044460006642    steps: 256    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1274   score: 5.0   memory length: 254856   epsilon: 0.6933831400066564    steps: 334    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1275   score: 4.0   memory length: 255131   epsilon: 0.6928386400066682    steps: 275    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1276   score: 2.0   memory length: 255350   epsilon: 0.6924050200066776    steps: 219    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1277   score: 4.0   memory length: 255626   epsilon: 0.6918585400066894    steps: 276    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1278   score: 5.0   memory length: 255921   epsilon: 0.6912744400067021    steps: 295    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1279   score: 2.0   memory length: 256119   epsilon: 0.6908824000067106    steps: 198    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1280   score: 3.0   memory length: 256364   epsilon: 0.6903973000067212    steps: 245    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1281   score: 3.0   memory length: 256593   epsilon: 0.689943880006731    steps: 229    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1282   score: 2.0   memory length: 256811   epsilon: 0.6895122400067404    steps: 218    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1283   score: 3.0   memory length: 257057   epsilon: 0.689025160006751    steps: 246    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1284   score: 2.0   memory length: 257255   epsilon: 0.6886331200067595    steps: 198    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1285   score: 6.0   memory length: 257639   epsilon: 0.687872800006776    steps: 384    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1286   score: 3.0   memory length: 257884   epsilon: 0.6873877000067865    steps: 245    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1287   score: 4.0   memory length: 258161   epsilon: 0.6868392400067984    steps: 277    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1288   score: 9.0   memory length: 258494   epsilon: 0.6861799000068127    steps: 333    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1289   score: 4.0   memory length: 258790   epsilon: 0.6855938200068254    steps: 296    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1290   score: 3.0   memory length: 259018   epsilon: 0.6851423800068352    steps: 228    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1291   score: 3.0   memory length: 259244   epsilon: 0.684694900006845    steps: 226    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1292   score: 4.0   memory length: 259499   epsilon: 0.6841900000068559    steps: 255    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1293   score: 3.0   memory length: 259745   epsilon: 0.6837029200068665    steps: 246    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1294   score: 5.0   memory length: 260050   epsilon: 0.6830990200068796    steps: 305    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1295   score: 4.0   memory length: 260349   epsilon: 0.6825070000068925    steps: 299    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1296   score: 3.0   memory length: 260574   epsilon: 0.6820615000069021    steps: 225    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1297   score: 5.0   memory length: 260900   epsilon: 0.6814160200069161    steps: 326    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1298   score: 4.0   memory length: 261177   epsilon: 0.680867560006928    steps: 277    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1299   score: 3.0   memory length: 261403   epsilon: 0.6804200800069378    steps: 226    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1300   score: 6.0   memory length: 261759   epsilon: 0.6797152000069531    steps: 356    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1301   score: 3.0   memory length: 261969   epsilon: 0.6792994000069621    steps: 210    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1302   score: 4.0   memory length: 262247   epsilon: 0.678748960006974    steps: 278    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1303   score: 5.0   memory length: 262568   epsilon: 0.6781133800069878    steps: 321    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1304   score: 3.0   memory length: 262817   epsilon: 0.6776203600069985    steps: 249    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1305   score: 4.0   memory length: 263071   epsilon: 0.6771174400070095    steps: 254    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1306   score: 3.0   memory length: 263298   epsilon: 0.6766679800070192    steps: 227    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1307   score: 3.0   memory length: 263541   epsilon: 0.6761868400070297    steps: 243    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1308   score: 3.0   memory length: 263768   epsilon: 0.6757373800070394    steps: 227    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1309   score: 4.0   memory length: 264043   epsilon: 0.6751928800070512    steps: 275    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1310   score: 6.0   memory length: 264407   epsilon: 0.6744721600070669    steps: 364    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1311   score: 1.0   memory length: 264557   epsilon: 0.6741751600070733    steps: 150    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1312   score: 2.0   memory length: 264755   epsilon: 0.6737831200070818    steps: 198    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1313   score: 4.0   memory length: 265053   epsilon: 0.6731930800070947    steps: 298    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1314   score: 3.0   memory length: 265279   epsilon: 0.6727456000071044    steps: 226    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1315   score: 8.0   memory length: 265678   epsilon: 0.6719555800071215    steps: 399    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1316   score: 5.0   memory length: 266003   epsilon: 0.6713120800071355    steps: 325    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1317   score: 3.0   memory length: 266250   epsilon: 0.6708230200071461    steps: 247    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1318   score: 2.0   memory length: 266448   epsilon: 0.6704309800071546    steps: 198    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1319   score: 3.0   memory length: 266657   epsilon: 0.6700171600071636    steps: 209    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1320   score: 6.0   memory length: 267005   epsilon: 0.6693281200071786    steps: 348    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1321   score: 2.0   memory length: 267223   epsilon: 0.6688964800071879    steps: 218    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1322   score: 6.0   memory length: 267579   epsilon: 0.6681916000072032    steps: 356    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1323   score: 3.0   memory length: 267806   epsilon: 0.667742140007213    steps: 227    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1324   score: 2.0   memory length: 268025   epsilon: 0.6673085200072224    steps: 219    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1325   score: 5.0   memory length: 268332   epsilon: 0.6667006600072356    steps: 307    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1326   score: 6.0   memory length: 268689   epsilon: 0.665993800007251    steps: 357    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1327   score: 3.0   memory length: 268915   epsilon: 0.6655463200072607    steps: 226    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1328   score: 5.0   memory length: 269200   epsilon: 0.6649820200072729    steps: 285    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1329   score: 3.0   memory length: 269429   epsilon: 0.6645286000072828    steps: 229    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1330   score: 5.0   memory length: 269769   epsilon: 0.6638554000072974    steps: 340    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1331   score: 5.0   memory length: 270113   epsilon: 0.6631742800073122    steps: 344    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1332   score: 4.0   memory length: 270410   epsilon: 0.6625862200073249    steps: 297    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1333   score: 3.0   memory length: 270674   epsilon: 0.6620635000073363    steps: 264    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1334   score: 5.0   memory length: 271018   epsilon: 0.6613823800073511    steps: 344    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1335   score: 6.0   memory length: 271372   epsilon: 0.6606814600073663    steps: 354    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1336   score: 1.0   memory length: 271523   epsilon: 0.6603824800073728    steps: 151    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1337   score: 3.0   memory length: 271771   epsilon: 0.6598914400073834    steps: 248    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1338   score: 3.0   memory length: 272018   epsilon: 0.659402380007394    steps: 247    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1339   score: 5.0   memory length: 272364   epsilon: 0.6587173000074089    steps: 346    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1340   score: 4.0   memory length: 272604   epsilon: 0.6582421000074192    steps: 240    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1341   score: 6.0   memory length: 272956   epsilon: 0.6575451400074344    steps: 352    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1342   score: 5.0   memory length: 273265   epsilon: 0.6569333200074476    steps: 309    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1343   score: 5.0   memory length: 273569   epsilon: 0.6563314000074607    steps: 304    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1344   score: 7.0   memory length: 273996   epsilon: 0.6554859400074791    steps: 427    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1345   score: 4.0   memory length: 274237   epsilon: 0.6550087600074894    steps: 241    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1346   score: 4.0   memory length: 274532   epsilon: 0.6544246600075021    steps: 295    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1347   score: 2.0   memory length: 274732   epsilon: 0.6540286600075107    steps: 200    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1348   score: 7.0   memory length: 275110   epsilon: 0.653280220007527    steps: 378    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1349   score: 3.0   memory length: 275336   epsilon: 0.6528327400075367    steps: 226    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1350   score: 3.0   memory length: 275579   epsilon: 0.6523516000075471    steps: 243    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1351   score: 5.0   memory length: 275878   epsilon: 0.65175958000756    steps: 299    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1352   score: 0.0   memory length: 276000   epsilon: 0.6515180200075652    steps: 122    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1353   score: 3.0   memory length: 276246   epsilon: 0.6510309400075758    steps: 246    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1354   score: 3.0   memory length: 276475   epsilon: 0.6505775200075856    steps: 229    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1355   score: 7.0   memory length: 276864   epsilon: 0.6498073000076023    steps: 389    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1356   score: 2.0   memory length: 277062   epsilon: 0.6494152600076109    steps: 198    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1357   score: 3.0   memory length: 277305   epsilon: 0.6489341200076213    steps: 243    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1358   score: 3.0   memory length: 277515   epsilon: 0.6485183200076303    steps: 210    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1359   score: 2.0   memory length: 277734   epsilon: 0.6480847000076397    steps: 219    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1360   score: 6.0   memory length: 278080   epsilon: 0.6473996200076546    steps: 346    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1361   score: 3.0   memory length: 278309   epsilon: 0.6469462000076645    steps: 229    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1362   score: 3.0   memory length: 278520   epsilon: 0.6465284200076735    steps: 211    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1363   score: 2.0   memory length: 278741   epsilon: 0.646090840007683    steps: 221    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1364   score: 5.0   memory length: 279032   epsilon: 0.6455146600076955    steps: 291    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1365   score: 5.0   memory length: 279336   epsilon: 0.6449127400077086    steps: 304    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1366   score: 8.0   memory length: 279821   epsilon: 0.6439524400077294    steps: 485    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1367   score: 5.0   memory length: 280110   epsilon: 0.6433802200077419    steps: 289    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1368   score: 0.0   memory length: 280233   epsilon: 0.6431366800077472    steps: 123    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1369   score: 3.0   memory length: 280479   epsilon: 0.6426496000077577    steps: 246    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1370   score: 5.0   memory length: 280805   epsilon: 0.6420041200077717    steps: 326    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1371   score: 4.0   memory length: 281064   epsilon: 0.6414913000077829    steps: 259    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1372   score: 3.0   memory length: 281277   epsilon: 0.641069560007792    steps: 213    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1373   score: 4.0   memory length: 281550   epsilon: 0.6405290200078038    steps: 273    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1374   score: 7.0   memory length: 281966   epsilon: 0.6397053400078216    steps: 416    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1375   score: 2.0   memory length: 282148   epsilon: 0.6393449800078295    steps: 182    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1376   score: 2.0   memory length: 282346   epsilon: 0.638952940007838    steps: 198    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1377   score: 9.0   memory length: 282865   epsilon: 0.6379253200078603    steps: 519    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1378   score: 5.0   memory length: 283171   epsilon: 0.6373194400078734    steps: 306    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1379   score: 2.0   memory length: 283353   epsilon: 0.6369590800078813    steps: 182    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1380   score: 5.0   memory length: 283642   epsilon: 0.6363868600078937    steps: 289    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1381   score: 3.0   memory length: 283888   epsilon: 0.6358997800079043    steps: 246    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1382   score: 12.0   memory length: 284345   epsilon: 0.6349949200079239    steps: 457    lr: 4e-05     evaluation reward: 4.05\n",
      "episode: 1383   score: 3.0   memory length: 284595   epsilon: 0.6344999200079346    steps: 250    lr: 4e-05     evaluation reward: 4.05\n",
      "episode: 1384   score: 3.0   memory length: 284821   epsilon: 0.6340524400079444    steps: 226    lr: 4e-05     evaluation reward: 4.06\n",
      "episode: 1385   score: 4.0   memory length: 285114   epsilon: 0.633472300007957    steps: 293    lr: 4e-05     evaluation reward: 4.04\n",
      "episode: 1386   score: 5.0   memory length: 285450   epsilon: 0.6328070200079714    steps: 336    lr: 4e-05     evaluation reward: 4.06\n",
      "episode: 1387   score: 5.0   memory length: 285765   epsilon: 0.6321833200079849    steps: 315    lr: 4e-05     evaluation reward: 4.07\n",
      "episode: 1388   score: 4.0   memory length: 286061   epsilon: 0.6315972400079977    steps: 296    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1389   score: 4.0   memory length: 286301   epsilon: 0.631122040008008    steps: 240    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1390   score: 3.0   memory length: 286547   epsilon: 0.6306349600080186    steps: 246    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1391   score: 1.0   memory length: 286698   epsilon: 0.630335980008025    steps: 151    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1392   score: 1.0   memory length: 286848   epsilon: 0.6300389800080315    steps: 150    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1393   score: 6.0   memory length: 287211   epsilon: 0.6293202400080471    steps: 363    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1394   score: 5.0   memory length: 287539   epsilon: 0.6286708000080612    steps: 328    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1395   score: 3.0   memory length: 287769   epsilon: 0.6282154000080711    steps: 230    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1396   score: 1.0   memory length: 287920   epsilon: 0.6279164200080776    steps: 151    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1397   score: 2.0   memory length: 288118   epsilon: 0.6275243800080861    steps: 198    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1398   score: 0.0   memory length: 288240   epsilon: 0.6272828200080913    steps: 122    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1399   score: 5.0   memory length: 288563   epsilon: 0.6266432800081052    steps: 323    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1400   score: 5.0   memory length: 288884   epsilon: 0.626007700008119    steps: 321    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1401   score: 3.0   memory length: 289111   epsilon: 0.6255582400081288    steps: 227    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1402   score: 6.0   memory length: 289471   epsilon: 0.6248454400081442    steps: 360    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1403   score: 4.0   memory length: 289730   epsilon: 0.6243326200081554    steps: 259    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1404   score: 3.0   memory length: 289956   epsilon: 0.6238851400081651    steps: 226    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1405   score: 5.0   memory length: 290300   epsilon: 0.6232040200081799    steps: 344    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1406   score: 2.0   memory length: 290518   epsilon: 0.6227723800081892    steps: 218    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1407   score: 2.0   memory length: 290697   epsilon: 0.6224179600081969    steps: 179    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1408   score: 2.0   memory length: 290913   epsilon: 0.6219902800082062    steps: 216    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1409   score: 5.0   memory length: 291253   epsilon: 0.6213170800082208    steps: 340    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1410   score: 4.0   memory length: 291531   epsilon: 0.6207666400082328    steps: 278    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1411   score: 4.0   memory length: 291806   epsilon: 0.6202221400082446    steps: 275    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1412   score: 4.0   memory length: 292119   epsilon: 0.6196024000082581    steps: 313    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1413   score: 6.0   memory length: 292489   epsilon: 0.618869800008274    steps: 370    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1414   score: 3.0   memory length: 292735   epsilon: 0.6183827200082845    steps: 246    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1415   score: 3.0   memory length: 292982   epsilon: 0.6178936600082952    steps: 247    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1416   score: 2.0   memory length: 293179   epsilon: 0.6175036000083036    steps: 197    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1417   score: 2.0   memory length: 293377   epsilon: 0.6171115600083121    steps: 198    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1418   score: 3.0   memory length: 293609   epsilon: 0.6166522000083221    steps: 232    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1419   score: 5.0   memory length: 293882   epsilon: 0.6161116600083338    steps: 273    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1420   score: 4.0   memory length: 294142   epsilon: 0.615596860008345    steps: 260    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1421   score: 6.0   memory length: 294497   epsilon: 0.6148939600083603    steps: 355    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1422   score: 3.0   memory length: 294725   epsilon: 0.6144425200083701    steps: 228    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1423   score: 5.0   memory length: 295053   epsilon: 0.6137930800083842    steps: 328    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1424   score: 1.0   memory length: 295224   epsilon: 0.6134545000083915    steps: 171    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1425   score: 3.0   memory length: 295454   epsilon: 0.6129991000084014    steps: 230    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1426   score: 6.0   memory length: 295827   epsilon: 0.6122605600084174    steps: 373    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1427   score: 2.0   memory length: 296045   epsilon: 0.6118289200084268    steps: 218    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1428   score: 2.0   memory length: 296263   epsilon: 0.6113972800084362    steps: 218    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1429   score: 3.0   memory length: 296493   epsilon: 0.6109418800084461    steps: 230    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1430   score: 10.0   memory length: 296994   epsilon: 0.6099499000084676    steps: 501    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1431   score: 5.0   memory length: 297338   epsilon: 0.6092687800084824    steps: 344    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1432   score: 3.0   memory length: 297549   epsilon: 0.6088510000084915    steps: 211    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1433   score: 3.0   memory length: 297778   epsilon: 0.6083975800085013    steps: 229    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1434   score: 3.0   memory length: 298025   epsilon: 0.6079085200085119    steps: 247    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1435   score: 5.0   memory length: 298334   epsilon: 0.6072967000085252    steps: 309    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1436   score: 3.0   memory length: 298545   epsilon: 0.6068789200085343    steps: 211    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1437   score: 4.0   memory length: 298802   epsilon: 0.6063700600085453    steps: 257    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1438   score: 6.0   memory length: 299161   epsilon: 0.6056592400085608    steps: 359    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1439   score: 6.0   memory length: 299479   epsilon: 0.6050296000085744    steps: 318    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1440   score: 2.0   memory length: 299676   epsilon: 0.6046395400085829    steps: 197    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1441   score: 7.0   memory length: 300070   epsilon: 0.6038594200085998    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1442   score: 4.0   memory length: 300313   epsilon: 0.6033782800086103    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1443   score: 4.0   memory length: 300588   epsilon: 0.6028337800086221    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1444   score: 6.0   memory length: 300948   epsilon: 0.6021209800086376    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1445   score: 3.0   memory length: 301178   epsilon: 0.6016655800086474    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1446   score: 5.0   memory length: 301484   epsilon: 0.6010597000086606    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1447   score: 2.0   memory length: 301687   epsilon: 0.6006577600086693    steps: 203    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1448   score: 3.0   memory length: 301918   epsilon: 0.6002003800086793    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1449   score: 5.0   memory length: 302246   epsilon: 0.5995509400086934    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1450   score: 4.0   memory length: 302525   epsilon: 0.5989985200087053    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1451   score: 1.0   memory length: 302695   epsilon: 0.5986619200087127    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1452   score: 2.0   memory length: 302913   epsilon: 0.598230280008722    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1453   score: 3.0   memory length: 303158   epsilon: 0.5977451800087326    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1454   score: 4.0   memory length: 303435   epsilon: 0.5971967200087445    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1455   score: 3.0   memory length: 303697   epsilon: 0.5966779600087557    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1456   score: 9.0   memory length: 304114   epsilon: 0.5958523000087736    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1457   score: 6.0   memory length: 304509   epsilon: 0.5950702000087906    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1458   score: 3.0   memory length: 304722   epsilon: 0.5946484600087998    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1459   score: 1.0   memory length: 304873   epsilon: 0.5943494800088063    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1460   score: 9.0   memory length: 305339   epsilon: 0.5934268000088263    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1461   score: 9.0   memory length: 305830   epsilon: 0.5924546200088474    steps: 491    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1462   score: 6.0   memory length: 306191   epsilon: 0.5917398400088629    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1463   score: 5.0   memory length: 306498   epsilon: 0.5911319800088761    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1464   score: 9.0   memory length: 306960   epsilon: 0.590217220008896    steps: 462    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1465   score: 3.0   memory length: 307171   epsilon: 0.589799440008905    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1466   score: 3.0   memory length: 307399   epsilon: 0.5893480000089149    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1467   score: 4.0   memory length: 307677   epsilon: 0.5887975600089268    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1468   score: 5.0   memory length: 307946   epsilon: 0.5882649400089384    steps: 269    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1469   score: 5.0   memory length: 308312   epsilon: 0.5875402600089541    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1470   score: 0.0   memory length: 308435   epsilon: 0.5872967200089594    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1471   score: 4.0   memory length: 308714   epsilon: 0.5867443000089714    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1472   score: 6.0   memory length: 309055   epsilon: 0.586069120008986    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1473   score: 4.0   memory length: 309331   epsilon: 0.5855226400089979    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1474   score: 5.0   memory length: 309638   epsilon: 0.5849147800090111    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1475   score: 3.0   memory length: 309869   epsilon: 0.584457400009021    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1476   score: 5.0   memory length: 310155   epsilon: 0.5838911200090333    steps: 286    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1477   score: 4.0   memory length: 310429   epsilon: 0.5833486000090451    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1478   score: 6.0   memory length: 310803   epsilon: 0.5826080800090612    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1479   score: 4.0   memory length: 311062   epsilon: 0.5820952600090723    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1480   score: 9.0   memory length: 311517   epsilon: 0.5811943600090919    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 4.11\n",
      "episode: 1481   score: 7.0   memory length: 311921   epsilon: 0.5803944400091092    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1482   score: 4.0   memory length: 312182   epsilon: 0.5798776600091204    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1483   score: 1.0   memory length: 312352   epsilon: 0.5795410600091278    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1484   score: 4.0   memory length: 312613   epsilon: 0.579024280009139    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1485   score: 5.0   memory length: 312905   epsilon: 0.5784461200091515    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1486   score: 8.0   memory length: 313308   epsilon: 0.5776481800091688    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1487   score: 8.0   memory length: 313747   epsilon: 0.5767789600091877    steps: 439    lr: 1.6000000000000003e-05     evaluation reward: 4.13\n",
      "episode: 1488   score: 5.0   memory length: 314072   epsilon: 0.5761354600092017    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1489   score: 5.0   memory length: 314395   epsilon: 0.5754959200092156    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1490   score: 6.0   memory length: 314755   epsilon: 0.574783120009231    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 4.18\n",
      "episode: 1491   score: 4.0   memory length: 315049   epsilon: 0.5742010000092437    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1492   score: 2.0   memory length: 315246   epsilon: 0.5738109400092521    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1493   score: 6.0   memory length: 315584   epsilon: 0.5731417000092667    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1494   score: 4.0   memory length: 315843   epsilon: 0.5726288800092778    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1495   score: 3.0   memory length: 316069   epsilon: 0.5721814000092875    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1496   score: 3.0   memory length: 316300   epsilon: 0.5717240200092975    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1497   score: 1.0   memory length: 316451   epsilon: 0.5714250400093039    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1498   score: 3.0   memory length: 316699   epsilon: 0.5709340000093146    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1499   score: 3.0   memory length: 316945   epsilon: 0.5704469200093252    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1500   score: 2.0   memory length: 317146   epsilon: 0.5700489400093338    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1501   score: 3.0   memory length: 317359   epsilon: 0.569627200009343    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1502   score: 1.0   memory length: 317510   epsilon: 0.5693282200093495    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1503   score: 3.0   memory length: 317720   epsilon: 0.5689124200093585    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1504   score: 5.0   memory length: 318048   epsilon: 0.5682629800093726    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1505   score: 4.0   memory length: 318324   epsilon: 0.5677165000093845    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1506   score: 6.0   memory length: 318650   epsilon: 0.5670710200093985    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1507   score: 6.0   memory length: 318985   epsilon: 0.5664077200094129    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1508   score: 3.0   memory length: 319231   epsilon: 0.5659206400094234    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1509   score: 3.0   memory length: 319457   epsilon: 0.5654731600094332    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1510   score: 4.0   memory length: 319750   epsilon: 0.5648930200094457    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1511   score: 5.0   memory length: 320074   epsilon: 0.5642515000094597    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1512   score: 3.0   memory length: 320303   epsilon: 0.5637980800094695    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1513   score: 4.0   memory length: 320581   epsilon: 0.5632476400094815    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1514   score: 2.0   memory length: 320762   epsilon: 0.5628892600094892    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1515   score: 6.0   memory length: 321126   epsilon: 0.5621685400095049    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1516   score: 7.0   memory length: 321553   epsilon: 0.5613230800095232    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1517   score: 4.0   memory length: 321846   epsilon: 0.5607429400095358    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1518   score: 3.0   memory length: 322075   epsilon: 0.5602895200095457    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1519   score: 5.0   memory length: 322423   epsilon: 0.5596004800095606    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1520   score: 5.0   memory length: 322747   epsilon: 0.5589589600095746    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1521   score: 3.0   memory length: 322973   epsilon: 0.5585114800095843    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1522   score: 6.0   memory length: 323343   epsilon: 0.5577788800096002    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1523   score: 4.0   memory length: 323602   epsilon: 0.5572660600096113    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1524   score: 6.0   memory length: 323945   epsilon: 0.5565869200096261    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1525   score: 5.0   memory length: 324266   epsilon: 0.5559513400096399    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 4.36\n",
      "episode: 1526   score: 5.0   memory length: 324613   epsilon: 0.5552642800096548    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
      "episode: 1527   score: 6.0   memory length: 324965   epsilon: 0.5545673200096699    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1528   score: 5.0   memory length: 325272   epsilon: 0.5539594600096831    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1529   score: 6.0   memory length: 325627   epsilon: 0.5532565600096984    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1530   score: 3.0   memory length: 325873   epsilon: 0.5527694800097089    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1531   score: 5.0   memory length: 326195   epsilon: 0.5521319200097228    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1532   score: 2.0   memory length: 326376   epsilon: 0.5517735400097306    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1533   score: 1.0   memory length: 326527   epsilon: 0.551474560009737    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
      "episode: 1534   score: 6.0   memory length: 326880   epsilon: 0.5507756200097522    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1535   score: 5.0   memory length: 327184   epsilon: 0.5501737000097653    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1536   score: 6.0   memory length: 327574   epsilon: 0.549401500009782    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
      "episode: 1537   score: 5.0   memory length: 327861   epsilon: 0.5488332400097944    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1538   score: 5.0   memory length: 328185   epsilon: 0.5481917200098083    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
      "episode: 1539   score: 5.0   memory length: 328517   epsilon: 0.5475343600098226    steps: 332    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1540   score: 4.0   memory length: 328816   epsilon: 0.5469423400098354    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1541   score: 7.0   memory length: 329239   epsilon: 0.5461048000098536    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1542   score: 2.0   memory length: 329460   epsilon: 0.5456672200098631    steps: 221    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1543   score: 4.0   memory length: 329776   epsilon: 0.5450415400098767    steps: 316    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1544   score: 5.0   memory length: 330085   epsilon: 0.54442972000989    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1545   score: 2.0   memory length: 330283   epsilon: 0.5440376800098985    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1546   score: 4.0   memory length: 330543   epsilon: 0.5435228800099097    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1547   score: 2.0   memory length: 330742   epsilon: 0.5431288600099182    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1548   score: 3.0   memory length: 330988   epsilon: 0.5426417800099288    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1549   score: 5.0   memory length: 331300   epsilon: 0.5420240200099422    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1550   score: 6.0   memory length: 331650   epsilon: 0.5413310200099573    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1551   score: 5.0   memory length: 331941   epsilon: 0.5407548400099698    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
      "episode: 1552   score: 6.0   memory length: 332312   epsilon: 0.5400202600099857    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1553   score: 7.0   memory length: 332736   epsilon: 0.5391807400100039    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
      "episode: 1554   score: 6.0   memory length: 333131   epsilon: 0.5383986400100209    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
      "episode: 1555   score: 8.0   memory length: 333572   epsilon: 0.5375254600100399    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1556   score: 3.0   memory length: 333803   epsilon: 0.5370680800100498    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1557   score: 5.0   memory length: 334119   epsilon: 0.5364424000100634    steps: 316    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
      "episode: 1558   score: 5.0   memory length: 334461   epsilon: 0.5357652400100781    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
      "episode: 1559   score: 2.0   memory length: 334661   epsilon: 0.5353692400100867    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.54\n",
      "episode: 1560   score: 5.0   memory length: 334985   epsilon: 0.5347277200101006    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1561   score: 1.0   memory length: 335136   epsilon: 0.5344287400101071    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1562   score: 7.0   memory length: 335546   epsilon: 0.5336169400101247    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
      "episode: 1563   score: 8.0   memory length: 335949   epsilon: 0.532819000010142    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1564   score: 7.0   memory length: 336353   epsilon: 0.5320190800101594    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 4.44\n",
      "episode: 1565   score: 8.0   memory length: 336743   epsilon: 0.5312468800101762    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1566   score: 3.0   memory length: 336970   epsilon: 0.5307974200101859    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1567   score: 4.0   memory length: 337227   epsilon: 0.530288560010197    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1568   score: 6.0   memory length: 337601   epsilon: 0.529548040010213    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1569   score: 5.0   memory length: 337905   epsilon: 0.5289461200102261    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1570   score: 6.0   memory length: 338255   epsilon: 0.5282531200102412    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
      "episode: 1571   score: 5.0   memory length: 338577   epsilon: 0.527615560010255    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 4.57\n",
      "episode: 1572   score: 8.0   memory length: 339031   epsilon: 0.5267166400102745    steps: 454    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1573   score: 7.0   memory length: 339450   epsilon: 0.5258870200102925    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1574   score: 8.0   memory length: 339884   epsilon: 0.5250277000103112    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1575   score: 4.0   memory length: 340143   epsilon: 0.5245148800103223    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1576   score: 3.0   memory length: 340369   epsilon: 0.524067400010332    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1577   score: 5.0   memory length: 340709   epsilon: 0.5233942000103466    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1578   score: 2.0   memory length: 340907   epsilon: 0.5230021600103552    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1579   score: 8.0   memory length: 341349   epsilon: 0.5221270000103742    steps: 442    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1580   score: 3.0   memory length: 341596   epsilon: 0.5216379400103848    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1581   score: 4.0   memory length: 341876   epsilon: 0.5210835400103968    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 4.56\n",
      "episode: 1582   score: 3.0   memory length: 342105   epsilon: 0.5206301200104066    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
      "episode: 1583   score: 6.0   memory length: 342446   epsilon: 0.5199549400104213    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1584   score: 3.0   memory length: 342712   epsilon: 0.5194282600104327    steps: 266    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1585   score: 4.0   memory length: 342990   epsilon: 0.5188778200104447    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1586   score: 3.0   memory length: 343220   epsilon: 0.5184224200104546    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.53\n",
      "episode: 1587   score: 3.0   memory length: 343433   epsilon: 0.5180006800104637    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.48\n",
      "episode: 1588   score: 6.0   memory length: 343774   epsilon: 0.5173255000104784    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1589   score: 4.0   memory length: 344055   epsilon: 0.5167691200104905    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 4.48\n",
      "episode: 1590   score: 3.0   memory length: 344299   epsilon: 0.516286000010501    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1591   score: 6.0   memory length: 344693   epsilon: 0.5155058800105179    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1592   score: 6.0   memory length: 345113   epsilon: 0.514674280010536    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
      "episode: 1593   score: 4.0   memory length: 345370   epsilon: 0.514165420010547    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1594   score: 2.0   memory length: 345568   epsilon: 0.5137733800105555    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1595   score: 3.0   memory length: 345781   epsilon: 0.5133516400105647    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1596   score: 5.0   memory length: 346106   epsilon: 0.5127081400105786    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1597   score: 7.0   memory length: 346528   epsilon: 0.5118725800105968    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
      "episode: 1598   score: 6.0   memory length: 346890   epsilon: 0.5111558200106123    steps: 362    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1599   score: 5.0   memory length: 347198   epsilon: 0.5105459800106256    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1600   score: 10.0   memory length: 347573   epsilon: 0.5098034800106417    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1601   score: 5.0   memory length: 347877   epsilon: 0.5092015600106548    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1602   score: 2.0   memory length: 348056   epsilon: 0.5088471400106624    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1603   score: 5.0   memory length: 348401   epsilon: 0.5081640400106773    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1604   score: 4.0   memory length: 348680   epsilon: 0.5076116200106893    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1605   score: 1.0   memory length: 348831   epsilon: 0.5073126400106958    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1606   score: 9.0   memory length: 349379   epsilon: 0.5062276000107193    steps: 548    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1607   score: 9.0   memory length: 349900   epsilon: 0.5051960200107417    steps: 521    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1608   score: 3.0   memory length: 350127   epsilon: 0.5047465600107515    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1609   score: 5.0   memory length: 350445   epsilon: 0.5041169200107651    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1610   score: 7.0   memory length: 350852   epsilon: 0.5033110600107826    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1611   score: 1.0   memory length: 351021   epsilon: 0.5029764400107899    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1612   score: 4.0   memory length: 351321   epsilon: 0.5023824400108028    steps: 300    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1613   score: 4.0   memory length: 351599   epsilon: 0.5018320000108147    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1614   score: 7.0   memory length: 351973   epsilon: 0.5010914800108308    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
      "episode: 1615   score: 3.0   memory length: 352186   epsilon: 0.50066974001084    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1616   score: 5.0   memory length: 352528   epsilon: 0.49999258001085445    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1617   score: 5.0   memory length: 352849   epsilon: 0.4993570000108504    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 4.78\n",
      "episode: 1618   score: 5.0   memory length: 353173   epsilon: 0.49871548001084637    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1619   score: 5.0   memory length: 353496   epsilon: 0.4980759400108423    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1620   score: 4.0   memory length: 353778   epsilon: 0.4975175800108388    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1621   score: 3.0   memory length: 354023   epsilon: 0.4970324800108357    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1622   score: 5.0   memory length: 354327   epsilon: 0.4964305600108319    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.78\n",
      "episode: 1623   score: 6.0   memory length: 354662   epsilon: 0.4957672600108277    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1624   score: 2.0   memory length: 354860   epsilon: 0.49537522001082523    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1625   score: 7.0   memory length: 355269   epsilon: 0.4945654000108201    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 4.78\n",
      "episode: 1626   score: 2.0   memory length: 355466   epsilon: 0.49417534001081764    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1627   score: 4.0   memory length: 355741   epsilon: 0.4936308400108142    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1628   score: 5.0   memory length: 356069   epsilon: 0.4929814000108101    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1629   score: 5.0   memory length: 356393   epsilon: 0.49233988001080603    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1630   score: 4.0   memory length: 356650   epsilon: 0.4918310200108028    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1631   score: 5.0   memory length: 356990   epsilon: 0.49115782001079855    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1632   score: 4.0   memory length: 357309   epsilon: 0.49052620001079456    steps: 319    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1633   score: 6.0   memory length: 357685   epsilon: 0.48978172001078984    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1634   score: 3.0   memory length: 357897   epsilon: 0.4893619600107872    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1635   score: 5.0   memory length: 358193   epsilon: 0.4887758800107835    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1636   score: 5.0   memory length: 358517   epsilon: 0.4881343600107794    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1637   score: 6.0   memory length: 358862   epsilon: 0.4874512600107751    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1638   score: 7.0   memory length: 359287   epsilon: 0.4866097600107698    steps: 425    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1639   score: 9.0   memory length: 359769   epsilon: 0.48565540001076374    steps: 482    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
      "episode: 1640   score: 4.0   memory length: 360068   epsilon: 0.48506338001076    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
      "episode: 1641   score: 0.0   memory length: 360191   epsilon: 0.48481984001075845    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1642   score: 6.0   memory length: 360526   epsilon: 0.48415654001075425    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
      "episode: 1643   score: 3.0   memory length: 360754   epsilon: 0.4837051000107514    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1644   score: 9.0   memory length: 361204   epsilon: 0.48281410001074576    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
      "episode: 1645   score: 3.0   memory length: 361432   epsilon: 0.4823626600107429    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1646   score: 4.0   memory length: 361707   epsilon: 0.48181816001073946    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1647   score: 9.0   memory length: 362163   epsilon: 0.48091528001073375    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1648   score: 5.0   memory length: 362510   epsilon: 0.4802282200107294    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1649   score: 3.0   memory length: 362740   epsilon: 0.4797728200107265    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1650   score: 14.0   memory length: 363185   epsilon: 0.47889172001072094    steps: 445    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1651   score: 5.0   memory length: 363527   epsilon: 0.47821456001071666    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1652   score: 4.0   memory length: 363802   epsilon: 0.4776700600107132    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.97\n",
      "episode: 1653   score: 11.0   memory length: 364365   epsilon: 0.47655532001070616    steps: 563    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1654   score: 5.0   memory length: 364732   epsilon: 0.47582866001070157    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1655   score: 7.0   memory length: 365155   epsilon: 0.47499112001069627    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1656   score: 5.0   memory length: 365498   epsilon: 0.47431198001069197    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1657   score: 8.0   memory length: 365964   epsilon: 0.47338930001068613    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1658   score: 7.0   memory length: 366390   epsilon: 0.4725458200106808    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1659   score: 4.0   memory length: 366665   epsilon: 0.47200132001067735    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1660   score: 5.0   memory length: 366962   epsilon: 0.47141326001067363    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1661   score: 6.0   memory length: 367301   epsilon: 0.4707420400106694    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
      "episode: 1662   score: 5.0   memory length: 367649   epsilon: 0.470053000010665    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1663   score: 5.0   memory length: 367974   epsilon: 0.46940950001066095    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1664   score: 3.0   memory length: 368202   epsilon: 0.4689580600106581    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1665   score: 3.0   memory length: 368447   epsilon: 0.468472960010655    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1666   score: 3.0   memory length: 368676   epsilon: 0.46801954001065216    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.99\n",
      "episode: 1667   score: 6.0   memory length: 369040   epsilon: 0.4672988200106476    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
      "episode: 1668   score: 5.0   memory length: 369327   epsilon: 0.466730560010644    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1669   score: 5.0   memory length: 369669   epsilon: 0.4660534000106397    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1670   score: 4.0   memory length: 369963   epsilon: 0.46547128001063603    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.98\n",
      "episode: 1671   score: 11.0   memory length: 370385   epsilon: 0.46463572001063075    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1672   score: 4.0   memory length: 370646   epsilon: 0.4641189400106275    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1673   score: 5.0   memory length: 370937   epsilon: 0.46354276001062383    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.98\n",
      "episode: 1674   score: 4.0   memory length: 371214   epsilon: 0.46299430001062036    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
      "episode: 1675   score: 1.0   memory length: 371365   epsilon: 0.46269532001061847    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
      "episode: 1676   score: 9.0   memory length: 371794   epsilon: 0.4618459000106131    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 4.97\n",
      "episode: 1677   score: 1.0   memory length: 371944   epsilon: 0.4615489000106112    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
      "episode: 1678   score: 11.0   memory length: 372503   epsilon: 0.4604420800106042    steps: 559    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1679   score: 4.0   memory length: 372761   epsilon: 0.459931240010601    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.98\n",
      "episode: 1680   score: 5.0   memory length: 373069   epsilon: 0.4593214000105971    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
      "episode: 1681   score: 6.0   memory length: 373429   epsilon: 0.4586086000105926    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1682   score: 10.0   memory length: 373925   epsilon: 0.4576265200105864    steps: 496    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1683   score: 3.0   memory length: 374137   epsilon: 0.45720676001058375    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1684   score: 3.0   memory length: 374364   epsilon: 0.4567573000105809    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1685   score: 5.0   memory length: 374704   epsilon: 0.45608410001057664    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1686   score: 6.0   memory length: 375068   epsilon: 0.4553633800105721    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1687   score: 8.0   memory length: 375482   epsilon: 0.4545436600105669    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 5.15\n",
      "episode: 1688   score: 8.0   memory length: 375932   epsilon: 0.45365266001056126    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1689   score: 3.0   memory length: 376158   epsilon: 0.45320518001055843    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
      "episode: 1690   score: 7.0   memory length: 376532   epsilon: 0.45246466001055374    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1691   score: 4.0   memory length: 376811   epsilon: 0.45191224001055025    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1692   score: 5.0   memory length: 377117   epsilon: 0.4513063600105464    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1693   score: 3.0   memory length: 377364   epsilon: 0.4508173000105433    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
      "episode: 1694   score: 2.0   memory length: 377546   epsilon: 0.45045694001054104    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
      "episode: 1695   score: 4.0   memory length: 377848   epsilon: 0.44985898001053726    steps: 302    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1696   score: 9.0   memory length: 378301   epsilon: 0.4489620400105316    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1697   score: 3.0   memory length: 378534   epsilon: 0.44850070001052866    steps: 233    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1698   score: 3.0   memory length: 378762   epsilon: 0.4480492600105258    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
      "episode: 1699   score: 7.0   memory length: 379117   epsilon: 0.44734636001052136    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
      "episode: 1700   score: 7.0   memory length: 379483   epsilon: 0.4466216800105168    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
      "episode: 1701   score: 4.0   memory length: 379758   epsilon: 0.44607718001051333    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
      "episode: 1702   score: 2.0   memory length: 379937   epsilon: 0.4457227600105111    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
      "episode: 1703   score: 5.0   memory length: 380222   epsilon: 0.4451584600105075    steps: 285    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
      "episode: 1704   score: 4.0   memory length: 380481   epsilon: 0.4446456400105043    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
      "episode: 1705   score: 8.0   memory length: 380884   epsilon: 0.4438477000104992    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1706   score: 7.0   memory length: 381308   epsilon: 0.4430081800104939    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1707   score: 6.0   memory length: 381658   epsilon: 0.44231518001048953    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
      "episode: 1708   score: 3.0   memory length: 381887   epsilon: 0.44186176001048666    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
      "episode: 1709   score: 4.0   memory length: 382162   epsilon: 0.4413172600104832    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
      "episode: 1710   score: 9.0   memory length: 382669   epsilon: 0.44031340001047686    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 5.15\n",
      "episode: 1711   score: 2.0   memory length: 382888   epsilon: 0.4398797800104741    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
      "episode: 1712   score: 5.0   memory length: 383173   epsilon: 0.43931548001047055    steps: 285    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1713   score: 4.0   memory length: 383451   epsilon: 0.43876504001046707    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1714   score: 1.0   memory length: 383621   epsilon: 0.43842844001046494    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1715   score: 9.0   memory length: 384079   epsilon: 0.4375216000104592    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1716   score: 7.0   memory length: 384467   epsilon: 0.43675336001045434    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1717   score: 3.0   memory length: 384732   epsilon: 0.436228660010451    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1718   score: 4.0   memory length: 385026   epsilon: 0.43564654001044734    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
      "episode: 1719   score: 10.0   memory length: 385455   epsilon: 0.43479712001044196    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1720   score: 4.0   memory length: 385750   epsilon: 0.43421302001043827    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1721   score: 1.0   memory length: 385919   epsilon: 0.43387840001043615    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1722   score: 7.0   memory length: 386305   epsilon: 0.4331141200104313    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1723   score: 8.0   memory length: 386729   epsilon: 0.432274600010426    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1724   score: 7.0   memory length: 387115   epsilon: 0.43151032001042117    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1725   score: 3.0   memory length: 387325   epsilon: 0.43109452001041854    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.24\n",
      "episode: 1726   score: 4.0   memory length: 387564   epsilon: 0.43062130001041554    steps: 239    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1727   score: 6.0   memory length: 387919   epsilon: 0.4299184000104111    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1728   score: 3.0   memory length: 388132   epsilon: 0.4294966600104084    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1729   score: 8.0   memory length: 388603   epsilon: 0.4285640800104025    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1730   score: 7.0   memory length: 389013   epsilon: 0.4277522800103974    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1731   score: 7.0   memory length: 389404   epsilon: 0.4269781000103925    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 5.34\n",
      "episode: 1732   score: 9.0   memory length: 389731   epsilon: 0.4263306400103884    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1733   score: 4.0   memory length: 389991   epsilon: 0.42581584001038514    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1734   score: 7.0   memory length: 390354   epsilon: 0.4250971000103806    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1735   score: 3.0   memory length: 390564   epsilon: 0.42468130001037796    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1736   score: 4.0   memory length: 390825   epsilon: 0.4241645200103747    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1737   score: 4.0   memory length: 391100   epsilon: 0.42362002001037125    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1738   score: 4.0   memory length: 391358   epsilon: 0.423109180010368    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1739   score: 5.0   memory length: 391663   epsilon: 0.4225052800103642    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1740   score: 3.0   memory length: 391892   epsilon: 0.4220518600103613    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1741   score: 10.0   memory length: 392435   epsilon: 0.4209767200103545    steps: 543    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1742   score: 7.0   memory length: 392801   epsilon: 0.42025204001034994    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1743   score: 7.0   memory length: 393238   epsilon: 0.41938678001034446    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1744   score: 3.0   memory length: 393505   epsilon: 0.4188581200103411    steps: 267    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1745   score: 7.0   memory length: 393924   epsilon: 0.41802850001033587    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1746   score: 5.0   memory length: 394249   epsilon: 0.4173850000103318    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1747   score: 3.0   memory length: 394459   epsilon: 0.41696920001032917    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1748   score: 13.0   memory length: 395017   epsilon: 0.4158643600103222    steps: 558    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1749   score: 8.0   memory length: 395475   epsilon: 0.41495752001031644    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1750   score: 3.0   memory length: 395704   epsilon: 0.41450410001031357    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1751   score: 13.0   memory length: 396246   epsilon: 0.4134309400103068    steps: 542    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1752   score: 6.0   memory length: 396586   epsilon: 0.4127577400103025    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1753   score: 4.0   memory length: 396848   epsilon: 0.41223898001029924    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1754   score: 5.0   memory length: 397159   epsilon: 0.41162320001029534    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1755   score: 4.0   memory length: 397433   epsilon: 0.4110806800102919    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1756   score: 7.0   memory length: 397820   epsilon: 0.41031442001028706    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1757   score: 7.0   memory length: 398244   epsilon: 0.40947490001028175    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1758   score: 3.0   memory length: 398492   epsilon: 0.40898386001027864    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1759   score: 4.0   memory length: 398769   epsilon: 0.4084354000102752    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1760   score: 7.0   memory length: 399192   epsilon: 0.4075978600102699    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1761   score: 4.0   memory length: 399443   epsilon: 0.40710088001026673    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1762   score: 5.0   memory length: 399768   epsilon: 0.40645738001026266    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1763   score: 6.0   memory length: 400091   epsilon: 0.4058178400102586    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 5.36\n",
      "episode: 1764   score: 6.0   memory length: 400463   epsilon: 0.40508128001025395    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 5.39\n",
      "episode: 1765   score: 6.0   memory length: 400799   epsilon: 0.40441600001024974    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 5.42\n",
      "episode: 1766   score: 7.0   memory length: 401160   epsilon: 0.4037012200102452    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 1767   score: 5.0   memory length: 401438   epsilon: 0.40315078001024174    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.45\n",
      "episode: 1768   score: 4.0   memory length: 401713   epsilon: 0.4026062800102383    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 5.44\n",
      "episode: 1769   score: 4.0   memory length: 402009   epsilon: 0.4020202000102346    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 5.43\n",
      "episode: 1770   score: 6.0   memory length: 402399   epsilon: 0.4012480000102297    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 5.45\n",
      "episode: 1771   score: 6.0   memory length: 402705   epsilon: 0.40064212001022587    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 5.4\n",
      "episode: 1772   score: 10.0   memory length: 403215   epsilon: 0.3996323200102195    steps: 510    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 1773   score: 4.0   memory length: 403457   epsilon: 0.39915316001021645    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 5.45\n",
      "episode: 1774   score: 3.0   memory length: 403670   epsilon: 0.3987314200102138    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.44\n",
      "episode: 1775   score: 6.0   memory length: 404042   epsilon: 0.3979948600102091    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 1776   score: 9.0   memory length: 404534   epsilon: 0.39702070001020295    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 1777   score: 8.0   memory length: 404990   epsilon: 0.39611782001019724    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1778   score: 1.0   memory length: 405162   epsilon: 0.3957772600101951    steps: 172    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 1779   score: 16.0   memory length: 405666   epsilon: 0.3947793400101888    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 5.58\n",
      "episode: 1780   score: 5.0   memory length: 405993   epsilon: 0.3941318800101847    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.58\n",
      "episode: 1781   score: 4.0   memory length: 406290   epsilon: 0.39354382001018096    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1782   score: 9.0   memory length: 406793   epsilon: 0.39254788001017465    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1783   score: 2.0   memory length: 406974   epsilon: 0.3921895000101724    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1784   score: 9.0   memory length: 407461   epsilon: 0.3912252400101663    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 5.6\n",
      "episode: 1785   score: 5.0   memory length: 407785   epsilon: 0.3905837200101622    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 5.6\n",
      "episode: 1786   score: 8.0   memory length: 408258   epsilon: 0.3896471800101563    steps: 473    lr: 6.400000000000001e-06     evaluation reward: 5.62\n",
      "episode: 1787   score: 4.0   memory length: 408554   epsilon: 0.3890611000101526    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 5.58\n",
      "episode: 1788   score: 4.0   memory length: 408816   epsilon: 0.3885423400101493    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1789   score: 4.0   memory length: 409093   epsilon: 0.38799388001014584    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1790   score: 3.0   memory length: 409305   epsilon: 0.3875741200101432    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1791   score: 5.0   memory length: 409614   epsilon: 0.3869623000101393    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.52\n",
      "episode: 1792   score: 4.0   memory length: 409890   epsilon: 0.38641582001013586    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1793   score: 6.0   memory length: 410265   epsilon: 0.38567332001013116    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1794   score: 4.0   memory length: 410523   epsilon: 0.38516248001012793    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1795   score: 6.0   memory length: 410877   epsilon: 0.3844615600101235    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 5.58\n",
      "episode: 1796   score: 7.0   memory length: 411298   epsilon: 0.3836279800101182    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1797   score: 6.0   memory length: 411675   epsilon: 0.3828815200101135    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 5.59\n",
      "episode: 1798   score: 3.0   memory length: 411888   epsilon: 0.38245978001011083    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.59\n",
      "episode: 1799   score: 3.0   memory length: 412117   epsilon: 0.38200636001010796    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1800   score: 6.0   memory length: 412461   epsilon: 0.38132524001010365    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1801   score: 3.0   memory length: 412711   epsilon: 0.3808302400101005    steps: 250    lr: 6.400000000000001e-06     evaluation reward: 5.53\n",
      "episode: 1802   score: 10.0   memory length: 413137   epsilon: 0.3799867600100952    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 5.61\n",
      "episode: 1803   score: 4.0   memory length: 413432   epsilon: 0.3794026600100915    steps: 295    lr: 6.400000000000001e-06     evaluation reward: 5.6\n",
      "episode: 1804   score: 3.0   memory length: 413643   epsilon: 0.37898488001008884    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 5.59\n",
      "episode: 1805   score: 4.0   memory length: 413886   epsilon: 0.3785037400100858    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1806   score: 3.0   memory length: 414118   epsilon: 0.3780443800100829    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1807   score: 6.0   memory length: 414492   epsilon: 0.3773038600100782    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1808   score: 4.0   memory length: 414788   epsilon: 0.3767177800100745    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 5.52\n",
      "episode: 1809   score: 3.0   memory length: 415001   epsilon: 0.37629604001007183    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1810   score: 6.0   memory length: 415391   epsilon: 0.37552384001006694    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 5.48\n",
      "episode: 1811   score: 7.0   memory length: 415781   epsilon: 0.37475164001006206    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 5.53\n",
      "episode: 1812   score: 8.0   memory length: 416237   epsilon: 0.37384876001005635    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1813   score: 12.0   memory length: 416844   epsilon: 0.37264690001004874    steps: 607    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
      "episode: 1814   score: 5.0   memory length: 417152   epsilon: 0.3720370600100449    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1815   score: 5.0   memory length: 417455   epsilon: 0.3714371200100411    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
      "episode: 1816   score: 6.0   memory length: 417808   epsilon: 0.37073818001003667    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 5.63\n",
      "episode: 1817   score: 6.0   memory length: 418165   epsilon: 0.3700313200100322    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.66\n",
      "episode: 1818   score: 4.0   memory length: 418461   epsilon: 0.3694452400100285    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 5.66\n",
      "episode: 1819   score: 4.0   memory length: 418724   epsilon: 0.3689245000100252    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.6\n",
      "episode: 1820   score: 8.0   memory length: 419164   epsilon: 0.3680533000100197    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
      "episode: 1821   score: 7.0   memory length: 419540   epsilon: 0.36730882001001497    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1822   score: 8.0   memory length: 419940   epsilon: 0.36651682001000996    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 1823   score: 10.0   memory length: 420476   epsilon: 0.36545554001000324    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1824   score: 5.0   memory length: 420787   epsilon: 0.36483976000999935    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 1825   score: 3.0   memory length: 421016   epsilon: 0.3643863400099965    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 1826   score: 5.0   memory length: 421325   epsilon: 0.3637745200099926    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 1827   score: 3.0   memory length: 421537   epsilon: 0.36335476000998995    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1828   score: 7.0   memory length: 421953   epsilon: 0.36253108000998474    steps: 416    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1829   score: 9.0   memory length: 422344   epsilon: 0.36175690000997984    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 1830   score: 6.0   memory length: 422703   epsilon: 0.36104608000997535    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1831   score: 7.0   memory length: 423108   epsilon: 0.36024418000997027    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1832   score: 6.0   memory length: 423446   epsilon: 0.35957494000996604    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1833   score: 8.0   memory length: 423910   epsilon: 0.3586562200099602    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 1834   score: 7.0   memory length: 424348   epsilon: 0.35778898000995474    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 1835   score: 8.0   memory length: 424804   epsilon: 0.356886100009949    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
      "episode: 1836   score: 9.0   memory length: 425299   epsilon: 0.3559060000099428    steps: 495    lr: 6.400000000000001e-06     evaluation reward: 5.84\n",
      "episode: 1837   score: 3.0   memory length: 425545   epsilon: 0.35541892000993974    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
      "episode: 1838   score: 6.0   memory length: 425898   epsilon: 0.3547199800099353    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 1839   score: 7.0   memory length: 426320   epsilon: 0.35388442000993003    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
      "episode: 1840   score: 7.0   memory length: 426763   epsilon: 0.3530072800099245    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1841   score: 8.0   memory length: 427182   epsilon: 0.35217766000991924    steps: 419    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1842   score: 2.0   memory length: 427380   epsilon: 0.35178562000991676    steps: 198    lr: 6.400000000000001e-06     evaluation reward: 5.84\n",
      "episode: 1843   score: 8.0   memory length: 427773   epsilon: 0.35100748000991183    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 1844   score: 5.0   memory length: 428080   epsilon: 0.350399620009908    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
      "episode: 1845   score: 6.0   memory length: 428480   epsilon: 0.349607620009903    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 5.86\n",
      "episode: 1846   score: 4.0   memory length: 428741   epsilon: 0.3490908400098997    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 1847   score: 7.0   memory length: 429131   epsilon: 0.3483186400098948    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1848   score: 6.0   memory length: 429506   epsilon: 0.3475761400098901    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
      "episode: 1849   score: 4.0   memory length: 429769   epsilon: 0.3470554000098868    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1850   score: 2.0   memory length: 429951   epsilon: 0.34669504000988455    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 1851   score: 6.0   memory length: 430309   epsilon: 0.34598620000988006    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1852   score: 4.0   memory length: 430603   epsilon: 0.3454040800098764    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1853   score: 4.0   memory length: 430865   epsilon: 0.3448853200098731    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1854   score: 5.0   memory length: 431190   epsilon: 0.344241820009869    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1855   score: 5.0   memory length: 431474   epsilon: 0.34367950000986547    steps: 284    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1856   score: 6.0   memory length: 431834   epsilon: 0.34296670000986096    steps: 360    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1857   score: 8.0   memory length: 432266   epsilon: 0.34211134000985555    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1858   score: 2.0   memory length: 432466   epsilon: 0.34171534000985304    steps: 200    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1859   score: 6.0   memory length: 432860   epsilon: 0.3409352200098481    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1860   score: 4.0   memory length: 433116   epsilon: 0.3404283400098449    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 1861   score: 6.0   memory length: 433434   epsilon: 0.3397987000098409    steps: 318    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1862   score: 4.0   memory length: 433711   epsilon: 0.33925024000983744    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1863   score: 8.0   memory length: 434152   epsilon: 0.3383770600098319    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1864   score: 6.0   memory length: 434487   epsilon: 0.3377137600098277    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1865   score: 3.0   memory length: 434718   epsilon: 0.33725638000982483    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 1866   score: 3.0   memory length: 434931   epsilon: 0.33683464000982216    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.63\n",
      "episode: 1867   score: 11.0   memory length: 435493   epsilon: 0.3357218800098151    steps: 562    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1868   score: 7.0   memory length: 435868   epsilon: 0.3349793800098104    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 1869   score: 6.0   memory length: 436200   epsilon: 0.33432202000980626    steps: 332    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 1870   score: 4.0   memory length: 436442   epsilon: 0.33384286000980323    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 1871   score: 4.0   memory length: 436700   epsilon: 0.3333320200098    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.7\n",
      "episode: 1872   score: 7.0   memory length: 437102   epsilon: 0.33253606000979496    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 1873   score: 8.0   memory length: 437582   epsilon: 0.33158566000978895    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 1874   score: 8.0   memory length: 438038   epsilon: 0.33068278000978324    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 1875   score: 8.0   memory length: 438425   epsilon: 0.3299165200097784    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1876   score: 6.0   memory length: 438785   epsilon: 0.3292037200097739    steps: 360    lr: 6.400000000000001e-06     evaluation reward: 5.75\n",
      "episode: 1877   score: 6.0   memory length: 439122   epsilon: 0.32853646000976966    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1878   score: 8.0   memory length: 439546   epsilon: 0.32769694000976435    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 5.8\n",
      "episode: 1879   score: 4.0   memory length: 439806   epsilon: 0.3271821400097611    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 1880   score: 4.0   memory length: 440074   epsilon: 0.32665150000975773    steps: 268    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 1881   score: 4.0   memory length: 440317   epsilon: 0.3261703600097547    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 1882   score: 4.0   memory length: 440576   epsilon: 0.32565754000975144    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.62\n",
      "episode: 1883   score: 7.0   memory length: 440969   epsilon: 0.3248794000097465    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 1884   score: 3.0   memory length: 441221   epsilon: 0.32438044000974336    steps: 252    lr: 6.400000000000001e-06     evaluation reward: 5.61\n",
      "episode: 1885   score: 9.0   memory length: 441716   epsilon: 0.32340034000973716    steps: 495    lr: 6.400000000000001e-06     evaluation reward: 5.65\n",
      "episode: 1886   score: 7.0   memory length: 442117   epsilon: 0.32260636000973214    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
      "episode: 1887   score: 9.0   memory length: 442567   epsilon: 0.3217153600097265    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 1888   score: 6.0   memory length: 442939   epsilon: 0.32097880000972184    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 1889   score: 9.0   memory length: 443411   epsilon: 0.32004424000971593    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 1890   score: 6.0   memory length: 443739   epsilon: 0.3193948000097118    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
      "episode: 1891   score: 4.0   memory length: 444002   epsilon: 0.3188740600097085    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1892   score: 4.0   memory length: 444279   epsilon: 0.31832560000970506    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1893   score: 6.0   memory length: 444633   epsilon: 0.3176246800097006    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1894   score: 12.0   memory length: 445156   epsilon: 0.31658914000969407    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 5.86\n",
      "episode: 1895   score: 8.0   memory length: 445616   epsilon: 0.3156783400096883    steps: 460    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 1896   score: 3.0   memory length: 445842   epsilon: 0.3152308600096855    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 5.84\n",
      "episode: 1897   score: 5.0   memory length: 446130   epsilon: 0.31466062000968187    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
      "episode: 1898   score: 8.0   memory length: 446410   epsilon: 0.31410622000967836    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 1899   score: 6.0   memory length: 446782   epsilon: 0.3133696600096737    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1900   score: 4.0   memory length: 447038   epsilon: 0.3128627800096705    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1901   score: 5.0   memory length: 447383   epsilon: 0.31217968000966617    steps: 345    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1902   score: 9.0   memory length: 447883   epsilon: 0.3111896800096599    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
      "episode: 1903   score: 12.0   memory length: 448516   epsilon: 0.309936340009652    steps: 633    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
      "episode: 1904   score: 6.0   memory length: 448874   epsilon: 0.3092275000096475    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
      "episode: 1905   score: 8.0   memory length: 449307   epsilon: 0.30837016000964207    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
      "episode: 1906   score: 4.0   memory length: 449570   epsilon: 0.3078494200096388    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 1907   score: 7.0   memory length: 449977   epsilon: 0.3070435600096337    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
      "episode: 1908   score: 6.0   memory length: 450354   epsilon: 0.30629710000962895    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 1909   score: 7.0   memory length: 450744   epsilon: 0.30552490000962407    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 1910   score: 5.0   memory length: 451069   epsilon: 0.30488140000962    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 1911   score: 7.0   memory length: 451439   epsilon: 0.30414880000961536    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 1912   score: 3.0   memory length: 451652   epsilon: 0.3037270600096127    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
      "episode: 1913   score: 4.0   memory length: 451952   epsilon: 0.30313306000960893    steps: 300    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 1914   score: 9.0   memory length: 452458   epsilon: 0.3021311800096026    steps: 506    lr: 6.400000000000001e-06     evaluation reward: 6.03\n",
      "episode: 1915   score: 8.0   memory length: 452844   epsilon: 0.30136690000959776    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 1916   score: 5.0   memory length: 453152   epsilon: 0.3007570600095939    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
      "episode: 1917   score: 6.0   memory length: 453526   epsilon: 0.3000165400095892    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
      "episode: 1918   score: 10.0   memory length: 453996   epsilon: 0.29908594000958333    steps: 470    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 1919   score: 4.0   memory length: 454297   epsilon: 0.29848996000957956    steps: 301    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 1920   score: 10.0   memory length: 454775   epsilon: 0.29754352000957357    steps: 478    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 1921   score: 10.0   memory length: 455266   epsilon: 0.2965713400095674    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 1922   score: 9.0   memory length: 455770   epsilon: 0.2955734200095611    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 6.17\n",
      "episode: 1923   score: 9.0   memory length: 456112   epsilon: 0.2948962600095568    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 1924   score: 8.0   memory length: 456513   epsilon: 0.2941022800095518    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 1925   score: 10.0   memory length: 457099   epsilon: 0.29294200000954446    steps: 586    lr: 6.400000000000001e-06     evaluation reward: 6.26\n",
      "episode: 1926   score: 7.0   memory length: 457498   epsilon: 0.29215198000953946    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 1927   score: 9.0   memory length: 457952   epsilon: 0.29125306000953377    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1928   score: 11.0   memory length: 458526   epsilon: 0.2901165400095266    steps: 574    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1929   score: 5.0   memory length: 458837   epsilon: 0.2895007600095227    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1930   score: 8.0   memory length: 459271   epsilon: 0.28864144000951725    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 1931   score: 4.0   memory length: 459532   epsilon: 0.288124660009514    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 1932   score: 10.0   memory length: 460062   epsilon: 0.28707526000950734    steps: 530    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1933   score: 7.0   memory length: 460436   epsilon: 0.28633474000950265    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 1934   score: 11.0   memory length: 460952   epsilon: 0.2853130600094962    steps: 516    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1935   score: 8.0   memory length: 461354   epsilon: 0.28451710000949115    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1936   score: 10.0   memory length: 461765   epsilon: 0.283703320009486    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1937   score: 5.0   memory length: 462074   epsilon: 0.28309150000948213    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1938   score: 9.0   memory length: 462531   epsilon: 0.2821866400094764    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
      "episode: 1939   score: 5.0   memory length: 462860   epsilon: 0.2815352200094723    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
      "episode: 1940   score: 5.0   memory length: 463167   epsilon: 0.28092736000946844    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 1941   score: 4.0   memory length: 463408   epsilon: 0.2804501800094654    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1942   score: 6.0   memory length: 463783   epsilon: 0.2797076800094607    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 1943   score: 6.0   memory length: 464153   epsilon: 0.2789750800094561    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1944   score: 4.0   memory length: 464416   epsilon: 0.2784543400094528    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 1945   score: 10.0   memory length: 464981   epsilon: 0.2773356400094457    steps: 565    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1946   score: 10.0   memory length: 465354   epsilon: 0.27659710000944104    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 1947   score: 5.0   memory length: 465646   epsilon: 0.2760189400094374    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
      "episode: 1948   score: 6.0   memory length: 466021   epsilon: 0.2752764400094327    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
      "episode: 1949   score: 4.0   memory length: 466313   epsilon: 0.27469828000942903    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
      "episode: 1950   score: 7.0   memory length: 466734   epsilon: 0.27386470000942376    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
      "episode: 1951   score: 6.0   memory length: 467088   epsilon: 0.2731637800094193    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
      "episode: 1952   score: 5.0   memory length: 467434   epsilon: 0.272478700009415    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
      "episode: 1953   score: 6.0   memory length: 467812   epsilon: 0.27173026000941025    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 1954   score: 7.0   memory length: 468170   epsilon: 0.27102142000940577    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 1955   score: 11.0   memory length: 468743   epsilon: 0.2698868800093986    steps: 573    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 1956   score: 4.0   memory length: 469021   epsilon: 0.2693364400093951    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.61\n",
      "episode: 1957   score: 7.0   memory length: 469411   epsilon: 0.2685642400093902    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 1958   score: 5.0   memory length: 469737   epsilon: 0.26791876000938614    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 1959   score: 7.0   memory length: 470106   epsilon: 0.2671881400093815    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 1960   score: 6.0   memory length: 470409   epsilon: 0.2665882000093777    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1961   score: 6.0   memory length: 470768   epsilon: 0.2658773800093732    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1962   score: 4.0   memory length: 471027   epsilon: 0.26536456000937    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1963   score: 8.0   memory length: 471416   epsilon: 0.2645943400093651    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1964   score: 7.0   memory length: 471804   epsilon: 0.26382610000936024    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 6.67\n",
      "episode: 1965   score: 5.0   memory length: 472111   epsilon: 0.2632182400093564    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.69\n",
      "episode: 1966   score: 11.0   memory length: 472653   epsilon: 0.2621450800093496    steps: 542    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1967   score: 10.0   memory length: 473202   epsilon: 0.26105806000934273    steps: 549    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1968   score: 4.0   memory length: 473464   epsilon: 0.26053930000933945    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1969   score: 9.0   memory length: 473975   epsilon: 0.25952752000933305    steps: 511    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1970   score: 5.0   memory length: 474284   epsilon: 0.2589157000093292    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1971   score: 10.0   memory length: 474791   epsilon: 0.2579118400093228    steps: 507    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1972   score: 4.0   memory length: 475034   epsilon: 0.2574307000093198    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1973   score: 7.0   memory length: 475458   epsilon: 0.25659118000931447    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1974   score: 12.0   memory length: 476029   epsilon: 0.2554606000093073    steps: 571    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1975   score: 8.0   memory length: 476461   epsilon: 0.2546052400093019    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1976   score: 3.0   memory length: 476691   epsilon: 0.254149840009299    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1977   score: 11.0   memory length: 477246   epsilon: 0.25305094000929207    steps: 555    lr: 6.400000000000001e-06     evaluation reward: 6.85\n",
      "episode: 1978   score: 5.0   memory length: 477551   epsilon: 0.25244704000928825    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1979   score: 5.0   memory length: 477855   epsilon: 0.25184512000928444    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1980   score: 6.0   memory length: 478246   epsilon: 0.25107094000927954    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 6.85\n",
      "episode: 1981   score: 11.0   memory length: 478832   epsilon: 0.2499106600092722    steps: 586    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
      "episode: 1982   score: 10.0   memory length: 479360   epsilon: 0.2488652200092656    steps: 528    lr: 6.400000000000001e-06     evaluation reward: 6.98\n",
      "episode: 1983   score: 9.0   memory length: 479848   epsilon: 0.24789898000925947    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
      "episode: 1984   score: 5.0   memory length: 480139   epsilon: 0.24732280000925583    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
      "episode: 1985   score: 11.0   memory length: 480693   epsilon: 0.2462258800092489    steps: 554    lr: 6.400000000000001e-06     evaluation reward: 7.04\n",
      "episode: 1986   score: 9.0   memory length: 481148   epsilon: 0.2453249800092432    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 1987   score: 4.0   memory length: 481407   epsilon: 0.24481216000923994    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 7.01\n",
      "episode: 1988   score: 10.0   memory length: 481889   epsilon: 0.2438578000092339    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 7.05\n",
      "episode: 1989   score: 3.0   memory length: 482136   epsilon: 0.2433687400092308    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
      "episode: 1990   score: 4.0   memory length: 482433   epsilon: 0.2427806800092271    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 6.97\n",
      "episode: 1991   score: 7.0   memory length: 482857   epsilon: 0.24194116000922178    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
      "episode: 1992   score: 10.0   memory length: 483393   epsilon: 0.24087988000921506    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 1993   score: 8.0   memory length: 483827   epsilon: 0.24002056000920963    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 7.08\n",
      "episode: 1994   score: 7.0   memory length: 484253   epsilon: 0.2391770800092043    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
      "episode: 1995   score: 8.0   memory length: 484645   epsilon: 0.23840092000919938    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
      "episode: 1996   score: 6.0   memory length: 485017   epsilon: 0.23766436000919472    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 1997   score: 6.0   memory length: 485354   epsilon: 0.2369971000091905    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 7.07\n",
      "episode: 1998   score: 11.0   memory length: 485897   epsilon: 0.2359219600091837    steps: 543    lr: 6.400000000000001e-06     evaluation reward: 7.1\n",
      "episode: 1999   score: 11.0   memory length: 486405   epsilon: 0.23491612000917733    steps: 508    lr: 6.400000000000001e-06     evaluation reward: 7.15\n",
      "episode: 2000   score: 7.0   memory length: 486771   epsilon: 0.23419144000917275    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 7.18\n",
      "episode: 2001   score: 4.0   memory length: 487049   epsilon: 0.23364100000916926    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2002   score: 7.0   memory length: 487436   epsilon: 0.23287474000916442    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 7.15\n",
      "episode: 2003   score: 5.0   memory length: 487746   epsilon: 0.23226094000916053    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2004   score: 6.0   memory length: 488072   epsilon: 0.23161546000915645    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2005   score: 11.0   memory length: 488625   epsilon: 0.23052052000914952    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2006   score: 4.0   memory length: 488900   epsilon: 0.22997602000914608    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2007   score: 7.0   memory length: 489303   epsilon: 0.22917808000914103    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2008   score: 5.0   memory length: 489584   epsilon: 0.2286217000091375    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2009   score: 8.0   memory length: 489992   epsilon: 0.2278138600091324    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2010   score: 5.0   memory length: 490320   epsilon: 0.2271644200091283    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2011   score: 5.0   memory length: 490632   epsilon: 0.22654666000912438    steps: 312    lr: 6.400000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2012   score: 4.0   memory length: 490876   epsilon: 0.22606354000912132    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2013   score: 7.0   memory length: 491319   epsilon: 0.22518640000911577    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 7.13\n",
      "episode: 2014   score: 8.0   memory length: 491744   epsilon: 0.22434490000911045    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 7.12\n",
      "episode: 2015   score: 4.0   memory length: 492039   epsilon: 0.22376080000910675    steps: 295    lr: 6.400000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2016   score: 5.0   memory length: 492367   epsilon: 0.22311136000910264    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2017   score: 5.0   memory length: 492693   epsilon: 0.22246588000909856    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 7.07\n",
      "episode: 2018   score: 8.0   memory length: 493148   epsilon: 0.22156498000909286    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2019   score: 12.0   memory length: 493741   epsilon: 0.22039084000908543    steps: 593    lr: 6.400000000000001e-06     evaluation reward: 7.13\n",
      "episode: 2020   score: 8.0   memory length: 494175   epsilon: 0.21953152000908    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2021   score: 5.0   memory length: 494448   epsilon: 0.21899098000907657    steps: 273    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2022   score: 9.0   memory length: 494927   epsilon: 0.21804256000907057    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2023   score: 7.0   memory length: 495347   epsilon: 0.2172109600090653    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2024   score: 6.0   memory length: 495736   epsilon: 0.21644074000906044    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
      "episode: 2025   score: 9.0   memory length: 496203   epsilon: 0.2155160800090546    steps: 467    lr: 6.400000000000001e-06     evaluation reward: 7.01\n",
      "episode: 2026   score: 9.0   memory length: 496683   epsilon: 0.21456568000904858    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
      "episode: 2027   score: 11.0   memory length: 497235   epsilon: 0.21347272000904166    steps: 552    lr: 6.400000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2028   score: 6.0   memory length: 497610   epsilon: 0.21273022000903696    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2029   score: 6.0   memory length: 497947   epsilon: 0.21206296000903274    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 7.01\n",
      "episode: 2030   score: 9.0   memory length: 498434   epsilon: 0.21109870000902664    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 7.02\n",
      "episode: 2031   score: 6.0   memory length: 498771   epsilon: 0.21043144000902242    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2032   score: 4.0   memory length: 499065   epsilon: 0.20984932000901874    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 6.98\n",
      "episode: 2033   score: 4.0   memory length: 499342   epsilon: 0.20930086000901527    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.95\n",
      "episode: 2034   score: 4.0   memory length: 499603   epsilon: 0.208784080009012    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 2035   score: 7.0   memory length: 500022   epsilon: 0.20795446000900675    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 6.87\n",
      "episode: 2036   score: 8.0   memory length: 500494   epsilon: 0.20701990000900083    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 6.85\n",
      "episode: 2037   score: 7.0   memory length: 500919   epsilon: 0.2061784000089955    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 6.87\n",
      "episode: 2038   score: 8.0   memory length: 501387   epsilon: 0.20525176000898965    steps: 468    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2039   score: 5.0   memory length: 501673   epsilon: 0.20468548000898606    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2040   score: 4.0   memory length: 501933   epsilon: 0.2041706800089828    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 6.85\n",
      "episode: 2041   score: 8.0   memory length: 502380   epsilon: 0.2032856200089772    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 6.89\n",
      "episode: 2042   score: 7.0   memory length: 502746   epsilon: 0.20256094000897262    steps: 366    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2043   score: 5.0   memory length: 503092   epsilon: 0.2018758600089683    steps: 346    lr: 2.560000000000001e-06     evaluation reward: 6.89\n",
      "episode: 2044   score: 8.0   memory length: 503530   epsilon: 0.2010086200089628    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
      "episode: 2045   score: 4.0   memory length: 503790   epsilon: 0.20049382000895954    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 6.87\n",
      "episode: 2046   score: 9.0   memory length: 504288   epsilon: 0.1995077800089533    steps: 498    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2047   score: 8.0   memory length: 504748   epsilon: 0.19859698000894754    steps: 460    lr: 2.560000000000001e-06     evaluation reward: 6.89\n",
      "episode: 2048   score: 8.0   memory length: 505203   epsilon: 0.19769608000894184    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 6.91\n",
      "episode: 2049   score: 3.0   memory length: 505416   epsilon: 0.19727434000893918    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2050   score: 9.0   memory length: 505881   epsilon: 0.19635364000893335    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 6.92\n",
      "episode: 2051   score: 8.0   memory length: 506315   epsilon: 0.1954943200089279    steps: 434    lr: 2.560000000000001e-06     evaluation reward: 6.94\n",
      "episode: 2052   score: 8.0   memory length: 506772   epsilon: 0.1945894600089222    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 6.97\n",
      "episode: 2053   score: 7.0   memory length: 507173   epsilon: 0.19379548000891716    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
      "episode: 2054   score: 4.0   memory length: 507452   epsilon: 0.19324306000891367    steps: 279    lr: 2.560000000000001e-06     evaluation reward: 6.95\n",
      "episode: 2055   score: 6.0   memory length: 507790   epsilon: 0.19257382000890944    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2056   score: 5.0   memory length: 508114   epsilon: 0.19193230000890538    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 6.91\n",
      "episode: 2057   score: 6.0   memory length: 508485   epsilon: 0.19119772000890073    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2058   score: 8.0   memory length: 508928   epsilon: 0.19032058000889518    steps: 443    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
      "episode: 2059   score: 7.0   memory length: 509332   epsilon: 0.18952066000889012    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
      "episode: 2060   score: 5.0   memory length: 509623   epsilon: 0.18894448000888647    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.92\n",
      "episode: 2061   score: 9.0   memory length: 510159   epsilon: 0.18788320000887976    steps: 536    lr: 2.560000000000001e-06     evaluation reward: 6.95\n",
      "episode: 2062   score: 4.0   memory length: 510416   epsilon: 0.18737434000887654    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 6.95\n",
      "episode: 2063   score: 3.0   memory length: 510629   epsilon: 0.18695260000887387    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2064   score: 8.0   memory length: 511089   epsilon: 0.1860418000088681    steps: 460    lr: 2.560000000000001e-06     evaluation reward: 6.91\n",
      "episode: 2065   score: 7.0   memory length: 511485   epsilon: 0.18525772000886315    steps: 396    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
      "episode: 2066   score: 3.0   memory length: 511714   epsilon: 0.18480430000886028    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 6.85\n",
      "episode: 2067   score: 6.0   memory length: 512033   epsilon: 0.18417268000885628    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2068   score: 4.0   memory length: 512277   epsilon: 0.18368956000885323    steps: 244    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2069   score: 9.0   memory length: 512816   epsilon: 0.18262234000884647    steps: 539    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2070   score: 8.0   memory length: 513273   epsilon: 0.18171748000884075    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2071   score: 7.0   memory length: 513622   epsilon: 0.18102646000883638    steps: 349    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2072   score: 6.0   memory length: 513999   epsilon: 0.18028000000883165    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.83\n",
      "episode: 2073   score: 6.0   memory length: 514370   epsilon: 0.179545420008827    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2074   score: 6.0   memory length: 514694   epsilon: 0.17890390000882295    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 6.76\n",
      "episode: 2075   score: 2.0   memory length: 514876   epsilon: 0.17854354000882067    steps: 182    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2076   score: 8.0   memory length: 515320   epsilon: 0.1776644200088151    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2077   score: 8.0   memory length: 515718   epsilon: 0.17687638000881012    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2078   score: 12.0   memory length: 516306   epsilon: 0.17571214000880275    steps: 588    lr: 2.560000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2079   score: 6.0   memory length: 516647   epsilon: 0.17503696000879848    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 6.8\n",
      "episode: 2080   score: 7.0   memory length: 517015   epsilon: 0.17430832000879387    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2081   score: 13.0   memory length: 517550   epsilon: 0.17324902000878717    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 6.83\n",
      "episode: 2082   score: 8.0   memory length: 517846   epsilon: 0.17266294000878346    steps: 296    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2083   score: 10.0   memory length: 518353   epsilon: 0.1716590800087771    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2084   score: 4.0   memory length: 518633   epsilon: 0.1711046800087736    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2085   score: 12.0   memory length: 519143   epsilon: 0.1700948800087672    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2086   score: 6.0   memory length: 519490   epsilon: 0.16940782000876287    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2087   score: 8.0   memory length: 519914   epsilon: 0.16856830000875755    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 6.83\n",
      "episode: 2088   score: 3.0   memory length: 520160   epsilon: 0.16808122000875447    steps: 246    lr: 2.560000000000001e-06     evaluation reward: 6.76\n",
      "episode: 2089   score: 11.0   memory length: 520690   epsilon: 0.16703182000874783    steps: 530    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2090   score: 3.0   memory length: 520901   epsilon: 0.1666140400087452    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 6.83\n",
      "episode: 2091   score: 8.0   memory length: 521299   epsilon: 0.1658260000087402    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2092   score: 8.0   memory length: 521728   epsilon: 0.16497658000873483    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2093   score: 4.0   memory length: 522005   epsilon: 0.16442812000873136    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2094   score: 8.0   memory length: 522426   epsilon: 0.16359454000872609    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2095   score: 6.0   memory length: 522754   epsilon: 0.16294510000872198    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 6.77\n",
      "episode: 2096   score: 8.0   memory length: 523170   epsilon: 0.16212142000871677    steps: 416    lr: 2.560000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2097   score: 8.0   memory length: 523593   epsilon: 0.16128388000871147    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2098   score: 6.0   memory length: 523949   epsilon: 0.160579000008707    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 6.76\n",
      "episode: 2099   score: 6.0   memory length: 524304   epsilon: 0.15987610000870256    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2100   score: 4.0   memory length: 524579   epsilon: 0.15933160000869911    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2101   score: 7.0   memory length: 524948   epsilon: 0.1586009800086945    steps: 369    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2102   score: 5.0   memory length: 525294   epsilon: 0.15791590000869016    steps: 346    lr: 2.560000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2103   score: 9.0   memory length: 525737   epsilon: 0.1570387600086846    steps: 443    lr: 2.560000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2104   score: 4.0   memory length: 525998   epsilon: 0.15652198000868134    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2105   score: 9.0   memory length: 526426   epsilon: 0.15567454000867598    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2106   score: 6.0   memory length: 526744   epsilon: 0.155044900008672    steps: 318    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2107   score: 6.0   memory length: 527099   epsilon: 0.15434200000866755    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2108   score: 13.0   memory length: 527717   epsilon: 0.1531183600086598    steps: 618    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2109   score: 8.0   memory length: 528189   epsilon: 0.1521838000086539    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2110   score: 7.0   memory length: 528609   epsilon: 0.15135220000864863    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 6.8\n",
      "episode: 2111   score: 7.0   memory length: 529011   epsilon: 0.1505562400086436    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2112   score: 8.0   memory length: 529436   epsilon: 0.14971474000863827    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2113   score: 5.0   memory length: 529744   epsilon: 0.1491049000086344    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2114   score: 9.0   memory length: 530268   epsilon: 0.14806738000862785    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 6.85\n",
      "episode: 2115   score: 5.0   memory length: 530559   epsilon: 0.1474912000086242    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2116   score: 10.0   memory length: 530953   epsilon: 0.14671108000861927    steps: 394    lr: 2.560000000000001e-06     evaluation reward: 6.91\n",
      "episode: 2117   score: 4.0   memory length: 531250   epsilon: 0.14612302000861555    steps: 297    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2118   score: 6.0   memory length: 531609   epsilon: 0.14541220000861105    steps: 359    lr: 2.560000000000001e-06     evaluation reward: 6.88\n",
      "episode: 2119   score: 5.0   memory length: 531939   epsilon: 0.1447588000086069    steps: 330    lr: 2.560000000000001e-06     evaluation reward: 6.81\n",
      "episode: 2120   score: 5.0   memory length: 532261   epsilon: 0.14412124000860288    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2121   score: 5.0   memory length: 532551   epsilon: 0.14354704000859925    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2122   score: 7.0   memory length: 532939   epsilon: 0.1427788000085944    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 6.76\n",
      "episode: 2123   score: 4.0   memory length: 533197   epsilon: 0.14226796000859115    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2124   score: 7.0   memory length: 533599   epsilon: 0.14147200000858612    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 6.74\n",
      "episode: 2125   score: 4.0   memory length: 533862   epsilon: 0.14095126000858282    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2126   score: 8.0   memory length: 534293   epsilon: 0.14009788000857742    steps: 431    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2127   score: 10.0   memory length: 534808   epsilon: 0.13907818000857097    steps: 515    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2128   score: 13.0   memory length: 535297   epsilon: 0.13810996000856485    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 6.74\n",
      "episode: 2129   score: 4.0   memory length: 535556   epsilon: 0.1375971400085616    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2130   score: 12.0   memory length: 536018   epsilon: 0.13668238000855581    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2131   score: 6.0   memory length: 536373   epsilon: 0.13597948000855137    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2132   score: 9.0   memory length: 536857   epsilon: 0.1350211600085453    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 6.8\n",
      "episode: 2133   score: 6.0   memory length: 537196   epsilon: 0.13434994000854106    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2134   score: 13.0   memory length: 537810   epsilon: 0.13313422000853337    steps: 614    lr: 2.560000000000001e-06     evaluation reward: 6.91\n",
      "episode: 2135   score: 6.0   memory length: 538185   epsilon: 0.13239172000852867    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2136   score: 7.0   memory length: 538592   epsilon: 0.13158586000852357    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 6.89\n",
      "episode: 2137   score: 8.0   memory length: 539037   epsilon: 0.130704760008518    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2138   score: 8.0   memory length: 539492   epsilon: 0.1298038600085123    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2139   score: 8.0   memory length: 539971   epsilon: 0.1288554400085063    steps: 479    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
      "episode: 2140   score: 9.0   memory length: 540436   epsilon: 0.12793474000850047    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
      "episode: 2141   score: 9.0   memory length: 540918   epsilon: 0.12698038000849443    steps: 482    lr: 2.560000000000001e-06     evaluation reward: 6.99\n",
      "episode: 2142   score: 11.0   memory length: 541391   epsilon: 0.1260438400084885    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.03\n",
      "episode: 2143   score: 12.0   memory length: 541824   epsilon: 0.12518650000848308    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2144   score: 15.0   memory length: 542451   epsilon: 0.12394504000848262    steps: 627    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2145   score: 5.0   memory length: 542774   epsilon: 0.12330550000848306    steps: 323    lr: 2.560000000000001e-06     evaluation reward: 7.18\n",
      "episode: 2146   score: 6.0   memory length: 543130   epsilon: 0.12260062000848354    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 7.15\n",
      "episode: 2147   score: 5.0   memory length: 543441   epsilon: 0.12198484000848396    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
      "episode: 2148   score: 10.0   memory length: 543862   epsilon: 0.12115126000848453    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
      "episode: 2149   score: 7.0   memory length: 544229   epsilon: 0.12042460000848502    steps: 367    lr: 2.560000000000001e-06     evaluation reward: 7.18\n",
      "episode: 2150   score: 7.0   memory length: 544619   epsilon: 0.11965240000848555    steps: 390    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
      "episode: 2151   score: 6.0   memory length: 544993   epsilon: 0.11891188000848606    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
      "episode: 2152   score: 4.0   memory length: 545271   epsilon: 0.11836144000848643    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2153   score: 8.0   memory length: 545747   epsilon: 0.11741896000848707    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2154   score: 4.0   memory length: 546044   epsilon: 0.11683090000848748    steps: 297    lr: 2.560000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2155   score: 9.0   memory length: 546532   epsilon: 0.11586466000848814    steps: 488    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
      "episode: 2156   score: 9.0   memory length: 547013   epsilon: 0.11491228000848878    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 7.18\n",
      "episode: 2157   score: 7.0   memory length: 547422   epsilon: 0.11410246000848934    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
      "episode: 2158   score: 9.0   memory length: 547893   epsilon: 0.11316988000848997    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
      "episode: 2159   score: 4.0   memory length: 548153   epsilon: 0.11265508000849032    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2160   score: 5.0   memory length: 548460   epsilon: 0.11204722000849074    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2161   score: 6.0   memory length: 548797   epsilon: 0.1113799600084912    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
      "episode: 2162   score: 7.0   memory length: 549185   epsilon: 0.11061172000849172    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2163   score: 10.0   memory length: 549706   epsilon: 0.10958014000849242    steps: 521    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
      "episode: 2164   score: 6.0   memory length: 550101   epsilon: 0.10879804000849295    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
      "episode: 2165   score: 5.0   memory length: 550442   epsilon: 0.10812286000849342    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
      "episode: 2166   score: 11.0   memory length: 551026   epsilon: 0.1069665400084942    steps: 584    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
      "episode: 2167   score: 10.0   memory length: 551558   epsilon: 0.10591318000849492    steps: 532    lr: 2.560000000000001e-06     evaluation reward: 7.32\n",
      "episode: 2168   score: 8.0   memory length: 551995   epsilon: 0.10504792000849551    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 7.36\n",
      "episode: 2169   score: 9.0   memory length: 552484   epsilon: 0.10407970000849617    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 7.36\n",
      "episode: 2170   score: 8.0   memory length: 552764   epsilon: 0.10352530000849655    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 7.36\n",
      "episode: 2171   score: 10.0   memory length: 553289   epsilon: 0.10248580000849726    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 2172   score: 9.0   memory length: 553756   epsilon: 0.10156114000849789    steps: 467    lr: 2.560000000000001e-06     evaluation reward: 7.42\n",
      "episode: 2173   score: 4.0   memory length: 554032   epsilon: 0.10101466000849826    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 7.4\n",
      "episode: 2174   score: 13.0   memory length: 554656   epsilon: 0.0997791400084991    steps: 624    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 2175   score: 7.0   memory length: 555061   epsilon: 0.09897724000849965    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 7.52\n",
      "episode: 2176   score: 22.0   memory length: 555913   epsilon: 0.0972902800085008    steps: 852    lr: 2.560000000000001e-06     evaluation reward: 7.66\n",
      "episode: 2177   score: 7.0   memory length: 556345   epsilon: 0.09643492000850139    steps: 432    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
      "episode: 2178   score: 8.0   memory length: 556787   epsilon: 0.09555976000850198    steps: 442    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
      "episode: 2179   score: 6.0   memory length: 557139   epsilon: 0.09486280000850246    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
      "episode: 2180   score: 5.0   memory length: 557466   epsilon: 0.0942153400085029    steps: 327    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2181   score: 8.0   memory length: 557884   epsilon: 0.09338770000850347    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 7.54\n",
      "episode: 2182   score: 13.0   memory length: 558535   epsilon: 0.09209872000850435    steps: 651    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2183   score: 10.0   memory length: 559045   epsilon: 0.09108892000850503    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2184   score: 8.0   memory length: 559466   epsilon: 0.0902553400085056    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
      "episode: 2185   score: 11.0   memory length: 559927   epsilon: 0.08934256000850622    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 7.62\n",
      "episode: 2186   score: 8.0   memory length: 560388   epsilon: 0.08842978000850685    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
      "episode: 2187   score: 9.0   memory length: 560843   epsilon: 0.08752888000850746    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
      "episode: 2188   score: 10.0   memory length: 561361   epsilon: 0.08650324000850816    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
      "episode: 2189   score: 9.0   memory length: 561828   epsilon: 0.08557858000850879    steps: 467    lr: 2.560000000000001e-06     evaluation reward: 7.7\n",
      "episode: 2190   score: 9.0   memory length: 562339   epsilon: 0.08456680000850948    steps: 511    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2191   score: 4.0   memory length: 562633   epsilon: 0.08398468000850988    steps: 294    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
      "episode: 2192   score: 11.0   memory length: 563157   epsilon: 0.08294716000851059    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2193   score: 6.0   memory length: 563530   epsilon: 0.08220862000851109    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2194   score: 7.0   memory length: 563938   epsilon: 0.08140078000851164    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2195   score: 11.0   memory length: 564486   epsilon: 0.08031574000851238    steps: 548    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2196   score: 7.0   memory length: 564903   epsilon: 0.07949008000851294    steps: 417    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2197   score: 9.0   memory length: 565357   epsilon: 0.07859116000851356    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2198   score: 9.0   memory length: 565857   epsilon: 0.07760116000851423    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2199   score: 7.0   memory length: 566228   epsilon: 0.07686658000851473    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2200   score: 11.0   memory length: 566624   epsilon: 0.07608250000851527    steps: 396    lr: 2.560000000000001e-06     evaluation reward: 7.92\n",
      "episode: 2201   score: 12.0   memory length: 567208   epsilon: 0.07492618000851606    steps: 584    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2202   score: 8.0   memory length: 567658   epsilon: 0.07403518000851667    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2203   score: 6.0   memory length: 567962   epsilon: 0.07343326000851708    steps: 304    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2204   score: 4.0   memory length: 568204   epsilon: 0.0729541000085174    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2205   score: 3.0   memory length: 568434   epsilon: 0.07249870000851771    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
      "episode: 2206   score: 10.0   memory length: 568934   epsilon: 0.07150870000851839    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 7.95\n",
      "episode: 2207   score: 11.0   memory length: 569505   epsilon: 0.07037812000851916    steps: 571    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2208   score: 10.0   memory length: 570033   epsilon: 0.06933268000851987    steps: 528    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2209   score: 8.0   memory length: 570427   epsilon: 0.0685525600085204    steps: 394    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2210   score: 10.0   memory length: 570928   epsilon: 0.06756058000852108    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2211   score: 5.0   memory length: 571220   epsilon: 0.06698242000852148    steps: 292    lr: 2.560000000000001e-06     evaluation reward: 7.98\n",
      "episode: 2212   score: 9.0   memory length: 571668   epsilon: 0.06609538000852208    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2213   score: 4.0   memory length: 571910   epsilon: 0.06561622000852241    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 7.98\n",
      "episode: 2214   score: 6.0   memory length: 572283   epsilon: 0.06487768000852291    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 7.95\n",
      "episode: 2215   score: 8.0   memory length: 572732   epsilon: 0.06398866000852352    steps: 449    lr: 2.560000000000001e-06     evaluation reward: 7.98\n",
      "episode: 2216   score: 3.0   memory length: 572945   epsilon: 0.0635669200085238    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
      "episode: 2217   score: 6.0   memory length: 573314   epsilon: 0.0628363000085243    steps: 369    lr: 2.560000000000001e-06     evaluation reward: 7.93\n",
      "episode: 2218   score: 9.0   memory length: 573785   epsilon: 0.06190372000852494    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 7.96\n",
      "episode: 2219   score: 7.0   memory length: 574170   epsilon: 0.06114142000852546    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 7.98\n",
      "episode: 2220   score: 12.0   memory length: 574799   epsilon: 0.05989600000852631    steps: 629    lr: 2.560000000000001e-06     evaluation reward: 8.05\n",
      "episode: 2221   score: 7.0   memory length: 575209   epsilon: 0.05908420000852686    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 8.07\n",
      "episode: 2222   score: 10.0   memory length: 575721   epsilon: 0.058070440008527555    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 8.1\n",
      "episode: 2223   score: 8.0   memory length: 576159   epsilon: 0.057203200008528146    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 8.14\n",
      "episode: 2224   score: 5.0   memory length: 576506   epsilon: 0.056516140008528615    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 8.12\n",
      "episode: 2225   score: 8.0   memory length: 576931   epsilon: 0.05567464000852919    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 8.16\n",
      "episode: 2226   score: 10.0   memory length: 577451   epsilon: 0.05464504000852989    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 8.18\n",
      "episode: 2227   score: 6.0   memory length: 577827   epsilon: 0.0539005600085304    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 8.14\n",
      "episode: 2228   score: 6.0   memory length: 578164   epsilon: 0.053233300008530854    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 8.07\n",
      "episode: 2229   score: 8.0   memory length: 578579   epsilon: 0.052411600008531414    steps: 415    lr: 2.560000000000001e-06     evaluation reward: 8.11\n",
      "episode: 2230   score: 5.0   memory length: 578888   epsilon: 0.05179978000853183    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 8.04\n",
      "episode: 2231   score: 7.0   memory length: 579313   epsilon: 0.050958280008532406    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 8.05\n",
      "episode: 2232   score: 7.0   memory length: 579693   epsilon: 0.05020588000853292    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 8.03\n",
      "episode: 2233   score: 9.0   memory length: 580167   epsilon: 0.04926736000853356    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 8.06\n",
      "episode: 2234   score: 8.0   memory length: 580619   epsilon: 0.04837240000853417    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 8.01\n",
      "episode: 2235   score: 10.0   memory length: 581060   epsilon: 0.047499220008534765    steps: 441    lr: 2.560000000000001e-06     evaluation reward: 8.05\n",
      "episode: 2236   score: 4.0   memory length: 581335   epsilon: 0.046954720008535136    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 8.02\n",
      "episode: 2237   score: 6.0   memory length: 581726   epsilon: 0.046180540008535664    steps: 391    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2238   score: 15.0   memory length: 582316   epsilon: 0.04501234000853646    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 8.07\n",
      "episode: 2239   score: 6.0   memory length: 582680   epsilon: 0.04429162000853695    steps: 364    lr: 2.560000000000001e-06     evaluation reward: 8.05\n",
      "episode: 2240   score: 8.0   memory length: 583134   epsilon: 0.043392700008537566    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 8.04\n",
      "episode: 2241   score: 7.0   memory length: 583536   epsilon: 0.04259674000853811    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 8.02\n",
      "episode: 2242   score: 8.0   memory length: 583990   epsilon: 0.04169782000853872    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2243   score: 4.0   memory length: 584283   epsilon: 0.04111768000853912    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
      "episode: 2244   score: 12.0   memory length: 584742   epsilon: 0.04020886000853974    steps: 459    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2245   score: 4.0   memory length: 585004   epsilon: 0.03969010000854009    steps: 262    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
      "episode: 2246   score: 8.0   memory length: 585405   epsilon: 0.03889612000854063    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 7.89\n",
      "episode: 2247   score: 6.0   memory length: 585740   epsilon: 0.038232820008541085    steps: 335    lr: 2.560000000000001e-06     evaluation reward: 7.9\n",
      "episode: 2248   score: 8.0   memory length: 586211   epsilon: 0.03730024000854172    steps: 471    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2249   score: 14.0   memory length: 586773   epsilon: 0.03618748000854248    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 7.95\n",
      "episode: 2250   score: 10.0   memory length: 587265   epsilon: 0.035213320008543145    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 7.98\n",
      "episode: 2251   score: 7.0   memory length: 587669   epsilon: 0.03441340000854369    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2252   score: 14.0   memory length: 588209   epsilon: 0.03334420000854442    steps: 540    lr: 2.560000000000001e-06     evaluation reward: 8.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Start training after random sample generation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(frame \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_frame): \u001b[38;5;66;03m# You can set train_frame to a lower value while testing your starts training earlier\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Update the target network only for Double DQN only\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m double_dqn \u001b[38;5;129;01mand\u001b[39;00m (frame \u001b[38;5;241m%\u001b[39m update_target_network_frequency)\u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/assignment5/agent.py:66\u001b[0m, in \u001b[0;36mAgent.train_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay\n\u001b[0;32m---> 66\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mini_batch, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     69\u001b[0m history \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(mini_batch[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/assignment5/memory.py:28\u001b[0m, in \u001b[0;36mReplayMemory.sample_mini_batch\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     26\u001b[0m sample \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(HISTORY_SIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     sample\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     30\u001b[0m sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sample, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m     31\u001b[0m mini_batch\u001b[38;5;241m.\u001b[39mappend((np\u001b[38;5;241m.\u001b[39mstack(sample[:, \u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action + 1)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 6 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                save_dir = \"./save_model/breakout_dqn_\"+str(e)+\".pth\"\n",
    "                torch.save(agent.policy_net, save_dir)\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trianing a double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 249   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 3.0   memory length: 497   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 2   score: 2.0   memory length: 694   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 2.6666666666666665\n",
      "episode: 3   score: 2.0   memory length: 891   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 4   score: 3.0   memory length: 1138   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 5   score: 0.0   memory length: 1260   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 2.1666666666666665\n",
      "episode: 6   score: 4.0   memory length: 1517   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 2.4285714285714284\n",
      "episode: 7   score: 1.0   memory length: 1685   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 8   score: 0.0   memory length: 1808   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 9   score: 3.0   memory length: 2018   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 10   score: 0.0   memory length: 2141   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9090909090909092\n",
      "episode: 11   score: 0.0   memory length: 2263   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 12   score: 2.0   memory length: 2479   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.7692307692307692\n",
      "episode: 13   score: 0.0   memory length: 2601   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 14   score: 4.0   memory length: 2879   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 15   score: 2.0   memory length: 3077   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8125\n",
      "episode: 16   score: 3.0   memory length: 3306   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.8823529411764706\n",
      "episode: 17   score: 0.0   memory length: 3429   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7777777777777777\n",
      "episode: 18   score: 6.0   memory length: 3804   epsilon: 1.0    steps: 375    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 19   score: 4.0   memory length: 4080   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 20   score: 3.0   memory length: 4327   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 2.142857142857143\n",
      "episode: 21   score: 0.0   memory length: 4449   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 2.0454545454545454\n",
      "episode: 22   score: 3.0   memory length: 4678   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 2.0869565217391304\n",
      "episode: 23   score: 0.0   memory length: 4801   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 24   score: 3.0   memory length: 5046   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 25   score: 3.0   memory length: 5275   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 2.076923076923077\n",
      "episode: 26   score: 0.0   memory length: 5397   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 27   score: 1.0   memory length: 5566   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.9642857142857142\n",
      "episode: 28   score: 0.0   memory length: 5689   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.896551724137931\n",
      "episode: 29   score: 4.0   memory length: 5988   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.9666666666666666\n",
      "episode: 30   score: 0.0   memory length: 6111   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.903225806451613\n",
      "episode: 31   score: 0.0   memory length: 6234   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.84375\n",
      "episode: 32   score: 0.0   memory length: 6357   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7878787878787878\n",
      "episode: 33   score: 0.0   memory length: 6479   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7352941176470589\n",
      "episode: 34   score: 2.0   memory length: 6694   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.7428571428571429\n",
      "episode: 35   score: 0.0   memory length: 6816   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6944444444444444\n",
      "episode: 36   score: 2.0   memory length: 7014   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7027027027027026\n",
      "episode: 37   score: 2.0   memory length: 7212   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7105263157894737\n",
      "episode: 38   score: 6.0   memory length: 7548   epsilon: 1.0    steps: 336    lr: 0.0001     evaluation reward: 1.8205128205128205\n",
      "episode: 39   score: 2.0   memory length: 7748   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.825\n",
      "episode: 40   score: 2.0   memory length: 7965   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.829268292682927\n",
      "episode: 41   score: 2.0   memory length: 8164   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 42   score: 1.0   memory length: 8333   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.813953488372093\n",
      "episode: 43   score: 1.0   memory length: 8501   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.7954545454545454\n",
      "episode: 44   score: 1.0   memory length: 8670   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7777777777777777\n",
      "episode: 45   score: 4.0   memory length: 8991   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.826086956521739\n",
      "episode: 46   score: 1.0   memory length: 9142   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8085106382978724\n",
      "episode: 47   score: 1.0   memory length: 9312   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7916666666666667\n",
      "episode: 48   score: 3.0   memory length: 9540   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.816326530612245\n",
      "episode: 49   score: 3.0   memory length: 9788   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 50   score: 2.0   memory length: 9986   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8431372549019607\n",
      "episode: 51   score: 3.0   memory length: 10251   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.8653846153846154\n",
      "episode: 52   score: 3.0   memory length: 10477   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.8867924528301887\n",
      "episode: 53   score: 4.0   memory length: 10743   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.9259259259259258\n",
      "episode: 54   score: 0.0   memory length: 10866   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8909090909090909\n",
      "episode: 55   score: 2.0   memory length: 11048   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.8928571428571428\n",
      "episode: 56   score: 2.0   memory length: 11245   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.894736842105263\n",
      "episode: 57   score: 3.0   memory length: 11512   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.9137931034482758\n",
      "episode: 58   score: 1.0   memory length: 11662   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.8983050847457628\n",
      "episode: 59   score: 4.0   memory length: 11940   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.9333333333333333\n",
      "episode: 60   score: 1.0   memory length: 12091   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.9180327868852458\n",
      "episode: 61   score: 3.0   memory length: 12301   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.935483870967742\n",
      "episode: 62   score: 0.0   memory length: 12424   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9047619047619047\n",
      "episode: 63   score: 3.0   memory length: 12671   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.921875\n",
      "episode: 64   score: 1.0   memory length: 12839   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.9076923076923078\n",
      "episode: 65   score: 5.0   memory length: 13181   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.9545454545454546\n",
      "episode: 66   score: 5.0   memory length: 13491   epsilon: 1.0    steps: 310    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 67   score: 0.0   memory length: 13613   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.9705882352941178\n",
      "episode: 68   score: 0.0   memory length: 13736   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9420289855072463\n",
      "episode: 69   score: 0.0   memory length: 13859   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9142857142857144\n",
      "episode: 70   score: 2.0   memory length: 14056   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.9154929577464788\n",
      "episode: 71   score: 0.0   memory length: 14179   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8888888888888888\n",
      "episode: 72   score: 2.0   memory length: 14376   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8904109589041096\n",
      "episode: 73   score: 0.0   memory length: 14499   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.864864864864865\n",
      "episode: 74   score: 1.0   memory length: 14650   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8533333333333333\n",
      "episode: 75   score: 1.0   memory length: 14819   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.8421052631578947\n",
      "episode: 76   score: 3.0   memory length: 15066   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.8571428571428572\n",
      "episode: 77   score: 3.0   memory length: 15294   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.8717948717948718\n",
      "episode: 78   score: 0.0   memory length: 15417   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8481012658227849\n",
      "episode: 79   score: 2.0   memory length: 15615   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 80   score: 0.0   memory length: 15738   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8271604938271604\n",
      "episode: 81   score: 1.0   memory length: 15888   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.8170731707317074\n",
      "episode: 82   score: 1.0   memory length: 16038   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.8072289156626506\n",
      "episode: 83   score: 1.0   memory length: 16207   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7976190476190477\n",
      "episode: 84   score: 1.0   memory length: 16376   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7882352941176471\n",
      "episode: 85   score: 3.0   memory length: 16622   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.802325581395349\n",
      "episode: 86   score: 1.0   memory length: 16772   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.793103448275862\n",
      "episode: 87   score: 0.0   memory length: 16895   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7727272727272727\n",
      "episode: 88   score: 0.0   memory length: 17018   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.752808988764045\n",
      "episode: 89   score: 4.0   memory length: 17312   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.7777777777777777\n",
      "episode: 90   score: 2.0   memory length: 17510   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7802197802197801\n",
      "episode: 91   score: 0.0   memory length: 17633   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7608695652173914\n",
      "episode: 92   score: 2.0   memory length: 17830   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7634408602150538\n",
      "episode: 93   score: 0.0   memory length: 17953   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7446808510638299\n",
      "episode: 94   score: 3.0   memory length: 18198   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.7578947368421052\n",
      "episode: 95   score: 1.0   memory length: 18367   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 96   score: 2.0   memory length: 18565   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7525773195876289\n",
      "episode: 97   score: 4.0   memory length: 18841   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.7755102040816326\n",
      "episode: 98   score: 3.0   memory length: 19068   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.7878787878787878\n",
      "episode: 99   score: 1.0   memory length: 19236   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 100   score: 1.0   memory length: 19387   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 101   score: 1.0   memory length: 19537   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 102   score: 3.0   memory length: 19784   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 103   score: 1.0   memory length: 19935   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 104   score: 2.0   memory length: 20136   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 105   score: 2.0   memory length: 20334   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 106   score: 1.0   memory length: 20505   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 107   score: 3.0   memory length: 20734   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 108   score: 1.0   memory length: 20884   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 109   score: 0.0   memory length: 21006   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 110   score: 0.0   memory length: 21129   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 111   score: 2.0   memory length: 21327   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 112   score: 2.0   memory length: 21548   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 113   score: 2.0   memory length: 21745   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 114   score: 1.0   memory length: 21916   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 115   score: 2.0   memory length: 22134   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 116   score: 4.0   memory length: 22410   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 117   score: 3.0   memory length: 22657   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 118   score: 0.0   memory length: 22779   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 119   score: 1.0   memory length: 22929   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 120   score: 1.0   memory length: 23079   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 121   score: 4.0   memory length: 23377   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 122   score: 2.0   memory length: 23579   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 123   score: 1.0   memory length: 23750   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 124   score: 0.0   memory length: 23872   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 125   score: 0.0   memory length: 23995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 126   score: 2.0   memory length: 24193   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 127   score: 3.0   memory length: 24423   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 128   score: 1.0   memory length: 24593   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 129   score: 1.0   memory length: 24744   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 130   score: 1.0   memory length: 24915   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 131   score: 0.0   memory length: 25038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 132   score: 1.0   memory length: 25209   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 133   score: 2.0   memory length: 25426   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 134   score: 1.0   memory length: 25595   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 135   score: 2.0   memory length: 25796   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 136   score: 2.0   memory length: 26012   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 137   score: 2.0   memory length: 26230   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 138   score: 2.0   memory length: 26448   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 139   score: 2.0   memory length: 26645   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 140   score: 1.0   memory length: 26813   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 141   score: 2.0   memory length: 27030   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 142   score: 0.0   memory length: 27152   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 143   score: 1.0   memory length: 27323   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 144   score: 2.0   memory length: 27521   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 145   score: 2.0   memory length: 27737   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 146   score: 1.0   memory length: 27888   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 147   score: 1.0   memory length: 28039   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 148   score: 3.0   memory length: 28265   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 149   score: 1.0   memory length: 28437   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 150   score: 3.0   memory length: 28687   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 151   score: 1.0   memory length: 28837   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 152   score: 1.0   memory length: 29006   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 153   score: 0.0   memory length: 29129   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 154   score: 4.0   memory length: 29405   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 155   score: 0.0   memory length: 29528   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 156   score: 1.0   memory length: 29679   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 157   score: 0.0   memory length: 29802   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 158   score: 0.0   memory length: 29924   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 159   score: 2.0   memory length: 30142   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 160   score: 0.0   memory length: 30265   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 161   score: 2.0   memory length: 30480   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 162   score: 4.0   memory length: 30802   epsilon: 1.0    steps: 322    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 163   score: 0.0   memory length: 30924   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 164   score: 2.0   memory length: 31143   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 165   score: 1.0   memory length: 31312   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 166   score: 2.0   memory length: 31529   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 167   score: 2.0   memory length: 31746   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 168   score: 1.0   memory length: 31917   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 169   score: 6.0   memory length: 32169   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 170   score: 0.0   memory length: 32292   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 171   score: 2.0   memory length: 32508   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 172   score: 0.0   memory length: 32630   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 173   score: 4.0   memory length: 32920   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 174   score: 2.0   memory length: 33136   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 175   score: 1.0   memory length: 33308   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 176   score: 0.0   memory length: 33430   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 177   score: 2.0   memory length: 33612   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 178   score: 2.0   memory length: 33828   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 179   score: 0.0   memory length: 33950   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 180   score: 2.0   memory length: 34129   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 181   score: 1.0   memory length: 34301   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 182   score: 0.0   memory length: 34424   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 183   score: 3.0   memory length: 34688   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 184   score: 4.0   memory length: 34965   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 185   score: 0.0   memory length: 35088   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 186   score: 0.0   memory length: 35210   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 187   score: 4.0   memory length: 35504   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 188   score: 1.0   memory length: 35675   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 189   score: 0.0   memory length: 35798   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 190   score: 7.0   memory length: 36217   epsilon: 1.0    steps: 419    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 191   score: 1.0   memory length: 36385   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 192   score: 0.0   memory length: 36508   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 193   score: 3.0   memory length: 36775   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 194   score: 1.0   memory length: 36944   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 195   score: 0.0   memory length: 37067   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 196   score: 1.0   memory length: 37218   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 197   score: 2.0   memory length: 37416   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 198   score: 0.0   memory length: 37539   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 199   score: 2.0   memory length: 37758   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 200   score: 1.0   memory length: 37927   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 201   score: 0.0   memory length: 38049   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 202   score: 0.0   memory length: 38172   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 203   score: 2.0   memory length: 38369   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 204   score: 2.0   memory length: 38567   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 205   score: 4.0   memory length: 38835   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 206   score: 3.0   memory length: 39046   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 207   score: 2.0   memory length: 39264   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 208   score: 2.0   memory length: 39462   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 209   score: 3.0   memory length: 39709   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 210   score: 3.0   memory length: 39976   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 211   score: 3.0   memory length: 40222   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 212   score: 2.0   memory length: 40420   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 213   score: 1.0   memory length: 40590   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 214   score: 2.0   memory length: 40788   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 215   score: 1.0   memory length: 40957   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 216   score: 3.0   memory length: 41203   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 217   score: 0.0   memory length: 41325   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 218   score: 1.0   memory length: 41477   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 219   score: 4.0   memory length: 41771   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 220   score: 2.0   memory length: 41969   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 221   score: 3.0   memory length: 42194   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 222   score: 0.0   memory length: 42317   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 223   score: 1.0   memory length: 42468   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 224   score: 3.0   memory length: 42699   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 225   score: 4.0   memory length: 42992   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 226   score: 3.0   memory length: 43261   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 227   score: 2.0   memory length: 43460   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 228   score: 0.0   memory length: 43583   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 229   score: 1.0   memory length: 43734   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 230   score: 1.0   memory length: 43885   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 231   score: 2.0   memory length: 44086   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 232   score: 2.0   memory length: 44284   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 233   score: 0.0   memory length: 44407   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 234   score: 1.0   memory length: 44577   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 235   score: 4.0   memory length: 44853   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 236   score: 2.0   memory length: 45056   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 237   score: 1.0   memory length: 45207   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 238   score: 1.0   memory length: 45375   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 239   score: 2.0   memory length: 45593   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 240   score: 1.0   memory length: 45743   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 241   score: 2.0   memory length: 45941   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 242   score: 3.0   memory length: 46190   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 243   score: 0.0   memory length: 46313   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 244   score: 0.0   memory length: 46436   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 245   score: 2.0   memory length: 46634   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 246   score: 0.0   memory length: 46757   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 247   score: 2.0   memory length: 46954   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 248   score: 5.0   memory length: 47289   epsilon: 1.0    steps: 335    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 249   score: 1.0   memory length: 47461   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 250   score: 2.0   memory length: 47676   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 251   score: 1.0   memory length: 47846   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 252   score: 0.0   memory length: 47969   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 253   score: 2.0   memory length: 48167   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 254   score: 3.0   memory length: 48412   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 255   score: 0.0   memory length: 48534   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 256   score: 0.0   memory length: 48657   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 257   score: 1.0   memory length: 48807   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 258   score: 0.0   memory length: 48930   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 259   score: 1.0   memory length: 49081   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 260   score: 1.0   memory length: 49249   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 261   score: 1.0   memory length: 49399   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 262   score: 2.0   memory length: 49597   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 263   score: 0.0   memory length: 49720   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 264   score: 4.0   memory length: 50017   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 265   score: 4.0   memory length: 50313   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 266   score: 3.0   memory length: 50543   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 267   score: 0.0   memory length: 50665   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 268   score: 0.0   memory length: 50787   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 269   score: 3.0   memory length: 51034   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 270   score: 2.0   memory length: 51231   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 271   score: 1.0   memory length: 51400   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 272   score: 1.0   memory length: 51571   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 273   score: 3.0   memory length: 51838   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 274   score: 2.0   memory length: 52035   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 275   score: 0.0   memory length: 52158   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 276   score: 1.0   memory length: 52327   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 277   score: 0.0   memory length: 52450   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 278   score: 1.0   memory length: 52601   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 279   score: 1.0   memory length: 52752   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 280   score: 4.0   memory length: 53038   epsilon: 1.0    steps: 286    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 281   score: 1.0   memory length: 53188   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 282   score: 2.0   memory length: 53390   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 283   score: 3.0   memory length: 53657   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 284   score: 1.0   memory length: 53808   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 285   score: 0.0   memory length: 53931   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 286   score: 1.0   memory length: 54082   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 287   score: 2.0   memory length: 54300   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 288   score: 1.0   memory length: 54472   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 289   score: 3.0   memory length: 54717   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 290   score: 0.0   memory length: 54840   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 291   score: 6.0   memory length: 55188   epsilon: 1.0    steps: 348    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 292   score: 3.0   memory length: 55455   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 293   score: 1.0   memory length: 55606   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 294   score: 1.0   memory length: 55757   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 295   score: 1.0   memory length: 55925   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 296   score: 5.0   memory length: 56231   epsilon: 1.0    steps: 306    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 297   score: 1.0   memory length: 56400   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 298   score: 1.0   memory length: 56568   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 299   score: 2.0   memory length: 56765   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 300   score: 2.0   memory length: 56962   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 301   score: 2.0   memory length: 57162   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 302   score: 0.0   memory length: 57285   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 303   score: 2.0   memory length: 57482   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 304   score: 1.0   memory length: 57651   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 305   score: 3.0   memory length: 57877   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 306   score: 1.0   memory length: 58028   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 307   score: 0.0   memory length: 58151   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 308   score: 2.0   memory length: 58332   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 309   score: 4.0   memory length: 58587   epsilon: 1.0    steps: 255    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 310   score: 3.0   memory length: 58815   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 311   score: 0.0   memory length: 58938   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 312   score: 2.0   memory length: 59136   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 313   score: 4.0   memory length: 59432   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 314   score: 3.0   memory length: 59702   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 315   score: 1.0   memory length: 59872   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 316   score: 2.0   memory length: 60089   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 317   score: 0.0   memory length: 60212   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 318   score: 1.0   memory length: 60381   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 319   score: 1.0   memory length: 60549   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 320   score: 1.0   memory length: 60717   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 321   score: 0.0   memory length: 60839   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 322   score: 3.0   memory length: 61088   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 323   score: 1.0   memory length: 61257   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 324   score: 1.0   memory length: 61426   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 325   score: 6.0   memory length: 61808   epsilon: 1.0    steps: 382    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 326   score: 1.0   memory length: 61959   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 327   score: 7.0   memory length: 62372   epsilon: 1.0    steps: 413    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 328   score: 0.0   memory length: 62495   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 329   score: 3.0   memory length: 62759   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 330   score: 1.0   memory length: 62909   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 331   score: 0.0   memory length: 63031   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 332   score: 3.0   memory length: 63259   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 333   score: 3.0   memory length: 63523   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 334   score: 2.0   memory length: 63721   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 335   score: 2.0   memory length: 63919   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 336   score: 1.0   memory length: 64089   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 337   score: 2.0   memory length: 64287   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 338   score: 1.0   memory length: 64458   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 339   score: 2.0   memory length: 64655   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 340   score: 1.0   memory length: 64825   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 341   score: 1.0   memory length: 64996   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 342   score: 1.0   memory length: 65167   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 343   score: 3.0   memory length: 65411   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 344   score: 2.0   memory length: 65609   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 345   score: 0.0   memory length: 65732   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 346   score: 0.0   memory length: 65855   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 347   score: 1.0   memory length: 66006   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 348   score: 3.0   memory length: 66235   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 349   score: 1.0   memory length: 66404   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 350   score: 0.0   memory length: 66526   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 351   score: 1.0   memory length: 66678   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 352   score: 3.0   memory length: 66925   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 353   score: 3.0   memory length: 67150   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 354   score: 2.0   memory length: 67347   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 355   score: 2.0   memory length: 67564   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 356   score: 2.0   memory length: 67744   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 357   score: 0.0   memory length: 67867   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 358   score: 1.0   memory length: 68038   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 359   score: 2.0   memory length: 68235   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 360   score: 0.0   memory length: 68358   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 361   score: 0.0   memory length: 68481   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 362   score: 0.0   memory length: 68604   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 363   score: 3.0   memory length: 68872   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 364   score: 1.0   memory length: 69023   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 365   score: 2.0   memory length: 69221   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 366   score: 3.0   memory length: 69450   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 367   score: 1.0   memory length: 69601   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 368   score: 1.0   memory length: 69771   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 369   score: 1.0   memory length: 69939   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 370   score: 1.0   memory length: 70109   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 371   score: 5.0   memory length: 70415   epsilon: 1.0    steps: 306    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 372   score: 3.0   memory length: 70683   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 373   score: 0.0   memory length: 70806   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 374   score: 0.0   memory length: 70928   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 375   score: 0.0   memory length: 71051   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 376   score: 1.0   memory length: 71221   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 377   score: 0.0   memory length: 71344   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 378   score: 0.0   memory length: 71467   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 379   score: 0.0   memory length: 71590   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 380   score: 3.0   memory length: 71817   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 381   score: 2.0   memory length: 71998   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 382   score: 0.0   memory length: 72121   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 383   score: 0.0   memory length: 72243   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 384   score: 1.0   memory length: 72394   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 385   score: 2.0   memory length: 72592   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 386   score: 4.0   memory length: 72889   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 387   score: 0.0   memory length: 73012   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 388   score: 1.0   memory length: 73164   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 389   score: 0.0   memory length: 73287   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 390   score: 0.0   memory length: 73410   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 391   score: 4.0   memory length: 73699   epsilon: 1.0    steps: 289    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 392   score: 1.0   memory length: 73869   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 393   score: 2.0   memory length: 74069   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 394   score: 1.0   memory length: 74240   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 395   score: 2.0   memory length: 74458   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 396   score: 1.0   memory length: 74609   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 397   score: 2.0   memory length: 74827   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 398   score: 1.0   memory length: 74998   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 399   score: 0.0   memory length: 75121   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 400   score: 1.0   memory length: 75272   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 401   score: 0.0   memory length: 75395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 402   score: 1.0   memory length: 75565   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 403   score: 3.0   memory length: 75796   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 404   score: 3.0   memory length: 76044   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 405   score: 2.0   memory length: 76242   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 406   score: 1.0   memory length: 76412   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 407   score: 2.0   memory length: 76614   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 408   score: 0.0   memory length: 76737   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 409   score: 1.0   memory length: 76905   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 410   score: 1.0   memory length: 77076   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 411   score: 2.0   memory length: 77274   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 412   score: 0.0   memory length: 77396   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 413   score: 0.0   memory length: 77519   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 414   score: 5.0   memory length: 77861   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 415   score: 0.0   memory length: 77983   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 416   score: 0.0   memory length: 78105   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 417   score: 0.0   memory length: 78227   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 418   score: 1.0   memory length: 78378   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 419   score: 0.0   memory length: 78501   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 420   score: 0.0   memory length: 78624   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 421   score: 1.0   memory length: 78793   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 422   score: 0.0   memory length: 78916   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 423   score: 0.0   memory length: 79039   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 424   score: 3.0   memory length: 79289   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 425   score: 3.0   memory length: 79533   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 426   score: 0.0   memory length: 79656   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 427   score: 0.0   memory length: 79779   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 428   score: 0.0   memory length: 79902   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 429   score: 3.0   memory length: 80152   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 430   score: 1.0   memory length: 80320   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 431   score: 0.0   memory length: 80443   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 432   score: 3.0   memory length: 80671   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 433   score: 1.0   memory length: 80823   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 434   score: 0.0   memory length: 80946   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 435   score: 0.0   memory length: 81069   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 436   score: 2.0   memory length: 81287   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 437   score: 4.0   memory length: 81584   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 438   score: 2.0   memory length: 81786   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 439   score: 1.0   memory length: 81955   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 440   score: 2.0   memory length: 82154   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 441   score: 0.0   memory length: 82276   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 442   score: 2.0   memory length: 82494   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 443   score: 1.0   memory length: 82663   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 444   score: 0.0   memory length: 82786   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 445   score: 2.0   memory length: 83004   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 446   score: 4.0   memory length: 83259   epsilon: 1.0    steps: 255    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 447   score: 2.0   memory length: 83441   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 448   score: 1.0   memory length: 83592   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 449   score: 2.0   memory length: 83790   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 450   score: 0.0   memory length: 83912   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 451   score: 0.0   memory length: 84035   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 452   score: 2.0   memory length: 84233   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 453   score: 0.0   memory length: 84356   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 454   score: 1.0   memory length: 84525   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 455   score: 1.0   memory length: 84694   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 456   score: 2.0   memory length: 84912   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 457   score: 1.0   memory length: 85081   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 458   score: 0.0   memory length: 85203   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 459   score: 2.0   memory length: 85390   epsilon: 1.0    steps: 187    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 460   score: 0.0   memory length: 85513   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 461   score: 2.0   memory length: 85710   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 462   score: 1.0   memory length: 85860   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 463   score: 3.0   memory length: 86088   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 464   score: 0.0   memory length: 86211   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 465   score: 4.0   memory length: 86525   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 466   score: 2.0   memory length: 86722   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 467   score: 1.0   memory length: 86891   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 468   score: 0.0   memory length: 87014   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 469   score: 1.0   memory length: 87185   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 470   score: 1.0   memory length: 87353   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 471   score: 5.0   memory length: 87700   epsilon: 1.0    steps: 347    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 472   score: 3.0   memory length: 87947   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 473   score: 2.0   memory length: 88145   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 474   score: 0.0   memory length: 88267   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 475   score: 1.0   memory length: 88437   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 476   score: 3.0   memory length: 88702   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 477   score: 0.0   memory length: 88824   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 478   score: 2.0   memory length: 89044   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 479   score: 2.0   memory length: 89242   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 480   score: 3.0   memory length: 89452   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 481   score: 2.0   memory length: 89649   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 482   score: 4.0   memory length: 89925   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 483   score: 3.0   memory length: 90190   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 484   score: 2.0   memory length: 90406   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 485   score: 2.0   memory length: 90586   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 486   score: 4.0   memory length: 90852   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 487   score: 1.0   memory length: 91002   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 488   score: 0.0   memory length: 91124   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 489   score: 3.0   memory length: 91368   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 490   score: 1.0   memory length: 91538   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 491   score: 4.0   memory length: 91816   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 492   score: 1.0   memory length: 91985   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 493   score: 5.0   memory length: 92323   epsilon: 1.0    steps: 338    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 494   score: 3.0   memory length: 92552   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 495   score: 4.0   memory length: 92845   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 496   score: 1.0   memory length: 93017   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 497   score: 0.0   memory length: 93140   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 498   score: 0.0   memory length: 93263   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 499   score: 0.0   memory length: 93386   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 500   score: 0.0   memory length: 93509   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 501   score: 2.0   memory length: 93706   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 502   score: 3.0   memory length: 93953   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 503   score: 1.0   memory length: 94123   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 504   score: 4.0   memory length: 94405   epsilon: 1.0    steps: 282    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 505   score: 0.0   memory length: 94528   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 506   score: 0.0   memory length: 94651   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 507   score: 1.0   memory length: 94820   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 508   score: 0.0   memory length: 94943   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 509   score: 3.0   memory length: 95188   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 510   score: 3.0   memory length: 95455   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 511   score: 1.0   memory length: 95626   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 512   score: 1.0   memory length: 95777   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 513   score: 1.0   memory length: 95928   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 514   score: 2.0   memory length: 96146   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 515   score: 1.0   memory length: 96297   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 516   score: 3.0   memory length: 96563   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 517   score: 1.0   memory length: 96732   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 518   score: 3.0   memory length: 96978   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 519   score: 0.0   memory length: 97101   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 520   score: 0.0   memory length: 97224   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 521   score: 2.0   memory length: 97421   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 522   score: 1.0   memory length: 97571   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 523   score: 0.0   memory length: 97694   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 524   score: 2.0   memory length: 97894   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 525   score: 2.0   memory length: 98074   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 526   score: 3.0   memory length: 98339   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 527   score: 0.0   memory length: 98462   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 528   score: 0.0   memory length: 98584   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 529   score: 2.0   memory length: 98782   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 530   score: 0.0   memory length: 98905   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 531   score: 3.0   memory length: 99152   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 532   score: 2.0   memory length: 99372   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 533   score: 0.0   memory length: 99495   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 534   score: 0.0   memory length: 99618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 535   score: 1.0   memory length: 99787   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 536   score: 1.0   memory length: 99957   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/assignment5/memory.py:30: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample = np.array(sample, dtype=object)\n",
      "/home/wxzhang/assignment5/agent_double.py:78: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  mini_batch = np.array(mini_batch, dtype=object).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 537   score: 1.0   memory length: 100108   epsilon: 0.9997841800000047    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 538   score: 1.0   memory length: 100278   epsilon: 0.999447580000012    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 539   score: 0.0   memory length: 100401   epsilon: 0.9992040400000173    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 540   score: 3.0   memory length: 100631   epsilon: 0.9987486400000272    steps: 230    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 541   score: 1.0   memory length: 100802   epsilon: 0.9984100600000345    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 542   score: 1.0   memory length: 100952   epsilon: 0.998113060000041    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 543   score: 1.0   memory length: 101103   epsilon: 0.9978140800000475    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 544   score: 7.0   memory length: 101513   epsilon: 0.9970022800000651    steps: 410    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 545   score: 3.0   memory length: 101763   epsilon: 0.9965072800000758    steps: 250    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 546   score: 1.0   memory length: 101932   epsilon: 0.9961726600000831    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 547   score: 1.0   memory length: 102084   epsilon: 0.9958717000000896    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 548   score: 2.0   memory length: 102300   epsilon: 0.9954440200000989    steps: 216    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 549   score: 5.0   memory length: 102627   epsilon: 0.994796560000113    steps: 327    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 550   score: 2.0   memory length: 102844   epsilon: 0.9943669000001223    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 551   score: 0.0   memory length: 102966   epsilon: 0.9941253400001275    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 552   score: 1.0   memory length: 103135   epsilon: 0.9937907200001348    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 553   score: 1.0   memory length: 103307   epsilon: 0.9934501600001422    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 554   score: 0.0   memory length: 103430   epsilon: 0.9932066200001475    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 555   score: 2.0   memory length: 103648   epsilon: 0.9927749800001568    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 556   score: 2.0   memory length: 103828   epsilon: 0.9924185800001646    steps: 180    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 557   score: 0.0   memory length: 103951   epsilon: 0.9921750400001699    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 558   score: 3.0   memory length: 104218   epsilon: 0.9916463800001813    steps: 267    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 559   score: 3.0   memory length: 104465   epsilon: 0.991157320000192    steps: 247    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 560   score: 2.0   memory length: 104663   epsilon: 0.9907652800002005    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 561   score: 1.0   memory length: 104814   epsilon: 0.990466300000207    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 562   score: 4.0   memory length: 105089   epsilon: 0.9899218000002188    steps: 275    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 563   score: 3.0   memory length: 105320   epsilon: 0.9894644200002287    steps: 231    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 564   score: 1.0   memory length: 105471   epsilon: 0.9891654400002352    steps: 151    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 565   score: 0.0   memory length: 105594   epsilon: 0.9889219000002405    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 566   score: 0.0   memory length: 105717   epsilon: 0.9886783600002458    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 567   score: 4.0   memory length: 106027   epsilon: 0.9880645600002591    steps: 310    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 568   score: 2.0   memory length: 106248   epsilon: 0.9876269800002686    steps: 221    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 569   score: 0.0   memory length: 106371   epsilon: 0.9873834400002739    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 570   score: 5.0   memory length: 106667   epsilon: 0.9867973600002866    steps: 296    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 571   score: 3.0   memory length: 106932   epsilon: 0.986272660000298    steps: 265    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 572   score: 5.0   memory length: 107297   epsilon: 0.9855499600003137    steps: 365    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 573   score: 0.0   memory length: 107420   epsilon: 0.985306420000319    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 574   score: 2.0   memory length: 107636   epsilon: 0.9848787400003283    steps: 216    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 575   score: 3.0   memory length: 107884   epsilon: 0.9843877000003389    steps: 248    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 576   score: 3.0   memory length: 108131   epsilon: 0.9838986400003495    steps: 247    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 577   score: 0.0   memory length: 108254   epsilon: 0.9836551000003548    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 578   score: 2.0   memory length: 108454   epsilon: 0.9832591000003634    steps: 200    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 579   score: 1.0   memory length: 108605   epsilon: 0.9829601200003699    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 580   score: 2.0   memory length: 108786   epsilon: 0.9826017400003777    steps: 181    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 581   score: 3.0   memory length: 109002   epsilon: 0.982174060000387    steps: 216    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 582   score: 1.0   memory length: 109154   epsilon: 0.9818731000003935    steps: 152    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 583   score: 1.0   memory length: 109304   epsilon: 0.9815761000004    steps: 150    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 584   score: 3.0   memory length: 109519   epsilon: 0.9811504000004092    steps: 215    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 585   score: 2.0   memory length: 109717   epsilon: 0.9807583600004177    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 586   score: 1.0   memory length: 109886   epsilon: 0.980423740000425    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 587   score: 1.0   memory length: 110054   epsilon: 0.9800911000004322    steps: 168    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 588   score: 2.0   memory length: 110252   epsilon: 0.9796990600004407    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 589   score: 1.0   memory length: 110422   epsilon: 0.979362460000448    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 590   score: 1.0   memory length: 110591   epsilon: 0.9790278400004553    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 591   score: 0.0   memory length: 110713   epsilon: 0.9787862800004605    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 592   score: 2.0   memory length: 110931   epsilon: 0.9783546400004699    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 593   score: 3.0   memory length: 111148   epsilon: 0.9779249800004792    steps: 217    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 594   score: 1.0   memory length: 111299   epsilon: 0.9776260000004857    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 595   score: 0.0   memory length: 111421   epsilon: 0.977384440000491    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 596   score: 0.0   memory length: 111543   epsilon: 0.9771428800004962    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 597   score: 0.0   memory length: 111665   epsilon: 0.9769013200005014    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 598   score: 4.0   memory length: 111927   epsilon: 0.9763825600005127    steps: 262    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 599   score: 4.0   memory length: 112243   epsilon: 0.9757568800005263    steps: 316    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 600   score: 1.0   memory length: 112394   epsilon: 0.9754579000005328    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 601   score: 0.0   memory length: 112516   epsilon: 0.975216340000538    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 602   score: 2.0   memory length: 112714   epsilon: 0.9748243000005465    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 603   score: 0.0   memory length: 112837   epsilon: 0.9745807600005518    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 604   score: 1.0   memory length: 112989   epsilon: 0.9742798000005584    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 605   score: 3.0   memory length: 113256   epsilon: 0.9737511400005698    steps: 267    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 606   score: 1.0   memory length: 113428   epsilon: 0.9734105800005772    steps: 172    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 607   score: 1.0   memory length: 113597   epsilon: 0.9730759600005845    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 608   score: 3.0   memory length: 113842   epsilon: 0.972590860000595    steps: 245    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 609   score: 2.0   memory length: 114022   epsilon: 0.9722344600006028    steps: 180    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 610   score: 3.0   memory length: 114270   epsilon: 0.9717434200006134    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 611   score: 1.0   memory length: 114440   epsilon: 0.9714068200006207    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 612   score: 1.0   memory length: 114591   epsilon: 0.9711078400006272    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 613   score: 2.0   memory length: 114807   epsilon: 0.9706801600006365    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 614   score: 2.0   memory length: 115005   epsilon: 0.970288120000645    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 615   score: 0.0   memory length: 115127   epsilon: 0.9700465600006503    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 616   score: 5.0   memory length: 115452   epsilon: 0.9694030600006642    steps: 325    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 617   score: 0.0   memory length: 115575   epsilon: 0.9691595200006695    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 618   score: 2.0   memory length: 115773   epsilon: 0.968767480000678    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 619   score: 0.0   memory length: 115895   epsilon: 0.9685259200006833    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 620   score: 2.0   memory length: 116114   epsilon: 0.9680923000006927    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 621   score: 1.0   memory length: 116283   epsilon: 0.9677576800007    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 622   score: 0.0   memory length: 116406   epsilon: 0.9675141400007052    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 623   score: 1.0   memory length: 116576   epsilon: 0.9671775400007125    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 624   score: 0.0   memory length: 116698   epsilon: 0.9669359800007178    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 625   score: 1.0   memory length: 116869   epsilon: 0.9665974000007251    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 626   score: 1.0   memory length: 117038   epsilon: 0.9662627800007324    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 627   score: 0.0   memory length: 117160   epsilon: 0.9660212200007376    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 628   score: 0.0   memory length: 117282   epsilon: 0.9657796600007429    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 629   score: 0.0   memory length: 117404   epsilon: 0.9655381000007481    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 630   score: 2.0   memory length: 117602   epsilon: 0.9651460600007566    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 631   score: 0.0   memory length: 117724   epsilon: 0.9649045000007619    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 632   score: 0.0   memory length: 117847   epsilon: 0.9646609600007672    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 633   score: 1.0   memory length: 118017   epsilon: 0.9643243600007745    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 634   score: 2.0   memory length: 118198   epsilon: 0.9639659800007823    steps: 181    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 635   score: 0.0   memory length: 118321   epsilon: 0.9637224400007876    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 636   score: 0.0   memory length: 118444   epsilon: 0.9634789000007928    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 637   score: 0.0   memory length: 118566   epsilon: 0.9632373400007981    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 638   score: 2.0   memory length: 118786   epsilon: 0.9628017400008075    steps: 220    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 639   score: 7.0   memory length: 119086   epsilon: 0.9622077400008204    steps: 300    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 640   score: 0.0   memory length: 119208   epsilon: 0.9619661800008257    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 641   score: 4.0   memory length: 119522   epsilon: 0.9613444600008392    steps: 314    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 642   score: 0.0   memory length: 119645   epsilon: 0.9611009200008445    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 643   score: 0.0   memory length: 119767   epsilon: 0.9608593600008497    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 644   score: 0.0   memory length: 119890   epsilon: 0.960615820000855    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 645   score: 2.0   memory length: 120088   epsilon: 0.9602237800008635    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 646   score: 2.0   memory length: 120285   epsilon: 0.959833720000872    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 647   score: 2.0   memory length: 120504   epsilon: 0.9594001000008814    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 648   score: 0.0   memory length: 120627   epsilon: 0.9591565600008867    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 649   score: 2.0   memory length: 120845   epsilon: 0.958724920000896    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 650   score: 0.0   memory length: 120968   epsilon: 0.9584813800009013    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 651   score: 0.0   memory length: 121090   epsilon: 0.9582398200009066    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 652   score: 0.0   memory length: 121213   epsilon: 0.9579962800009119    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 653   score: 0.0   memory length: 121336   epsilon: 0.9577527400009171    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 654   score: 0.0   memory length: 121459   epsilon: 0.9575092000009224    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 655   score: 0.0   memory length: 121582   epsilon: 0.9572656600009277    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 656   score: 3.0   memory length: 121810   epsilon: 0.9568142200009375    steps: 228    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 657   score: 2.0   memory length: 122027   epsilon: 0.9563845600009468    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 658   score: 1.0   memory length: 122196   epsilon: 0.9560499400009541    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 659   score: 2.0   memory length: 122397   epsilon: 0.9556519600009628    steps: 201    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 660   score: 0.0   memory length: 122519   epsilon: 0.955410400000968    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 661   score: 2.0   memory length: 122737   epsilon: 0.9549787600009774    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 662   score: 3.0   memory length: 122963   epsilon: 0.9545312800009871    steps: 226    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 663   score: 2.0   memory length: 123182   epsilon: 0.9540976600009965    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 664   score: 1.0   memory length: 123350   epsilon: 0.9537650200010037    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 665   score: 0.0   memory length: 123472   epsilon: 0.953523460001009    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 666   score: 0.0   memory length: 123594   epsilon: 0.9532819000010142    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 667   score: 3.0   memory length: 123837   epsilon: 0.9528007600010246    steps: 243    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 668   score: 2.0   memory length: 124056   epsilon: 0.9523671400010341    steps: 219    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 669   score: 4.0   memory length: 124349   epsilon: 0.9517870000010467    steps: 293    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 670   score: 0.0   memory length: 124472   epsilon: 0.9515434600010519    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 671   score: 3.0   memory length: 124718   epsilon: 0.9510563800010625    steps: 246    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 672   score: 2.0   memory length: 124938   epsilon: 0.950620780001072    steps: 220    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 673   score: 2.0   memory length: 125137   epsilon: 0.9502267600010805    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 674   score: 2.0   memory length: 125335   epsilon: 0.949834720001089    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 675   score: 4.0   memory length: 125630   epsilon: 0.9492506200011017    steps: 295    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 676   score: 3.0   memory length: 125877   epsilon: 0.9487615600011123    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 677   score: 0.0   memory length: 125999   epsilon: 0.9485200000011176    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 678   score: 1.0   memory length: 126150   epsilon: 0.9482210200011241    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 679   score: 2.0   memory length: 126351   epsilon: 0.9478230400011327    steps: 201    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 680   score: 0.0   memory length: 126473   epsilon: 0.947581480001138    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 681   score: 5.0   memory length: 126782   epsilon: 0.9469696600011512    steps: 309    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 682   score: 3.0   memory length: 127029   epsilon: 0.9464806000011619    steps: 247    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 683   score: 3.0   memory length: 127256   epsilon: 0.9460311400011716    steps: 227    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 684   score: 1.0   memory length: 127407   epsilon: 0.9457321600011781    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 685   score: 3.0   memory length: 127675   epsilon: 0.9452015200011896    steps: 268    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 686   score: 1.0   memory length: 127844   epsilon: 0.9448669000011969    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 687   score: 0.0   memory length: 127967   epsilon: 0.9446233600012022    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 688   score: 2.0   memory length: 128167   epsilon: 0.9442273600012108    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 689   score: 4.0   memory length: 128443   epsilon: 0.9436808800012226    steps: 276    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 690   score: 2.0   memory length: 128643   epsilon: 0.9432848800012312    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 691   score: 1.0   memory length: 128794   epsilon: 0.9429859000012377    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 692   score: 1.0   memory length: 128964   epsilon: 0.942649300001245    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 693   score: 5.0   memory length: 129285   epsilon: 0.9420137200012588    steps: 321    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 694   score: 4.0   memory length: 129587   epsilon: 0.9414157600012718    steps: 302    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 695   score: 0.0   memory length: 129710   epsilon: 0.9411722200012771    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 696   score: 1.0   memory length: 129861   epsilon: 0.9408732400012836    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 697   score: 0.0   memory length: 129984   epsilon: 0.9406297000012889    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 698   score: 0.0   memory length: 130107   epsilon: 0.9403861600012942    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 699   score: 1.0   memory length: 130277   epsilon: 0.9400495600013015    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 700   score: 0.0   memory length: 130400   epsilon: 0.9398060200013068    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 701   score: 1.0   memory length: 130551   epsilon: 0.9395070400013132    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 702   score: 3.0   memory length: 130795   epsilon: 0.9390239200013237    steps: 244    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 703   score: 2.0   memory length: 131014   epsilon: 0.9385903000013331    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 704   score: 2.0   memory length: 131212   epsilon: 0.9381982600013417    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 705   score: 1.0   memory length: 131363   epsilon: 0.9378992800013481    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 706   score: 3.0   memory length: 131608   epsilon: 0.9374141800013587    steps: 245    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 707   score: 2.0   memory length: 131824   epsilon: 0.936986500001368    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 708   score: 1.0   memory length: 131975   epsilon: 0.9366875200013745    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 709   score: 3.0   memory length: 132219   epsilon: 0.9362044000013849    steps: 244    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 710   score: 2.0   memory length: 132422   epsilon: 0.9358024600013937    steps: 203    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 711   score: 4.0   memory length: 132673   epsilon: 0.9353054800014045    steps: 251    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 712   score: 1.0   memory length: 132844   epsilon: 0.9349669000014118    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 713   score: 1.0   memory length: 133014   epsilon: 0.9346303000014191    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 714   score: 3.0   memory length: 133242   epsilon: 0.9341788600014289    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 715   score: 0.0   memory length: 133365   epsilon: 0.9339353200014342    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 716   score: 4.0   memory length: 133661   epsilon: 0.9333492400014469    steps: 296    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 717   score: 1.0   memory length: 133829   epsilon: 0.9330166000014541    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 718   score: 3.0   memory length: 134095   epsilon: 0.9324899200014656    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 719   score: 0.0   memory length: 134217   epsilon: 0.9322483600014708    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 720   score: 6.0   memory length: 134611   epsilon: 0.9314682400014878    steps: 394    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 721   score: 1.0   memory length: 134782   epsilon: 0.9311296600014951    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 722   score: 3.0   memory length: 135008   epsilon: 0.9306821800015048    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 723   score: 3.0   memory length: 135275   epsilon: 0.9301535200015163    steps: 267    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 724   score: 5.0   memory length: 135619   epsilon: 0.9294724000015311    steps: 344    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 725   score: 3.0   memory length: 135845   epsilon: 0.9290249200015408    steps: 226    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 726   score: 6.0   memory length: 136218   epsilon: 0.9282863800015568    steps: 373    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 727   score: 2.0   memory length: 136399   epsilon: 0.9279280000015646    steps: 181    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 728   score: 1.0   memory length: 136550   epsilon: 0.9276290200015711    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 729   score: 0.0   memory length: 136673   epsilon: 0.9273854800015764    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 730   score: 2.0   memory length: 136871   epsilon: 0.9269934400015849    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 731   score: 1.0   memory length: 137021   epsilon: 0.9266964400015913    steps: 150    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 732   score: 3.0   memory length: 137267   epsilon: 0.9262093600016019    steps: 246    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 733   score: 0.0   memory length: 137390   epsilon: 0.9259658200016072    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 734   score: 2.0   memory length: 137609   epsilon: 0.9255322000016166    steps: 219    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 735   score: 2.0   memory length: 137809   epsilon: 0.9251362000016252    steps: 200    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 736   score: 2.0   memory length: 137991   epsilon: 0.924775840001633    steps: 182    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 737   score: 2.0   memory length: 138189   epsilon: 0.9243838000016416    steps: 198    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 738   score: 1.0   memory length: 138358   epsilon: 0.9240491800016488    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 739   score: 1.0   memory length: 138509   epsilon: 0.9237502000016553    steps: 151    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 740   score: 3.0   memory length: 138754   epsilon: 0.9232651000016658    steps: 245    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 741   score: 1.0   memory length: 138904   epsilon: 0.9229681000016723    steps: 150    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 742   score: 1.0   memory length: 139076   epsilon: 0.9226275400016797    steps: 172    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 743   score: 0.0   memory length: 139198   epsilon: 0.9223859800016849    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 744   score: 5.0   memory length: 139544   epsilon: 0.9217009000016998    steps: 346    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 745   score: 2.0   memory length: 139745   epsilon: 0.9213029200017084    steps: 201    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 746   score: 0.0   memory length: 139868   epsilon: 0.9210593800017137    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 747   score: 1.0   memory length: 140019   epsilon: 0.9207604000017202    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 748   score: 2.0   memory length: 140237   epsilon: 0.9203287600017296    steps: 218    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 749   score: 0.0   memory length: 140360   epsilon: 0.9200852200017349    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 750   score: 4.0   memory length: 140639   epsilon: 0.9195328000017469    steps: 279    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 751   score: 3.0   memory length: 140887   epsilon: 0.9190417600017575    steps: 248    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 752   score: 2.0   memory length: 141085   epsilon: 0.918649720001766    steps: 198    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 753   score: 0.0   memory length: 141208   epsilon: 0.9184061800017713    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 754   score: 3.0   memory length: 141475   epsilon: 0.9178775200017828    steps: 267    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 755   score: 0.0   memory length: 141597   epsilon: 0.917635960001788    steps: 122    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 756   score: 1.0   memory length: 141748   epsilon: 0.9173369800017945    steps: 151    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 757   score: 0.0   memory length: 141871   epsilon: 0.9170934400017998    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 758   score: 1.0   memory length: 142040   epsilon: 0.9167588200018071    steps: 169    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 759   score: 0.0   memory length: 142163   epsilon: 0.9165152800018124    steps: 123    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 760   score: 0.0   memory length: 142285   epsilon: 0.9162737200018176    steps: 122    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 761   score: 1.0   memory length: 142436   epsilon: 0.9159747400018241    steps: 151    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 762   score: 1.0   memory length: 142586   epsilon: 0.9156777400018306    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 763   score: 1.0   memory length: 142755   epsilon: 0.9153431200018378    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 764   score: 2.0   memory length: 142952   epsilon: 0.9149530600018463    steps: 197    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 765   score: 1.0   memory length: 143103   epsilon: 0.9146540800018528    steps: 151    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 766   score: 2.0   memory length: 143301   epsilon: 0.9142620400018613    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 767   score: 1.0   memory length: 143469   epsilon: 0.9139294000018685    steps: 168    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 768   score: 4.0   memory length: 143728   epsilon: 0.9134165800018796    steps: 259    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 769   score: 2.0   memory length: 143908   epsilon: 0.9130601800018874    steps: 180    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 770   score: 3.0   memory length: 144176   epsilon: 0.9125295400018989    steps: 268    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 771   score: 1.0   memory length: 144345   epsilon: 0.9121949200019062    steps: 169    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 772   score: 3.0   memory length: 144574   epsilon: 0.911741500001916    steps: 229    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 773   score: 3.0   memory length: 144824   epsilon: 0.9112465000019268    steps: 250    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 774   score: 2.0   memory length: 145022   epsilon: 0.9108544600019353    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 775   score: 4.0   memory length: 145282   epsilon: 0.9103396600019464    steps: 260    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 776   score: 1.0   memory length: 145452   epsilon: 0.9100030600019537    steps: 170    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 777   score: 3.0   memory length: 145679   epsilon: 0.9095536000019635    steps: 227    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 778   score: 4.0   memory length: 145953   epsilon: 0.9090110800019753    steps: 274    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 779   score: 2.0   memory length: 146153   epsilon: 0.9086150800019839    steps: 200    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 780   score: 0.0   memory length: 146276   epsilon: 0.9083715400019892    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 781   score: 0.0   memory length: 146398   epsilon: 0.9081299800019944    steps: 122    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 782   score: 1.0   memory length: 146549   epsilon: 0.9078310000020009    steps: 151    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 783   score: 1.0   memory length: 146721   epsilon: 0.9074904400020083    steps: 172    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 784   score: 1.0   memory length: 146891   epsilon: 0.9071538400020156    steps: 170    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 785   score: 1.0   memory length: 147042   epsilon: 0.9068548600020221    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 786   score: 1.0   memory length: 147193   epsilon: 0.9065558800020286    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 787   score: 2.0   memory length: 147391   epsilon: 0.9061638400020371    steps: 198    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 788   score: 4.0   memory length: 147705   epsilon: 0.9055421200020506    steps: 314    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 789   score: 1.0   memory length: 147873   epsilon: 0.9052094800020578    steps: 168    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 790   score: 3.0   memory length: 148099   epsilon: 0.9047620000020675    steps: 226    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 791   score: 1.0   memory length: 148267   epsilon: 0.9044293600020747    steps: 168    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 792   score: 7.0   memory length: 148661   epsilon: 0.9036492400020917    steps: 394    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 793   score: 2.0   memory length: 148879   epsilon: 0.903217600002101    steps: 218    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 794   score: 0.0   memory length: 149002   epsilon: 0.9029740600021063    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 795   score: 3.0   memory length: 149229   epsilon: 0.9025246000021161    steps: 227    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 796   score: 0.0   memory length: 149352   epsilon: 0.9022810600021214    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 797   score: 1.0   memory length: 149522   epsilon: 0.9019444600021287    steps: 170    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 798   score: 4.0   memory length: 149816   epsilon: 0.9013623400021413    steps: 294    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 799   score: 3.0   memory length: 150064   epsilon: 0.900871300002152    steps: 248    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 800   score: 3.0   memory length: 150310   epsilon: 0.9003842200021626    steps: 246    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 801   score: 3.0   memory length: 150556   epsilon: 0.8998971400021731    steps: 246    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 802   score: 2.0   memory length: 150774   epsilon: 0.8994655000021825    steps: 218    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 803   score: 2.0   memory length: 150974   epsilon: 0.8990695000021911    steps: 200    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 804   score: 2.0   memory length: 151174   epsilon: 0.8986735000021997    steps: 200    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 805   score: 1.0   memory length: 151325   epsilon: 0.8983745200022062    steps: 151    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 806   score: 0.0   memory length: 151448   epsilon: 0.8981309800022115    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 807   score: 3.0   memory length: 151695   epsilon: 0.8976419200022221    steps: 247    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 808   score: 5.0   memory length: 152003   epsilon: 0.8970320800022353    steps: 308    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 809   score: 3.0   memory length: 152232   epsilon: 0.8965786600022452    steps: 229    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 810   score: 1.0   memory length: 152403   epsilon: 0.8962400800022525    steps: 171    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 811   score: 4.0   memory length: 152698   epsilon: 0.8956559800022652    steps: 295    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 812   score: 4.0   memory length: 152993   epsilon: 0.8950718800022779    steps: 295    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 813   score: 4.0   memory length: 153311   epsilon: 0.8944422400022916    steps: 318    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 814   score: 0.0   memory length: 153434   epsilon: 0.8941987000022968    steps: 123    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 815   score: 4.0   memory length: 153713   epsilon: 0.8936462800023088    steps: 279    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 816   score: 2.0   memory length: 153910   epsilon: 0.8932562200023173    steps: 197    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 817   score: 4.0   memory length: 154227   epsilon: 0.8926285600023309    steps: 317    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 818   score: 0.0   memory length: 154349   epsilon: 0.8923870000023362    steps: 122    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 819   score: 2.0   memory length: 154547   epsilon: 0.8919949600023447    steps: 198    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 820   score: 8.0   memory length: 154877   epsilon: 0.8913415600023589    steps: 330    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 821   score: 3.0   memory length: 155103   epsilon: 0.8908940800023686    steps: 226    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 822   score: 2.0   memory length: 155300   epsilon: 0.890504020002377    steps: 197    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 823   score: 6.0   memory length: 155642   epsilon: 0.8898268600023918    steps: 342    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 824   score: 1.0   memory length: 155810   epsilon: 0.889494220002399    steps: 168    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 825   score: 1.0   memory length: 155979   epsilon: 0.8891596000024062    steps: 169    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 826   score: 1.0   memory length: 156131   epsilon: 0.8888586400024128    steps: 152    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 827   score: 3.0   memory length: 156395   epsilon: 0.8883359200024241    steps: 264    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 828   score: 1.0   memory length: 156546   epsilon: 0.8880369400024306    steps: 151    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 829   score: 0.0   memory length: 156669   epsilon: 0.8877934000024359    steps: 123    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 830   score: 5.0   memory length: 156994   epsilon: 0.8871499000024499    steps: 325    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 831   score: 3.0   memory length: 157238   epsilon: 0.8866667800024604    steps: 244    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 832   score: 0.0   memory length: 157361   epsilon: 0.8864232400024656    steps: 123    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 833   score: 0.0   memory length: 157483   epsilon: 0.8861816800024709    steps: 122    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 834   score: 2.0   memory length: 157663   epsilon: 0.8858252800024786    steps: 180    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 835   score: 4.0   memory length: 157938   epsilon: 0.8852807800024904    steps: 275    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 836   score: 1.0   memory length: 158088   epsilon: 0.8849837800024969    steps: 150    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 837   score: 3.0   memory length: 158359   epsilon: 0.8844472000025085    steps: 271    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 838   score: 3.0   memory length: 158607   epsilon: 0.8839561600025192    steps: 248    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 839   score: 3.0   memory length: 158833   epsilon: 0.8835086800025289    steps: 226    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 840   score: 3.0   memory length: 159098   epsilon: 0.8829839800025403    steps: 265    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 841   score: 2.0   memory length: 159278   epsilon: 0.882627580002548    steps: 180    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 842   score: 8.0   memory length: 159624   epsilon: 0.8819425000025629    steps: 346    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 843   score: 1.0   memory length: 159794   epsilon: 0.8816059000025702    steps: 170    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 844   score: 2.0   memory length: 159991   epsilon: 0.8812158400025787    steps: 197    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 845   score: 1.0   memory length: 160141   epsilon: 0.8809188400025851    steps: 150    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 846   score: 3.0   memory length: 160387   epsilon: 0.8804317600025957    steps: 246    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 847   score: 2.0   memory length: 160585   epsilon: 0.8800397200026042    steps: 198    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 848   score: 2.0   memory length: 160782   epsilon: 0.8796496600026127    steps: 197    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 849   score: 2.0   memory length: 160981   epsilon: 0.8792556400026212    steps: 199    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 850   score: 4.0   memory length: 161243   epsilon: 0.8787368800026325    steps: 262    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 851   score: 4.0   memory length: 161533   epsilon: 0.878162680002645    steps: 290    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 852   score: 3.0   memory length: 161799   epsilon: 0.8776360000026564    steps: 266    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 853   score: 3.0   memory length: 162025   epsilon: 0.8771885200026661    steps: 226    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 854   score: 4.0   memory length: 162301   epsilon: 0.876642040002678    steps: 276    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 855   score: 4.0   memory length: 162598   epsilon: 0.8760539800026907    steps: 297    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 856   score: 2.0   memory length: 162795   epsilon: 0.8756639200026992    steps: 197    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 857   score: 3.0   memory length: 163041   epsilon: 0.8751768400027098    steps: 246    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 858   score: 1.0   memory length: 163212   epsilon: 0.8748382600027171    steps: 171    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 859   score: 3.0   memory length: 163459   epsilon: 0.8743492000027278    steps: 247    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 860   score: 4.0   memory length: 163757   epsilon: 0.8737591600027406    steps: 298    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 861   score: 2.0   memory length: 163974   epsilon: 0.8733295000027499    steps: 217    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 862   score: 2.0   memory length: 164154   epsilon: 0.8729731000027576    steps: 180    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 863   score: 2.0   memory length: 164356   epsilon: 0.8725731400027663    steps: 202    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 864   score: 5.0   memory length: 164673   epsilon: 0.8719454800027799    steps: 317    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 865   score: 4.0   memory length: 164947   epsilon: 0.8714029600027917    steps: 274    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 866   score: 1.0   memory length: 165117   epsilon: 0.871066360002799    steps: 170    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 867   score: 2.0   memory length: 165335   epsilon: 0.8706347200028084    steps: 218    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 868   score: 2.0   memory length: 165532   epsilon: 0.8702446600028169    steps: 197    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 869   score: 6.0   memory length: 165869   epsilon: 0.8695774000028313    steps: 337    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 870   score: 5.0   memory length: 166194   epsilon: 0.8689339000028453    steps: 325    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 871   score: 5.0   memory length: 166520   epsilon: 0.8682884200028593    steps: 326    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 872   score: 3.0   memory length: 166786   epsilon: 0.8677617400028708    steps: 266    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 873   score: 5.0   memory length: 167113   epsilon: 0.8671142800028848    steps: 327    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 874   score: 11.0   memory length: 167545   epsilon: 0.8662589200029034    steps: 432    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 875   score: 3.0   memory length: 167790   epsilon: 0.8657738200029139    steps: 245    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 876   score: 3.0   memory length: 168017   epsilon: 0.8653243600029237    steps: 227    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 877   score: 2.0   memory length: 168218   epsilon: 0.8649263800029323    steps: 201    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 878   score: 3.0   memory length: 168465   epsilon: 0.8644373200029429    steps: 247    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 879   score: 1.0   memory length: 168616   epsilon: 0.8641383400029494    steps: 151    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 880   score: 6.0   memory length: 168959   epsilon: 0.8634592000029642    steps: 343    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 881   score: 1.0   memory length: 169110   epsilon: 0.8631602200029707    steps: 151    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 882   score: 2.0   memory length: 169308   epsilon: 0.8627681800029792    steps: 198    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 883   score: 0.0   memory length: 169431   epsilon: 0.8625246400029845    steps: 123    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 884   score: 3.0   memory length: 169676   epsilon: 0.862039540002995    steps: 245    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 885   score: 5.0   memory length: 169972   epsilon: 0.8614534600030077    steps: 296    lr: 0.0001     evaluation reward: 2.76\n",
      "episode: 886   score: 5.0   memory length: 170287   epsilon: 0.8608297600030212    steps: 315    lr: 0.0001     evaluation reward: 2.8\n",
      "episode: 887   score: 3.0   memory length: 170534   epsilon: 0.8603407000030319    steps: 247    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 888   score: 1.0   memory length: 170685   epsilon: 0.8600417200030384    steps: 151    lr: 0.0001     evaluation reward: 2.78\n",
      "episode: 889   score: 6.0   memory length: 171058   epsilon: 0.8593031800030544    steps: 373    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 890   score: 3.0   memory length: 171286   epsilon: 0.8588517400030642    steps: 228    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 891   score: 2.0   memory length: 171484   epsilon: 0.8584597000030727    steps: 198    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 892   score: 1.0   memory length: 171635   epsilon: 0.8581607200030792    steps: 151    lr: 0.0001     evaluation reward: 2.78\n",
      "episode: 893   score: 2.0   memory length: 171833   epsilon: 0.8577686800030877    steps: 198    lr: 0.0001     evaluation reward: 2.78\n",
      "episode: 894   score: 2.0   memory length: 172051   epsilon: 0.8573370400030971    steps: 218    lr: 0.0001     evaluation reward: 2.8\n",
      "episode: 895   score: 5.0   memory length: 172397   epsilon: 0.856651960003112    steps: 346    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 896   score: 3.0   memory length: 172663   epsilon: 0.8561252800031234    steps: 266    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 897   score: 4.0   memory length: 172961   epsilon: 0.8555352400031362    steps: 298    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 898   score: 2.0   memory length: 173162   epsilon: 0.8551372600031448    steps: 201    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 899   score: 4.0   memory length: 173420   epsilon: 0.8546264200031559    steps: 258    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 900   score: 0.0   memory length: 173542   epsilon: 0.8543848600031612    steps: 122    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 901   score: 3.0   memory length: 173768   epsilon: 0.8539373800031709    steps: 226    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 902   score: 2.0   memory length: 173990   epsilon: 0.8534978200031804    steps: 222    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 903   score: 2.0   memory length: 174170   epsilon: 0.8531414200031882    steps: 180    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 904   score: 2.0   memory length: 174350   epsilon: 0.8527850200031959    steps: 180    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 905   score: 3.0   memory length: 174579   epsilon: 0.8523316000032057    steps: 229    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 906   score: 3.0   memory length: 174847   epsilon: 0.8518009600032173    steps: 268    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 907   score: 5.0   memory length: 175156   epsilon: 0.8511891400032305    steps: 309    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 908   score: 5.0   memory length: 175478   epsilon: 0.8505515800032444    steps: 322    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 909   score: 4.0   memory length: 175755   epsilon: 0.8500031200032563    steps: 277    lr: 0.0001     evaluation reward: 2.92\n",
      "episode: 910   score: 4.0   memory length: 176031   epsilon: 0.8494566400032681    steps: 276    lr: 0.0001     evaluation reward: 2.95\n",
      "episode: 911   score: 3.0   memory length: 176241   epsilon: 0.8490408400032772    steps: 210    lr: 0.0001     evaluation reward: 2.94\n",
      "episode: 912   score: 3.0   memory length: 176492   epsilon: 0.848543860003288    steps: 251    lr: 0.0001     evaluation reward: 2.93\n",
      "episode: 913   score: 2.0   memory length: 176690   epsilon: 0.8481518200032965    steps: 198    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 914   score: 0.0   memory length: 176813   epsilon: 0.8479082800033018    steps: 123    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 915   score: 4.0   memory length: 177086   epsilon: 0.8473677400033135    steps: 273    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 916   score: 4.0   memory length: 177365   epsilon: 0.8468153200033255    steps: 279    lr: 0.0001     evaluation reward: 2.93\n",
      "episode: 917   score: 4.0   memory length: 177661   epsilon: 0.8462292400033382    steps: 296    lr: 0.0001     evaluation reward: 2.93\n",
      "episode: 918   score: 3.0   memory length: 177927   epsilon: 0.8457025600033496    steps: 266    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 919   score: 3.0   memory length: 178172   epsilon: 0.8452174600033602    steps: 245    lr: 0.0001     evaluation reward: 2.97\n",
      "episode: 920   score: 2.0   memory length: 178370   epsilon: 0.8448254200033687    steps: 198    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 921   score: 0.0   memory length: 178493   epsilon: 0.844581880003374    steps: 123    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 922   score: 3.0   memory length: 178737   epsilon: 0.8440987600033845    steps: 244    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 923   score: 2.0   memory length: 178935   epsilon: 0.843706720003393    steps: 198    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 924   score: 1.0   memory length: 179104   epsilon: 0.8433721000034002    steps: 169    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 925   score: 4.0   memory length: 179400   epsilon: 0.842786020003413    steps: 296    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 926   score: 3.0   memory length: 179643   epsilon: 0.8423048800034234    steps: 243    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 927   score: 1.0   memory length: 179794   epsilon: 0.8420059000034299    steps: 151    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 928   score: 3.0   memory length: 180019   epsilon: 0.8415604000034396    steps: 225    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 929   score: 1.0   memory length: 180188   epsilon: 0.8412257800034468    steps: 169    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 930   score: 1.0   memory length: 180359   epsilon: 0.8408872000034542    steps: 171    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 931   score: 0.0   memory length: 180482   epsilon: 0.8406436600034595    steps: 123    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 932   score: 0.0   memory length: 180604   epsilon: 0.8404021000034647    steps: 122    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 933   score: 6.0   memory length: 180978   epsilon: 0.8396615800034808    steps: 374    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 934   score: 2.0   memory length: 181179   epsilon: 0.8392636000034894    steps: 201    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 935   score: 2.0   memory length: 181377   epsilon: 0.8388715600034979    steps: 198    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 936   score: 3.0   memory length: 181605   epsilon: 0.8384201200035077    steps: 228    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 937   score: 3.0   memory length: 181832   epsilon: 0.8379706600035175    steps: 227    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 938   score: 4.0   memory length: 182107   epsilon: 0.8374261600035293    steps: 275    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 939   score: 2.0   memory length: 182324   epsilon: 0.8369965000035386    steps: 217    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 940   score: 2.0   memory length: 182522   epsilon: 0.8366044600035472    steps: 198    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 941   score: 3.0   memory length: 182769   epsilon: 0.8361154000035578    steps: 247    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 942   score: 3.0   memory length: 183019   epsilon: 0.8356204000035685    steps: 250    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 943   score: 2.0   memory length: 183236   epsilon: 0.8351907400035778    steps: 217    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 944   score: 2.0   memory length: 183436   epsilon: 0.8347947400035864    steps: 200    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 945   score: 2.0   memory length: 183617   epsilon: 0.8344363600035942    steps: 181    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 946   score: 4.0   memory length: 183899   epsilon: 0.8338780000036063    steps: 282    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 947   score: 4.0   memory length: 184175   epsilon: 0.8333315200036182    steps: 276    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 948   score: 3.0   memory length: 184420   epsilon: 0.8328464200036287    steps: 245    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 949   score: 2.0   memory length: 184619   epsilon: 0.8324524000036373    steps: 199    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 950   score: 2.0   memory length: 184818   epsilon: 0.8320583800036458    steps: 199    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 951   score: 5.0   memory length: 185141   epsilon: 0.8314188400036597    steps: 323    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 952   score: 0.0   memory length: 185264   epsilon: 0.831175300003665    steps: 123    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 953   score: 0.0   memory length: 185387   epsilon: 0.8309317600036703    steps: 123    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 954   score: 4.0   memory length: 185683   epsilon: 0.830345680003683    steps: 296    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 955   score: 5.0   memory length: 185990   epsilon: 0.8297378200036962    steps: 307    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 956   score: 5.0   memory length: 186313   epsilon: 0.8290982800037101    steps: 323    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 957   score: 0.0   memory length: 186435   epsilon: 0.8288567200037154    steps: 122    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 958   score: 2.0   memory length: 186652   epsilon: 0.8284270600037247    steps: 217    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 959   score: 4.0   memory length: 186927   epsilon: 0.8278825600037365    steps: 275    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 960   score: 0.0   memory length: 187050   epsilon: 0.8276390200037418    steps: 123    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 961   score: 0.0   memory length: 187172   epsilon: 0.827397460003747    steps: 122    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 962   score: 3.0   memory length: 187437   epsilon: 0.8268727600037584    steps: 265    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 963   score: 5.0   memory length: 187748   epsilon: 0.8262569800037718    steps: 311    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 964   score: 3.0   memory length: 187974   epsilon: 0.8258095000037815    steps: 226    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 965   score: 0.0   memory length: 188097   epsilon: 0.8255659600037868    steps: 123    lr: 0.0001     evaluation reward: 2.79\n",
      "episode: 966   score: 1.0   memory length: 188248   epsilon: 0.8252669800037933    steps: 151    lr: 0.0001     evaluation reward: 2.79\n",
      "episode: 967   score: 4.0   memory length: 188549   epsilon: 0.8246710000038062    steps: 301    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 968   score: 1.0   memory length: 188718   epsilon: 0.8243363800038135    steps: 169    lr: 0.0001     evaluation reward: 2.8\n",
      "episode: 969   score: 3.0   memory length: 188946   epsilon: 0.8238849400038233    steps: 228    lr: 0.0001     evaluation reward: 2.77\n",
      "episode: 970   score: 2.0   memory length: 189163   epsilon: 0.8234552800038326    steps: 217    lr: 0.0001     evaluation reward: 2.74\n",
      "episode: 971   score: 3.0   memory length: 189390   epsilon: 0.8230058200038424    steps: 227    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 972   score: 4.0   memory length: 189648   epsilon: 0.8224949800038535    steps: 258    lr: 0.0001     evaluation reward: 2.73\n",
      "episode: 973   score: 4.0   memory length: 189945   epsilon: 0.8219069200038662    steps: 297    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 974   score: 2.0   memory length: 190163   epsilon: 0.8214752800038756    steps: 218    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 975   score: 3.0   memory length: 190409   epsilon: 0.8209882000038862    steps: 246    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 976   score: 1.0   memory length: 190560   epsilon: 0.8206892200038927    steps: 151    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 977   score: 0.0   memory length: 190682   epsilon: 0.8204476600038979    steps: 122    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 978   score: 1.0   memory length: 190852   epsilon: 0.8201110600039052    steps: 170    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 979   score: 2.0   memory length: 191050   epsilon: 0.8197190200039137    steps: 198    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 980   score: 3.0   memory length: 191296   epsilon: 0.8192319400039243    steps: 246    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 981   score: 0.0   memory length: 191418   epsilon: 0.8189903800039295    steps: 122    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 982   score: 2.0   memory length: 191602   epsilon: 0.8186260600039375    steps: 184    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 983   score: 3.0   memory length: 191849   epsilon: 0.8181370000039481    steps: 247    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 984   score: 1.0   memory length: 192018   epsilon: 0.8178023800039553    steps: 169    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 985   score: 0.0   memory length: 192141   epsilon: 0.8175588400039606    steps: 123    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 986   score: 4.0   memory length: 192397   epsilon: 0.8170519600039716    steps: 256    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 987   score: 2.0   memory length: 192597   epsilon: 0.8166559600039802    steps: 200    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 988   score: 1.0   memory length: 192748   epsilon: 0.8163569800039867    steps: 151    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 989   score: 2.0   memory length: 192945   epsilon: 0.8159669200039952    steps: 197    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 990   score: 0.0   memory length: 193068   epsilon: 0.8157233800040005    steps: 123    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 991   score: 6.0   memory length: 193320   epsilon: 0.8152244200040113    steps: 252    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 992   score: 2.0   memory length: 193520   epsilon: 0.8148284200040199    steps: 200    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 993   score: 2.0   memory length: 193717   epsilon: 0.8144383600040284    steps: 197    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 994   score: 3.0   memory length: 193967   epsilon: 0.8139433600040391    steps: 250    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 995   score: 3.0   memory length: 194214   epsilon: 0.8134543000040497    steps: 247    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 996   score: 4.0   memory length: 194511   epsilon: 0.8128662400040625    steps: 297    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 997   score: 3.0   memory length: 194758   epsilon: 0.8123771800040731    steps: 247    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 998   score: 5.0   memory length: 195085   epsilon: 0.8117297200040872    steps: 327    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 999   score: 2.0   memory length: 195283   epsilon: 0.8113376800040957    steps: 198    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 1000   score: 0.0   memory length: 195406   epsilon: 0.811094140004101    steps: 123    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 1001   score: 0.0   memory length: 195529   epsilon: 0.8108506000041062    steps: 123    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 1002   score: 3.0   memory length: 195739   epsilon: 0.8104348000041153    steps: 210    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1003   score: 3.0   memory length: 195987   epsilon: 0.8099437600041259    steps: 248    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1004   score: 4.0   memory length: 196245   epsilon: 0.809432920004137    steps: 258    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 1005   score: 3.0   memory length: 196472   epsilon: 0.8089834600041468    steps: 227    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 1006   score: 2.0   memory length: 196670   epsilon: 0.8085914200041553    steps: 198    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 1007   score: 2.0   memory length: 196886   epsilon: 0.8081637400041646    steps: 216    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 1008   score: 2.0   memory length: 197084   epsilon: 0.8077717000041731    steps: 198    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1009   score: 2.0   memory length: 197264   epsilon: 0.8074153000041808    steps: 180    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1010   score: 1.0   memory length: 197433   epsilon: 0.8070806800041881    steps: 169    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1011   score: 1.0   memory length: 197602   epsilon: 0.8067460600041954    steps: 169    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1012   score: 3.0   memory length: 197848   epsilon: 0.8062589800042059    steps: 246    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1013   score: 2.0   memory length: 198048   epsilon: 0.8058629800042145    steps: 200    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1014   score: 1.0   memory length: 198199   epsilon: 0.805564000004221    steps: 151    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1015   score: 6.0   memory length: 198574   epsilon: 0.8048215000042371    steps: 375    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1016   score: 3.0   memory length: 198791   epsilon: 0.8043918400042465    steps: 217    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1017   score: 5.0   memory length: 199107   epsilon: 0.80376616000426    steps: 316    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1018   score: 3.0   memory length: 199353   epsilon: 0.8032790800042706    steps: 246    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1019   score: 2.0   memory length: 199569   epsilon: 0.8028514000042799    steps: 216    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1020   score: 3.0   memory length: 199776   epsilon: 0.8024415400042888    steps: 207    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1021   score: 3.0   memory length: 200043   epsilon: 0.8019128800043003    steps: 267    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1022   score: 8.0   memory length: 200451   epsilon: 0.8011050400043178    steps: 408    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1023   score: 1.0   memory length: 200620   epsilon: 0.8007704200043251    steps: 169    lr: 4e-05     evaluation reward: 2.43\n",
      "episode: 1024   score: 3.0   memory length: 200845   epsilon: 0.8003249200043347    steps: 225    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1025   score: 0.0   memory length: 200968   epsilon: 0.80008138000434    steps: 123    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1026   score: 9.0   memory length: 201417   epsilon: 0.7991923600043593    steps: 449    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1027   score: 6.0   memory length: 201754   epsilon: 0.7985251000043738    steps: 337    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1028   score: 0.0   memory length: 201877   epsilon: 0.7982815600043791    steps: 123    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1029   score: 3.0   memory length: 202103   epsilon: 0.7978340800043888    steps: 226    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1030   score: 1.0   memory length: 202273   epsilon: 0.7974974800043961    steps: 170    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1031   score: 4.0   memory length: 202552   epsilon: 0.7969450600044081    steps: 279    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1032   score: 1.0   memory length: 202702   epsilon: 0.7966480600044146    steps: 150    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1033   score: 6.0   memory length: 203096   epsilon: 0.7958679400044315    steps: 394    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1034   score: 2.0   memory length: 203296   epsilon: 0.7954719400044401    steps: 200    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1035   score: 3.0   memory length: 203562   epsilon: 0.7949452600044515    steps: 266    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1036   score: 5.0   memory length: 203885   epsilon: 0.7943057200044654    steps: 323    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1037   score: 2.0   memory length: 204085   epsilon: 0.793909720004474    steps: 200    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1038   score: 1.0   memory length: 204257   epsilon: 0.7935691600044814    steps: 172    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1039   score: 6.0   memory length: 204646   epsilon: 0.7927989400044981    steps: 389    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1040   score: 8.0   memory length: 205065   epsilon: 0.7919693200045161    steps: 419    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1041   score: 4.0   memory length: 205360   epsilon: 0.7913852200045288    steps: 295    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1042   score: 3.0   memory length: 205590   epsilon: 0.7909298200045387    steps: 230    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1043   score: 3.0   memory length: 205834   epsilon: 0.7904467000045492    steps: 244    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1044   score: 2.0   memory length: 206031   epsilon: 0.7900566400045577    steps: 197    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1045   score: 4.0   memory length: 206307   epsilon: 0.7895101600045695    steps: 276    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1046   score: 5.0   memory length: 206618   epsilon: 0.7888943800045829    steps: 311    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1047   score: 1.0   memory length: 206788   epsilon: 0.7885577800045902    steps: 170    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1048   score: 2.0   memory length: 206968   epsilon: 0.7882013800045979    steps: 180    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1049   score: 5.0   memory length: 207272   epsilon: 0.787599460004611    steps: 304    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1050   score: 3.0   memory length: 207501   epsilon: 0.7871460400046209    steps: 229    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1051   score: 2.0   memory length: 207701   epsilon: 0.7867500400046294    steps: 200    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1052   score: 3.0   memory length: 207911   epsilon: 0.7863342400046385    steps: 210    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1053   score: 3.0   memory length: 208137   epsilon: 0.7858867600046482    steps: 226    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1054   score: 1.0   memory length: 208288   epsilon: 0.7855877800046547    steps: 151    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1055   score: 2.0   memory length: 208487   epsilon: 0.7851937600046632    steps: 199    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1056   score: 6.0   memory length: 208836   epsilon: 0.7845027400046782    steps: 349    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1057   score: 5.0   memory length: 209155   epsilon: 0.783871120004692    steps: 319    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1058   score: 3.0   memory length: 209423   epsilon: 0.7833404800047035    steps: 268    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1059   score: 3.0   memory length: 209653   epsilon: 0.7828850800047134    steps: 230    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1060   score: 0.0   memory length: 209775   epsilon: 0.7826435200047186    steps: 122    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1061   score: 6.0   memory length: 210169   epsilon: 0.7818634000047355    steps: 394    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1062   score: 3.0   memory length: 210413   epsilon: 0.781380280004746    steps: 244    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1063   score: 5.0   memory length: 210739   epsilon: 0.78073480000476    steps: 326    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1064   score: 6.0   memory length: 211132   epsilon: 0.7799566600047769    steps: 393    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1065   score: 3.0   memory length: 211377   epsilon: 0.7794715600047875    steps: 245    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1066   score: 3.0   memory length: 211602   epsilon: 0.7790260600047971    steps: 225    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1067   score: 4.0   memory length: 211881   epsilon: 0.7784736400048091    steps: 279    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1068   score: 3.0   memory length: 212144   epsilon: 0.7779529000048204    steps: 263    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1069   score: 2.0   memory length: 212342   epsilon: 0.7775608600048289    steps: 198    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1070   score: 5.0   memory length: 212671   epsilon: 0.7769094400048431    steps: 329    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1071   score: 2.0   memory length: 212850   epsilon: 0.7765550200048508    steps: 179    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1072   score: 5.0   memory length: 213138   epsilon: 0.7759847800048632    steps: 288    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1073   score: 4.0   memory length: 213416   epsilon: 0.7754343400048751    steps: 278    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1074   score: 5.0   memory length: 213698   epsilon: 0.7748759800048872    steps: 282    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1075   score: 3.0   memory length: 213927   epsilon: 0.7744225600048971    steps: 229    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1076   score: 2.0   memory length: 214127   epsilon: 0.7740265600049057    steps: 200    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1077   score: 4.0   memory length: 214420   epsilon: 0.7734464200049183    steps: 293    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1078   score: 0.0   memory length: 214543   epsilon: 0.7732028800049235    steps: 123    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1079   score: 3.0   memory length: 214753   epsilon: 0.7727870800049326    steps: 210    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1080   score: 4.0   memory length: 215028   epsilon: 0.7722425800049444    steps: 275    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1081   score: 2.0   memory length: 215226   epsilon: 0.7718505400049529    steps: 198    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1082   score: 7.0   memory length: 215649   epsilon: 0.7710130000049711    steps: 423    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1083   score: 4.0   memory length: 215890   epsilon: 0.7705358200049814    steps: 241    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1084   score: 0.0   memory length: 216013   epsilon: 0.7702922800049867    steps: 123    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1085   score: 1.0   memory length: 216184   epsilon: 0.7699537000049941    steps: 171    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1086   score: 5.0   memory length: 216522   epsilon: 0.7692844600050086    steps: 338    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1087   score: 2.0   memory length: 216720   epsilon: 0.7688924200050171    steps: 198    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1088   score: 1.0   memory length: 216871   epsilon: 0.7685934400050236    steps: 151    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1089   score: 8.0   memory length: 217311   epsilon: 0.7677222400050425    steps: 440    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1090   score: 4.0   memory length: 217588   epsilon: 0.7671737800050544    steps: 277    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1091   score: 2.0   memory length: 217805   epsilon: 0.7667441200050638    steps: 217    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1092   score: 4.0   memory length: 218097   epsilon: 0.7661659600050763    steps: 292    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1093   score: 7.0   memory length: 218520   epsilon: 0.7653284200050945    steps: 423    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1094   score: 2.0   memory length: 218718   epsilon: 0.764936380005103    steps: 198    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1095   score: 6.0   memory length: 219102   epsilon: 0.7641760600051195    steps: 384    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1096   score: 0.0   memory length: 219224   epsilon: 0.7639345000051248    steps: 122    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1097   score: 4.0   memory length: 219523   epsilon: 0.7633424800051376    steps: 299    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1098   score: 4.0   memory length: 219803   epsilon: 0.7627880800051496    steps: 280    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1099   score: 4.0   memory length: 220116   epsilon: 0.7621683400051631    steps: 313    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1100   score: 3.0   memory length: 220342   epsilon: 0.7617208600051728    steps: 226    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1101   score: 3.0   memory length: 220590   epsilon: 0.7612298200051835    steps: 248    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1102   score: 1.0   memory length: 220759   epsilon: 0.7608952000051907    steps: 169    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1103   score: 6.0   memory length: 221139   epsilon: 0.7601428000052071    steps: 380    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1104   score: 2.0   memory length: 221356   epsilon: 0.7597131400052164    steps: 217    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1105   score: 3.0   memory length: 221603   epsilon: 0.759224080005227    steps: 247    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1106   score: 3.0   memory length: 221834   epsilon: 0.7587667000052369    steps: 231    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1107   score: 4.0   memory length: 222130   epsilon: 0.7581806200052497    steps: 296    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1108   score: 1.0   memory length: 222301   epsilon: 0.757842040005257    steps: 171    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1109   score: 1.0   memory length: 222470   epsilon: 0.7575074200052643    steps: 169    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1110   score: 1.0   memory length: 222639   epsilon: 0.7571728000052715    steps: 169    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1111   score: 6.0   memory length: 222992   epsilon: 0.7564738600052867    steps: 353    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1112   score: 0.0   memory length: 223114   epsilon: 0.756232300005292    steps: 122    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1113   score: 2.0   memory length: 223313   epsilon: 0.7558382800053005    steps: 199    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1114   score: 5.0   memory length: 223654   epsilon: 0.7551631000053152    steps: 341    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1115   score: 6.0   memory length: 224011   epsilon: 0.7544562400053305    steps: 357    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1116   score: 2.0   memory length: 224192   epsilon: 0.7540978600053383    steps: 181    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1117   score: 3.0   memory length: 224456   epsilon: 0.7535751400053496    steps: 264    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1118   score: 3.0   memory length: 224702   epsilon: 0.7530880600053602    steps: 246    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1119   score: 3.0   memory length: 224949   epsilon: 0.7525990000053708    steps: 247    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1120   score: 1.0   memory length: 225099   epsilon: 0.7523020000053773    steps: 150    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1121   score: 5.0   memory length: 225440   epsilon: 0.7516268200053919    steps: 341    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1122   score: 4.0   memory length: 225714   epsilon: 0.7510843000054037    steps: 274    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1123   score: 3.0   memory length: 225941   epsilon: 0.7506348400054135    steps: 227    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1124   score: 8.0   memory length: 226386   epsilon: 0.7497537400054326    steps: 445    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1125   score: 6.0   memory length: 226699   epsilon: 0.749134000005446    steps: 313    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1126   score: 5.0   memory length: 227004   epsilon: 0.7485301000054592    steps: 305    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1127   score: 2.0   memory length: 227221   epsilon: 0.7481004400054685    steps: 217    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1128   score: 2.0   memory length: 227440   epsilon: 0.7476668200054779    steps: 219    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1129   score: 1.0   memory length: 227608   epsilon: 0.7473341800054851    steps: 168    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1130   score: 9.0   memory length: 228065   epsilon: 0.7464293200055048    steps: 457    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1131   score: 2.0   memory length: 228264   epsilon: 0.7460353000055133    steps: 199    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1132   score: 3.0   memory length: 228490   epsilon: 0.745587820005523    steps: 226    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1133   score: 3.0   memory length: 228718   epsilon: 0.7451363800055328    steps: 228    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1134   score: 2.0   memory length: 228935   epsilon: 0.7447067200055422    steps: 217    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1135   score: 4.0   memory length: 229252   epsilon: 0.7440790600055558    steps: 317    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1136   score: 5.0   memory length: 229560   epsilon: 0.743469220005569    steps: 308    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1137   score: 2.0   memory length: 229781   epsilon: 0.7430316400055785    steps: 221    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1138   score: 2.0   memory length: 230000   epsilon: 0.7425980200055879    steps: 219    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1139   score: 7.0   memory length: 230385   epsilon: 0.7418357200056045    steps: 385    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1140   score: 4.0   memory length: 230682   epsilon: 0.7412476600056173    steps: 297    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1141   score: 5.0   memory length: 230988   epsilon: 0.7406417800056304    steps: 306    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1142   score: 7.0   memory length: 231398   epsilon: 0.739829980005648    steps: 410    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1143   score: 7.0   memory length: 231825   epsilon: 0.7389845200056664    steps: 427    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1144   score: 2.0   memory length: 232025   epsilon: 0.738588520005675    steps: 200    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1145   score: 1.0   memory length: 232197   epsilon: 0.7382479600056824    steps: 172    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1146   score: 2.0   memory length: 232399   epsilon: 0.7378480000056911    steps: 202    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1147   score: 5.0   memory length: 232725   epsilon: 0.7372025200057051    steps: 326    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1148   score: 0.0   memory length: 232848   epsilon: 0.7369589800057104    steps: 123    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1149   score: 1.0   memory length: 232998   epsilon: 0.7366619800057168    steps: 150    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1150   score: 2.0   memory length: 233178   epsilon: 0.7363055800057245    steps: 180    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1151   score: 6.0   memory length: 233552   epsilon: 0.7355650600057406    steps: 374    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1152   score: 8.0   memory length: 233990   epsilon: 0.7346978200057594    steps: 438    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1153   score: 4.0   memory length: 234307   epsilon: 0.7340701600057731    steps: 317    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1154   score: 5.0   memory length: 234617   epsilon: 0.7334563600057864    steps: 310    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1155   score: 7.0   memory length: 234980   epsilon: 0.732737620005802    steps: 363    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1156   score: 2.0   memory length: 235178   epsilon: 0.7323455800058105    steps: 198    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1157   score: 1.0   memory length: 235328   epsilon: 0.732048580005817    steps: 150    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1158   score: 4.0   memory length: 235589   epsilon: 0.7315318000058282    steps: 261    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1159   score: 5.0   memory length: 235900   epsilon: 0.7309160200058415    steps: 311    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1160   score: 0.0   memory length: 236022   epsilon: 0.7306744600058468    steps: 122    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1161   score: 7.0   memory length: 236411   epsilon: 0.7299042400058635    steps: 389    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1162   score: 4.0   memory length: 236707   epsilon: 0.7293181600058762    steps: 296    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1163   score: 8.0   memory length: 237034   epsilon: 0.7286707000058903    steps: 327    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1164   score: 6.0   memory length: 237363   epsilon: 0.7280192800059044    steps: 329    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1165   score: 6.0   memory length: 237691   epsilon: 0.7273698400059185    steps: 328    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1166   score: 0.0   memory length: 237814   epsilon: 0.7271263000059238    steps: 123    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1167   score: 3.0   memory length: 238040   epsilon: 0.7266788200059335    steps: 226    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1168   score: 1.0   memory length: 238209   epsilon: 0.7263442000059408    steps: 169    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1169   score: 2.0   memory length: 238390   epsilon: 0.7259858200059486    steps: 181    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1170   score: 2.0   memory length: 238588   epsilon: 0.7255937800059571    steps: 198    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1171   score: 8.0   memory length: 238998   epsilon: 0.7247819800059747    steps: 410    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1172   score: 4.0   memory length: 239273   epsilon: 0.7242374800059865    steps: 275    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1173   score: 7.0   memory length: 239679   epsilon: 0.723433600006004    steps: 406    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1174   score: 4.0   memory length: 239974   epsilon: 0.7228495000060167    steps: 295    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1175   score: 4.0   memory length: 240248   epsilon: 0.7223069800060284    steps: 274    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1176   score: 3.0   memory length: 240473   epsilon: 0.7218614800060381    steps: 225    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1177   score: 1.0   memory length: 240623   epsilon: 0.7215644800060446    steps: 150    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1178   score: 3.0   memory length: 240868   epsilon: 0.7210793800060551    steps: 245    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1179   score: 3.0   memory length: 241094   epsilon: 0.7206319000060648    steps: 226    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1180   score: 3.0   memory length: 241307   epsilon: 0.720210160006074    steps: 213    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1181   score: 3.0   memory length: 241554   epsilon: 0.7197211000060846    steps: 247    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1182   score: 5.0   memory length: 241919   epsilon: 0.7189984000061003    steps: 365    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1183   score: 4.0   memory length: 242212   epsilon: 0.7184182600061129    steps: 293    lr: 4e-05     evaluation reward: 3.57\n",
      "episode: 1184   score: 1.0   memory length: 242362   epsilon: 0.7181212600061193    steps: 150    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1185   score: 4.0   memory length: 242634   epsilon: 0.717582700006131    steps: 272    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1186   score: 3.0   memory length: 242861   epsilon: 0.7171332400061408    steps: 227    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1187   score: 4.0   memory length: 243124   epsilon: 0.7166125000061521    steps: 263    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1188   score: 6.0   memory length: 243497   epsilon: 0.7158739600061681    steps: 373    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1189   score: 2.0   memory length: 243694   epsilon: 0.7154839000061766    steps: 197    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1190   score: 9.0   memory length: 244174   epsilon: 0.7145335000061972    steps: 480    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1191   score: 4.0   memory length: 244450   epsilon: 0.7139870200062091    steps: 276    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1192   score: 5.0   memory length: 244757   epsilon: 0.7133791600062223    steps: 307    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1193   score: 7.0   memory length: 245185   epsilon: 0.7125317200062407    steps: 428    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1194   score: 3.0   memory length: 245412   epsilon: 0.7120822600062504    steps: 227    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1195   score: 3.0   memory length: 245660   epsilon: 0.7115912200062611    steps: 248    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1196   score: 2.0   memory length: 245840   epsilon: 0.7112348200062688    steps: 180    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1197   score: 4.0   memory length: 246095   epsilon: 0.7107299200062798    steps: 255    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1198   score: 4.0   memory length: 246369   epsilon: 0.7101874000062915    steps: 274    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1199   score: 6.0   memory length: 246742   epsilon: 0.7094488600063076    steps: 373    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1200   score: 7.0   memory length: 247119   epsilon: 0.7087024000063238    steps: 377    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1201   score: 0.0   memory length: 247242   epsilon: 0.7084588600063291    steps: 123    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1202   score: 8.0   memory length: 247678   epsilon: 0.7075955800063478    steps: 436    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1203   score: 3.0   memory length: 247904   epsilon: 0.7071481000063575    steps: 226    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1204   score: 3.0   memory length: 248148   epsilon: 0.706664980006368    steps: 244    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1205   score: 3.0   memory length: 248396   epsilon: 0.7061739400063787    steps: 248    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1206   score: 2.0   memory length: 248613   epsilon: 0.705744280006388    steps: 217    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1207   score: 3.0   memory length: 248859   epsilon: 0.7052572000063986    steps: 246    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1208   score: 5.0   memory length: 249205   epsilon: 0.7045721200064134    steps: 346    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1209   score: 2.0   memory length: 249407   epsilon: 0.7041721600064221    steps: 202    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1210   score: 2.0   memory length: 249628   epsilon: 0.7037345800064316    steps: 221    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1211   score: 2.0   memory length: 249828   epsilon: 0.7033385800064402    steps: 200    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1212   score: 3.0   memory length: 250055   epsilon: 0.70288912000645    steps: 227    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1213   score: 0.0   memory length: 250178   epsilon: 0.7026455800064553    steps: 123    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1214   score: 2.0   memory length: 250380   epsilon: 0.702245620006464    steps: 202    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1215   score: 3.0   memory length: 250606   epsilon: 0.7017981400064737    steps: 226    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1216   score: 3.0   memory length: 250833   epsilon: 0.7013486800064834    steps: 227    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1217   score: 4.0   memory length: 251095   epsilon: 0.7008299200064947    steps: 262    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1218   score: 1.0   memory length: 251246   epsilon: 0.7005309400065012    steps: 151    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1219   score: 3.0   memory length: 251474   epsilon: 0.700079500006511    steps: 228    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1220   score: 10.0   memory length: 251965   epsilon: 0.6991073200065321    steps: 491    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1221   score: 5.0   memory length: 252264   epsilon: 0.6985153000065449    steps: 299    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1222   score: 4.0   memory length: 252524   epsilon: 0.6980005000065561    steps: 260    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1223   score: 2.0   memory length: 252740   epsilon: 0.6975728200065654    steps: 216    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1224   score: 4.0   memory length: 253008   epsilon: 0.6970421800065769    steps: 268    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1225   score: 3.0   memory length: 253236   epsilon: 0.6965907400065867    steps: 228    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1226   score: 2.0   memory length: 253434   epsilon: 0.6961987000065952    steps: 198    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1227   score: 2.0   memory length: 253633   epsilon: 0.6958046800066038    steps: 199    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1228   score: 10.0   memory length: 254039   epsilon: 0.6950008000066212    steps: 406    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1229   score: 7.0   memory length: 254425   epsilon: 0.6942365200066378    steps: 386    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1230   score: 5.0   memory length: 254767   epsilon: 0.6935593600066525    steps: 342    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1231   score: 1.0   memory length: 254937   epsilon: 0.6932227600066598    steps: 170    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1232   score: 4.0   memory length: 255215   epsilon: 0.6926723200066718    steps: 278    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1233   score: 3.0   memory length: 255464   epsilon: 0.6921793000066825    steps: 249    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1234   score: 5.0   memory length: 255785   epsilon: 0.6915437200066963    steps: 321    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1235   score: 4.0   memory length: 256080   epsilon: 0.690959620006709    steps: 295    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1236   score: 6.0   memory length: 256494   epsilon: 0.6901399000067268    steps: 414    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1237   score: 1.0   memory length: 256644   epsilon: 0.6898429000067332    steps: 150    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1238   score: 2.0   memory length: 256842   epsilon: 0.6894508600067417    steps: 198    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1239   score: 1.0   memory length: 256992   epsilon: 0.6891538600067482    steps: 150    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1240   score: 0.0   memory length: 257114   epsilon: 0.6889123000067534    steps: 122    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1241   score: 7.0   memory length: 257528   epsilon: 0.6880925800067712    steps: 414    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1242   score: 5.0   memory length: 257826   epsilon: 0.687502540006784    steps: 298    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1243   score: 8.0   memory length: 258121   epsilon: 0.6869184400067967    steps: 295    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1244   score: 3.0   memory length: 258334   epsilon: 0.6864967000068058    steps: 213    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1245   score: 4.0   memory length: 258599   epsilon: 0.6859720000068172    steps: 265    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1246   score: 4.0   memory length: 258902   epsilon: 0.6853720600068303    steps: 303    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1247   score: 3.0   memory length: 259167   epsilon: 0.6848473600068417    steps: 265    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1248   score: 7.0   memory length: 259571   epsilon: 0.684047440006859    steps: 404    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1249   score: 4.0   memory length: 259833   epsilon: 0.6835286800068703    steps: 262    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1250   score: 2.0   memory length: 260031   epsilon: 0.6831366400068788    steps: 198    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1251   score: 6.0   memory length: 260396   epsilon: 0.6824139400068945    steps: 365    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1252   score: 4.0   memory length: 260674   epsilon: 0.6818635000069064    steps: 278    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1253   score: 7.0   memory length: 261073   epsilon: 0.6810734800069236    steps: 399    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1254   score: 3.0   memory length: 261320   epsilon: 0.6805844200069342    steps: 247    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1255   score: 7.0   memory length: 261724   epsilon: 0.6797845000069516    steps: 404    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1256   score: 7.0   memory length: 262099   epsilon: 0.6790420000069677    steps: 375    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1257   score: 3.0   memory length: 262309   epsilon: 0.6786262000069767    steps: 210    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1258   score: 3.0   memory length: 262557   epsilon: 0.6781351600069874    steps: 248    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1259   score: 9.0   memory length: 263028   epsilon: 0.6772025800070076    steps: 471    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1260   score: 5.0   memory length: 263348   epsilon: 0.6765689800070214    steps: 320    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1261   score: 3.0   memory length: 263576   epsilon: 0.6761175400070312    steps: 228    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1262   score: 2.0   memory length: 263776   epsilon: 0.6757215400070398    steps: 200    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1263   score: 4.0   memory length: 264054   epsilon: 0.6751711000070517    steps: 278    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1264   score: 4.0   memory length: 264328   epsilon: 0.6746285800070635    steps: 274    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1265   score: 3.0   memory length: 264593   epsilon: 0.6741038800070749    steps: 265    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1266   score: 5.0   memory length: 264897   epsilon: 0.673501960007088    steps: 304    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1267   score: 4.0   memory length: 265141   epsilon: 0.6730188400070984    steps: 244    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1268   score: 2.0   memory length: 265339   epsilon: 0.672626800007107    steps: 198    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1269   score: 3.0   memory length: 265551   epsilon: 0.6722070400071161    steps: 212    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1270   score: 5.0   memory length: 265876   epsilon: 0.67156354000713    steps: 325    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1271   score: 6.0   memory length: 266215   epsilon: 0.6708923200071446    steps: 339    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1272   score: 12.0   memory length: 266745   epsilon: 0.6698429200071674    steps: 530    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1273   score: 2.0   memory length: 266929   epsilon: 0.6694786000071753    steps: 184    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1274   score: 1.0   memory length: 267079   epsilon: 0.6691816000071817    steps: 150    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1275   score: 1.0   memory length: 267230   epsilon: 0.6688826200071882    steps: 151    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1276   score: 3.0   memory length: 267458   epsilon: 0.668431180007198    steps: 228    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1277   score: 4.0   memory length: 267758   epsilon: 0.6678371800072109    steps: 300    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1278   score: 6.0   memory length: 268108   epsilon: 0.667144180007226    steps: 350    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1279   score: 3.0   memory length: 268359   epsilon: 0.6666472000072368    steps: 251    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1280   score: 3.0   memory length: 268606   epsilon: 0.6661581400072474    steps: 247    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1281   score: 1.0   memory length: 268775   epsilon: 0.6658235200072546    steps: 169    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1282   score: 4.0   memory length: 269050   epsilon: 0.6652790200072665    steps: 275    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1283   score: 6.0   memory length: 269423   epsilon: 0.6645404800072825    steps: 373    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1284   score: 4.0   memory length: 269680   epsilon: 0.6640316200072935    steps: 257    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1285   score: 4.0   memory length: 269934   epsilon: 0.6635287000073045    steps: 254    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1286   score: 5.0   memory length: 270245   epsilon: 0.6629129200073178    steps: 311    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1287   score: 3.0   memory length: 270495   epsilon: 0.6624179200073286    steps: 250    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1288   score: 4.0   memory length: 270793   epsilon: 0.6618278800073414    steps: 298    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1289   score: 5.0   memory length: 271091   epsilon: 0.6612378400073542    steps: 298    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1290   score: 5.0   memory length: 271377   epsilon: 0.6606715600073665    steps: 286    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1291   score: 7.0   memory length: 271786   epsilon: 0.6598617400073841    steps: 409    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1292   score: 3.0   memory length: 272016   epsilon: 0.659406340007394    steps: 230    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1293   score: 4.0   memory length: 272275   epsilon: 0.6588935200074051    steps: 259    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1294   score: 4.0   memory length: 272535   epsilon: 0.6583787200074163    steps: 260    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1295   score: 3.0   memory length: 272801   epsilon: 0.6578520400074277    steps: 266    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1296   score: 5.0   memory length: 273127   epsilon: 0.6572065600074417    steps: 326    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1297   score: 5.0   memory length: 273433   epsilon: 0.6566006800074549    steps: 306    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1298   score: 10.0   memory length: 273916   epsilon: 0.6556443400074756    steps: 483    lr: 4e-05     evaluation reward: 4.05\n",
      "episode: 1299   score: 3.0   memory length: 274144   epsilon: 0.6551929000074854    steps: 228    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1300   score: 6.0   memory length: 274508   epsilon: 0.6544721800075011    steps: 364    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1301   score: 5.0   memory length: 274812   epsilon: 0.6538702600075141    steps: 304    lr: 4e-05     evaluation reward: 4.06\n",
      "episode: 1302   score: 3.0   memory length: 275042   epsilon: 0.653414860007524    steps: 230    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1303   score: 6.0   memory length: 275401   epsilon: 0.6527040400075395    steps: 359    lr: 4e-05     evaluation reward: 4.04\n",
      "episode: 1304   score: 1.0   memory length: 275570   epsilon: 0.6523694200075467    steps: 169    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1305   score: 6.0   memory length: 275928   epsilon: 0.6516605800075621    steps: 358    lr: 4e-05     evaluation reward: 4.05\n",
      "episode: 1306   score: 8.0   memory length: 276369   epsilon: 0.6507874000075811    steps: 441    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1307   score: 1.0   memory length: 276519   epsilon: 0.6504904000075875    steps: 150    lr: 4e-05     evaluation reward: 4.09\n",
      "episode: 1308   score: 6.0   memory length: 276882   epsilon: 0.6497716600076031    steps: 363    lr: 4e-05     evaluation reward: 4.1\n",
      "episode: 1309   score: 5.0   memory length: 277190   epsilon: 0.6491618200076164    steps: 308    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1310   score: 5.0   memory length: 277506   epsilon: 0.6485361400076299    steps: 316    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1311   score: 3.0   memory length: 277752   epsilon: 0.6480490600076405    steps: 246    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1312   score: 4.0   memory length: 277993   epsilon: 0.6475718800076509    steps: 241    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1313   score: 3.0   memory length: 278239   epsilon: 0.6470848000076614    steps: 246    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1314   score: 2.0   memory length: 278456   epsilon: 0.6466551400076708    steps: 217    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1315   score: 3.0   memory length: 278702   epsilon: 0.6461680600076813    steps: 246    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1316   score: 4.0   memory length: 278977   epsilon: 0.6456235600076932    steps: 275    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1317   score: 3.0   memory length: 279190   epsilon: 0.6452018200077023    steps: 213    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1318   score: 2.0   memory length: 279388   epsilon: 0.6448097800077108    steps: 198    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1319   score: 6.0   memory length: 279715   epsilon: 0.6441623200077249    steps: 327    lr: 4e-05     evaluation reward: 4.25\n",
      "episode: 1320   score: 2.0   memory length: 279895   epsilon: 0.6438059200077326    steps: 180    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1321   score: 4.0   memory length: 280194   epsilon: 0.6432139000077455    steps: 299    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1322   score: 4.0   memory length: 280464   epsilon: 0.6426793000077571    steps: 270    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1323   score: 2.0   memory length: 280680   epsilon: 0.6422516200077664    steps: 216    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1324   score: 5.0   memory length: 281004   epsilon: 0.6416101000077803    steps: 324    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1325   score: 4.0   memory length: 281299   epsilon: 0.641026000007793    steps: 295    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1326   score: 3.0   memory length: 281528   epsilon: 0.6405725800078028    steps: 229    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1327   score: 9.0   memory length: 281983   epsilon: 0.6396716800078224    steps: 455    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1328   score: 3.0   memory length: 282251   epsilon: 0.6391410400078339    steps: 268    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1329   score: 5.0   memory length: 282540   epsilon: 0.6385688200078463    steps: 289    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1330   score: 5.0   memory length: 282852   epsilon: 0.6379510600078597    steps: 312    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1331   score: 6.0   memory length: 283205   epsilon: 0.6372521200078749    steps: 353    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1332   score: 8.0   memory length: 283673   epsilon: 0.636325480007895    steps: 468    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1333   score: 3.0   memory length: 283885   epsilon: 0.6359057200079041    steps: 212    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1334   score: 3.0   memory length: 284132   epsilon: 0.6354166600079147    steps: 247    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1335   score: 3.0   memory length: 284378   epsilon: 0.6349295800079253    steps: 246    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1336   score: 1.0   memory length: 284547   epsilon: 0.6345949600079326    steps: 169    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1337   score: 9.0   memory length: 285012   epsilon: 0.6336742600079526    steps: 465    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1338   score: 3.0   memory length: 285259   epsilon: 0.6331852000079632    steps: 247    lr: 4e-05     evaluation reward: 4.27\n",
      "episode: 1339   score: 1.0   memory length: 285409   epsilon: 0.6328882000079696    steps: 150    lr: 4e-05     evaluation reward: 4.27\n",
      "episode: 1340   score: 3.0   memory length: 285655   epsilon: 0.6324011200079802    steps: 246    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1341   score: 8.0   memory length: 286053   epsilon: 0.6316130800079973    steps: 398    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1342   score: 4.0   memory length: 286324   epsilon: 0.631076500008009    steps: 271    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1343   score: 5.0   memory length: 286647   epsilon: 0.6304369600080229    steps: 323    lr: 4e-05     evaluation reward: 4.27\n",
      "episode: 1344   score: 6.0   memory length: 287021   epsilon: 0.6296964400080389    steps: 374    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1345   score: 5.0   memory length: 287327   epsilon: 0.6290905600080521    steps: 306    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1346   score: 1.0   memory length: 287478   epsilon: 0.6287915800080586    steps: 151    lr: 4e-05     evaluation reward: 4.28\n",
      "episode: 1347   score: 3.0   memory length: 287703   epsilon: 0.6283460800080682    steps: 225    lr: 4e-05     evaluation reward: 4.28\n",
      "episode: 1348   score: 2.0   memory length: 287884   epsilon: 0.627987700008076    steps: 181    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1349   score: 2.0   memory length: 288100   epsilon: 0.6275600200080853    steps: 216    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1350   score: 5.0   memory length: 288424   epsilon: 0.6269185000080992    steps: 324    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1351   score: 5.0   memory length: 288732   epsilon: 0.6263086600081125    steps: 308    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1352   score: 3.0   memory length: 288983   epsilon: 0.6258116800081233    steps: 251    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1353   score: 1.0   memory length: 289153   epsilon: 0.6254750800081306    steps: 170    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1354   score: 3.0   memory length: 289398   epsilon: 0.6249899800081411    steps: 245    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1355   score: 8.0   memory length: 289867   epsilon: 0.6240613600081613    steps: 469    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1356   score: 9.0   memory length: 290375   epsilon: 0.6230555200081831    steps: 508    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1357   score: 2.0   memory length: 290572   epsilon: 0.6226654600081916    steps: 197    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1358   score: 3.0   memory length: 290801   epsilon: 0.6222120400082014    steps: 229    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1359   score: 7.0   memory length: 291207   epsilon: 0.6214081600082189    steps: 406    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1360   score: 1.0   memory length: 291378   epsilon: 0.6210695800082262    steps: 171    lr: 4e-05     evaluation reward: 4.12\n",
      "episode: 1361   score: 1.0   memory length: 291529   epsilon: 0.6207706000082327    steps: 151    lr: 4e-05     evaluation reward: 4.1\n",
      "episode: 1362   score: 6.0   memory length: 291885   epsilon: 0.620065720008248    steps: 356    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1363   score: 5.0   memory length: 292227   epsilon: 0.6193885600082627    steps: 342    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1364   score: 3.0   memory length: 292456   epsilon: 0.6189351400082725    steps: 229    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1365   score: 5.0   memory length: 292763   epsilon: 0.6183272800082857    steps: 307    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1366   score: 1.0   memory length: 292935   epsilon: 0.6179867200082931    steps: 172    lr: 4e-05     evaluation reward: 4.12\n",
      "episode: 1367   score: 5.0   memory length: 293252   epsilon: 0.6173590600083068    steps: 317    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1368   score: 3.0   memory length: 293519   epsilon: 0.6168304000083182    steps: 267    lr: 4e-05     evaluation reward: 4.14\n",
      "episode: 1369   score: 7.0   memory length: 293911   epsilon: 0.6160542400083351    steps: 392    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1370   score: 3.0   memory length: 294138   epsilon: 0.6156047800083448    steps: 227    lr: 4e-05     evaluation reward: 4.16\n",
      "episode: 1371   score: 3.0   memory length: 294368   epsilon: 0.6151493800083547    steps: 230    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1372   score: 4.0   memory length: 294663   epsilon: 0.6145652800083674    steps: 295    lr: 4e-05     evaluation reward: 4.05\n",
      "episode: 1373   score: 3.0   memory length: 294908   epsilon: 0.6140801800083779    steps: 245    lr: 4e-05     evaluation reward: 4.06\n",
      "episode: 1374   score: 6.0   memory length: 295266   epsilon: 0.6133713400083933    steps: 358    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1375   score: 8.0   memory length: 295728   epsilon: 0.6124565800084132    steps: 462    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1376   score: 4.0   memory length: 296003   epsilon: 0.611912080008425    steps: 275    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1377   score: 5.0   memory length: 296309   epsilon: 0.6113062000084382    steps: 306    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1378   score: 5.0   memory length: 296654   epsilon: 0.610623100008453    steps: 345    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1379   score: 3.0   memory length: 296865   epsilon: 0.6102053200084621    steps: 211    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1380   score: 6.0   memory length: 297222   epsilon: 0.6094984600084774    steps: 357    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1381   score: 3.0   memory length: 297470   epsilon: 0.6090074200084881    steps: 248    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1382   score: 2.0   memory length: 297670   epsilon: 0.6086114200084967    steps: 200    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1383   score: 5.0   memory length: 297996   epsilon: 0.6079659400085107    steps: 326    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1384   score: 7.0   memory length: 298244   epsilon: 0.6074749000085213    steps: 248    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1385   score: 3.0   memory length: 298470   epsilon: 0.607027420008531    steps: 226    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1386   score: 4.0   memory length: 298747   epsilon: 0.606478960008543    steps: 277    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1387   score: 3.0   memory length: 298997   epsilon: 0.6059839600085537    steps: 250    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1388   score: 6.0   memory length: 299349   epsilon: 0.6052870000085688    steps: 352    lr: 4e-05     evaluation reward: 4.24\n",
      "episode: 1389   score: 1.0   memory length: 299517   epsilon: 0.604954360008576    steps: 168    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1390   score: 2.0   memory length: 299716   epsilon: 0.6045603400085846    steps: 199    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1391   score: 5.0   memory length: 300022   epsilon: 0.6039544600085978    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1392   score: 5.0   memory length: 300347   epsilon: 0.6033109600086117    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.17\n",
      "episode: 1393   score: 7.0   memory length: 300733   epsilon: 0.6025466800086283    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1394   score: 5.0   memory length: 301015   epsilon: 0.6019883200086404    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1395   score: 6.0   memory length: 301385   epsilon: 0.6012557200086563    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1396   score: 7.0   memory length: 301775   epsilon: 0.6004835200086731    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1397   score: 4.0   memory length: 302016   epsilon: 0.6000063400086835    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1398   score: 5.0   memory length: 302314   epsilon: 0.5994163000086963    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1399   score: 6.0   memory length: 302665   epsilon: 0.5987213200087114    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1400   score: 4.0   memory length: 302942   epsilon: 0.5981728600087233    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1401   score: 1.0   memory length: 303092   epsilon: 0.5978758600087297    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 4.17\n",
      "episode: 1402   score: 1.0   memory length: 303243   epsilon: 0.5975768800087362    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1403   score: 4.0   memory length: 303501   epsilon: 0.5970660400087473    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.13\n",
      "episode: 1404   score: 9.0   memory length: 303993   epsilon: 0.5960918800087684    steps: 492    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1405   score: 9.0   memory length: 304486   epsilon: 0.5951157400087896    steps: 493    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1406   score: 4.0   memory length: 304729   epsilon: 0.5946346000088001    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1407   score: 3.0   memory length: 304972   epsilon: 0.5941534600088105    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1408   score: 3.0   memory length: 305220   epsilon: 0.5936624200088212    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1409   score: 4.0   memory length: 305516   epsilon: 0.5930763400088339    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.18\n",
      "episode: 1410   score: 3.0   memory length: 305761   epsilon: 0.5925912400088444    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1411   score: 4.0   memory length: 306018   epsilon: 0.5920823800088555    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.17\n",
      "episode: 1412   score: 5.0   memory length: 306343   epsilon: 0.5914388800088695    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.18\n",
      "episode: 1413   score: 5.0   memory length: 306650   epsilon: 0.5908310200088827    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1414   score: 2.0   memory length: 306848   epsilon: 0.5904389800088912    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1415   score: 2.0   memory length: 307030   epsilon: 0.590078620008899    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1416   score: 5.0   memory length: 307368   epsilon: 0.5894093800089135    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1417   score: 6.0   memory length: 307683   epsilon: 0.5887856800089271    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1418   score: 6.0   memory length: 308046   epsilon: 0.5880669400089427    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1419   score: 1.0   memory length: 308197   epsilon: 0.5877679600089492    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1420   score: 3.0   memory length: 308410   epsilon: 0.5873462200089583    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1421   score: 3.0   memory length: 308658   epsilon: 0.586855180008969    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1422   score: 1.0   memory length: 308830   epsilon: 0.5865146200089764    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1423   score: 6.0   memory length: 309227   epsilon: 0.5857285600089934    steps: 397    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1424   score: 7.0   memory length: 309643   epsilon: 0.5849048800090113    steps: 416    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1425   score: 6.0   memory length: 309997   epsilon: 0.5842039600090265    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1426   score: 3.0   memory length: 310209   epsilon: 0.5837842000090356    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1427   score: 5.0   memory length: 310517   epsilon: 0.5831743600090489    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1428   score: 6.0   memory length: 310852   epsilon: 0.5825110600090633    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1429   score: 3.0   memory length: 311098   epsilon: 0.5820239800090738    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1430   score: 4.0   memory length: 311376   epsilon: 0.5814735400090858    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1431   score: 2.0   memory length: 311562   epsilon: 0.5811052600090938    steps: 186    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1432   score: 1.0   memory length: 311730   epsilon: 0.580772620009101    steps: 168    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1433   score: 3.0   memory length: 311959   epsilon: 0.5803192000091109    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1434   score: 2.0   memory length: 312141   epsilon: 0.5799588400091187    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.11\n",
      "episode: 1435   score: 6.0   memory length: 312532   epsilon: 0.5791846600091355    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1436   score: 7.0   memory length: 312954   epsilon: 0.5783491000091536    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1437   score: 7.0   memory length: 313367   epsilon: 0.5775313600091714    steps: 413    lr: 1.6000000000000003e-05     evaluation reward: 4.18\n",
      "episode: 1438   score: 3.0   memory length: 313614   epsilon: 0.577042300009182    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.18\n",
      "episode: 1439   score: 2.0   memory length: 313797   epsilon: 0.5766799600091899    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1440   score: 3.0   memory length: 314042   epsilon: 0.5761948600092004    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1441   score: 5.0   memory length: 314382   epsilon: 0.575521660009215    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1442   score: 5.0   memory length: 314686   epsilon: 0.5749197400092281    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.17\n",
      "episode: 1443   score: 7.0   memory length: 315092   epsilon: 0.5741158600092455    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1444   score: 3.0   memory length: 315317   epsilon: 0.5736703600092552    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1445   score: 3.0   memory length: 315529   epsilon: 0.5732506000092643    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1446   score: 6.0   memory length: 315866   epsilon: 0.5725833400092788    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1447   score: 3.0   memory length: 316092   epsilon: 0.5721358600092885    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1448   score: 3.0   memory length: 316305   epsilon: 0.5717141200092977    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1449   score: 2.0   memory length: 316506   epsilon: 0.5713161400093063    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1450   score: 7.0   memory length: 316910   epsilon: 0.5705162200093237    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1451   score: 2.0   memory length: 317090   epsilon: 0.5701598200093314    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1452   score: 5.0   memory length: 317439   epsilon: 0.5694688000093464    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1453   score: 5.0   memory length: 317767   epsilon: 0.5688193600093605    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1454   score: 4.0   memory length: 318029   epsilon: 0.5683006000093718    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1455   score: 2.0   memory length: 318214   epsilon: 0.5679343000093797    steps: 185    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1456   score: 5.0   memory length: 318539   epsilon: 0.5672908000093937    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1457   score: 7.0   memory length: 318966   epsilon: 0.566445340009412    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1458   score: 4.0   memory length: 319208   epsilon: 0.5659661800094224    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1459   score: 4.0   memory length: 319505   epsilon: 0.5653781200094352    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1460   score: 3.0   memory length: 319734   epsilon: 0.5649247000094451    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1461   score: 3.0   memory length: 319985   epsilon: 0.5644277200094558    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1462   score: 3.0   memory length: 320211   epsilon: 0.5639802400094656    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1463   score: 2.0   memory length: 320393   epsilon: 0.5636198800094734    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 4.17\n",
      "episode: 1464   score: 7.0   memory length: 320789   epsilon: 0.5628358000094904    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1465   score: 7.0   memory length: 321142   epsilon: 0.5621368600095056    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1466   score: 3.0   memory length: 321371   epsilon: 0.5616834400095154    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1467   score: 4.0   memory length: 321636   epsilon: 0.5611587400095268    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1468   score: 9.0   memory length: 322056   epsilon: 0.5603271400095449    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1469   score: 4.0   memory length: 322332   epsilon: 0.5597806600095567    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1470   score: 4.0   memory length: 322625   epsilon: 0.5592005200095693    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1471   score: 3.0   memory length: 322852   epsilon: 0.5587510600095791    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1472   score: 4.0   memory length: 323128   epsilon: 0.558204580009591    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1473   score: 7.0   memory length: 323374   epsilon: 0.5577175000096015    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1474   score: 3.0   memory length: 323621   epsilon: 0.5572284400096121    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1475   score: 7.0   memory length: 324024   epsilon: 0.5564305000096295    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1476   score: 3.0   memory length: 324271   epsilon: 0.5559414400096401    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1477   score: 4.0   memory length: 324567   epsilon: 0.5553553600096528    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1478   score: 6.0   memory length: 324940   epsilon: 0.5546168200096688    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1479   score: 4.0   memory length: 325199   epsilon: 0.55410400000968    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1480   score: 3.0   memory length: 325444   epsilon: 0.5536189000096905    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1481   score: 3.0   memory length: 325670   epsilon: 0.5531714200097002    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1482   score: 2.0   memory length: 325868   epsilon: 0.5527793800097087    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1483   score: 13.0   memory length: 326356   epsilon: 0.5518131400097297    steps: 488    lr: 1.6000000000000003e-05     evaluation reward: 4.33\n",
      "episode: 1484   score: 6.0   memory length: 326719   epsilon: 0.5510944000097453    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1485   score: 5.0   memory length: 327044   epsilon: 0.5504509000097593    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1486   score: 2.0   memory length: 327224   epsilon: 0.550094500009767    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1487   score: 9.0   memory length: 327565   epsilon: 0.5494193200097817    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1488   score: 5.0   memory length: 327913   epsilon: 0.5487302800097966    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1489   score: 8.0   memory length: 328356   epsilon: 0.5478531400098157    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 4.44\n",
      "episode: 1490   score: 5.0   memory length: 328702   epsilon: 0.5471680600098305    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1491   score: 4.0   memory length: 328977   epsilon: 0.5466235600098424    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1492   score: 2.0   memory length: 329158   epsilon: 0.5462651800098501    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
      "episode: 1493   score: 3.0   memory length: 329386   epsilon: 0.5458137400098599    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1494   score: 3.0   memory length: 329612   epsilon: 0.5453662600098697    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.37\n",
      "episode: 1495   score: 8.0   memory length: 330064   epsilon: 0.5444713000098891    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1496   score: 3.0   memory length: 330312   epsilon: 0.5439802600098997    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.35\n",
      "episode: 1497   score: 5.0   memory length: 330617   epsilon: 0.5433763600099129    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 4.36\n",
      "episode: 1498   score: 10.0   memory length: 331124   epsilon: 0.5423725000099346    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 4.41\n",
      "episode: 1499   score: 4.0   memory length: 331399   epsilon: 0.5418280000099465    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.39\n",
      "episode: 1500   score: 5.0   memory length: 331703   epsilon: 0.5412260800099595    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1501   score: 1.0   memory length: 331873   epsilon: 0.5408894800099668    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1502   score: 6.0   memory length: 332246   epsilon: 0.5401509400099829    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1503   score: 4.0   memory length: 332507   epsilon: 0.5396341600099941    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1504   score: 6.0   memory length: 332837   epsilon: 0.5389807600100083    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 4.42\n",
      "episode: 1505   score: 7.0   memory length: 333263   epsilon: 0.5381372800100266    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1506   score: 4.0   memory length: 333559   epsilon: 0.5375512000100393    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1507   score: 7.0   memory length: 333962   epsilon: 0.5367532600100566    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 4.44\n",
      "episode: 1508   score: 11.0   memory length: 334541   epsilon: 0.5356068400100815    steps: 579    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1509   score: 7.0   memory length: 334939   epsilon: 0.5348188000100986    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
      "episode: 1510   score: 6.0   memory length: 335274   epsilon: 0.534155500010113    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1511   score: 6.0   memory length: 335623   epsilon: 0.533464480010128    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1512   score: 6.0   memory length: 335968   epsilon: 0.5327813800101429    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1513   score: 4.0   memory length: 336225   epsilon: 0.5322725200101539    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1514   score: 3.0   memory length: 336438   epsilon: 0.5318507800101631    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1515   score: 11.0   memory length: 336997   epsilon: 0.5307439600101871    steps: 559    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1516   score: 3.0   memory length: 337222   epsilon: 0.5302984600101968    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1517   score: 2.0   memory length: 337420   epsilon: 0.5299064200102053    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1518   score: 6.0   memory length: 337756   epsilon: 0.5292411400102197    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1519   score: 6.0   memory length: 338065   epsilon: 0.528629320010233    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1520   score: 4.0   memory length: 338327   epsilon: 0.5281105600102443    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1521   score: 4.0   memory length: 338608   epsilon: 0.5275541800102563    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1522   score: 5.0   memory length: 338932   epsilon: 0.5269126600102703    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1523   score: 3.0   memory length: 339163   epsilon: 0.5264552800102802    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1524   score: 5.0   memory length: 339506   epsilon: 0.5257761400102949    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1525   score: 6.0   memory length: 339878   epsilon: 0.5250395800103109    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1526   score: 8.0   memory length: 340344   epsilon: 0.524116900010331    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1527   score: 6.0   memory length: 340697   epsilon: 0.5234179600103461    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.76\n",
      "episode: 1528   score: 11.0   memory length: 341163   epsilon: 0.5224952800103662    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1529   score: 3.0   memory length: 341389   epsilon: 0.5220478000103759    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.81\n",
      "episode: 1530   score: 7.0   memory length: 341772   epsilon: 0.5212894600103923    steps: 383    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
      "episode: 1531   score: 7.0   memory length: 342208   epsilon: 0.5204261800104111    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
      "episode: 1532   score: 10.0   memory length: 342726   epsilon: 0.5194005400104333    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 4.98\n",
      "episode: 1533   score: 9.0   memory length: 343243   epsilon: 0.5183768800104556    steps: 517    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1534   score: 6.0   memory length: 343634   epsilon: 0.5176027000104724    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1535   score: 5.0   memory length: 343958   epsilon: 0.5169611800104863    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1536   score: 4.0   memory length: 344236   epsilon: 0.5164107400104982    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.04\n",
      "episode: 1537   score: 5.0   memory length: 344514   epsilon: 0.5158603000105102    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1538   score: 4.0   memory length: 344791   epsilon: 0.5153118400105221    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1539   score: 8.0   memory length: 345232   epsilon: 0.5144386600105411    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1540   score: 4.0   memory length: 345492   epsilon: 0.5139238600105522    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1541   score: 5.0   memory length: 345817   epsilon: 0.5132803600105662    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1542   score: 4.0   memory length: 346074   epsilon: 0.5127715000105773    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1543   score: 6.0   memory length: 346439   epsilon: 0.5120488000105929    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 5.08\n",
      "episode: 1544   score: 4.0   memory length: 346696   epsilon: 0.511539940010604    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
      "episode: 1545   score: 6.0   memory length: 347067   epsilon: 0.5108053600106199    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
      "episode: 1546   score: 5.0   memory length: 347411   epsilon: 0.5101242400106347    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1547   score: 9.0   memory length: 347818   epsilon: 0.5093183800106522    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1548   score: 4.0   memory length: 348092   epsilon: 0.508775860010664    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1549   score: 6.0   memory length: 348440   epsilon: 0.508086820010679    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1550   score: 8.0   memory length: 348878   epsilon: 0.5072195800106978    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1551   score: 11.0   memory length: 349306   epsilon: 0.5063721400107162    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1552   score: 3.0   memory length: 349535   epsilon: 0.505918720010726    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1553   score: 7.0   memory length: 349937   epsilon: 0.5051227600107433    steps: 402    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1554   score: 9.0   memory length: 350401   epsilon: 0.5042040400107632    steps: 464    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1555   score: 3.0   memory length: 350647   epsilon: 0.5037169600107738    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1556   score: 7.0   memory length: 351031   epsilon: 0.5029566400107903    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1557   score: 7.0   memory length: 351413   epsilon: 0.5022002800108067    steps: 382    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1558   score: 3.0   memory length: 351638   epsilon: 0.5017547800108164    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1559   score: 4.0   memory length: 351936   epsilon: 0.5011647400108292    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1560   score: 6.0   memory length: 352300   epsilon: 0.5004440200108449    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1561   score: 3.0   memory length: 352510   epsilon: 0.5000282200108539    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1562   score: 3.0   memory length: 352738   epsilon: 0.4995767800108518    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1563   score: 6.0   memory length: 353094   epsilon: 0.49887190001084736    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1564   score: 1.0   memory length: 353244   epsilon: 0.4985749000108455    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1565   score: 4.0   memory length: 353487   epsilon: 0.49809376001084243    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1566   score: 2.0   memory length: 353685   epsilon: 0.49770172001083995    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1567   score: 5.0   memory length: 353994   epsilon: 0.4970899000108361    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1568   score: 6.0   memory length: 354369   epsilon: 0.4963474000108314    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.34\n",
      "episode: 1569   score: 3.0   memory length: 354616   epsilon: 0.4958583400108283    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1570   score: 6.0   memory length: 354938   epsilon: 0.49522078001082426    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1571   score: 4.0   memory length: 355234   epsilon: 0.49463470001082055    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1572   score: 6.0   memory length: 355609   epsilon: 0.49389220001081585    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1573   score: 3.0   memory length: 355857   epsilon: 0.49340116001081274    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 5.34\n",
      "episode: 1574   score: 5.0   memory length: 356163   epsilon: 0.4927952800108089    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1575   score: 2.0   memory length: 356382   epsilon: 0.49236166001080617    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1576   score: 5.0   memory length: 356685   epsilon: 0.4917617200108024    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1577   score: 3.0   memory length: 356914   epsilon: 0.4913083000107995    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1578   score: 6.0   memory length: 357285   epsilon: 0.49057372001079486    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1579   score: 8.0   memory length: 357737   epsilon: 0.4896787600107892    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1580   score: 2.0   memory length: 357917   epsilon: 0.48932236001078694    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1581   score: 3.0   memory length: 358164   epsilon: 0.48883330001078384    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1582   score: 3.0   memory length: 358430   epsilon: 0.4883066200107805    steps: 266    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1583   score: 5.0   memory length: 358756   epsilon: 0.48766114001077643    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1584   score: 8.0   memory length: 359215   epsilon: 0.4867523200107707    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1585   score: 4.0   memory length: 359475   epsilon: 0.4862375200107674    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1586   score: 1.0   memory length: 359626   epsilon: 0.48593854001076553    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1587   score: 3.0   memory length: 359838   epsilon: 0.4855187800107629    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1588   score: 11.0   memory length: 360379   epsilon: 0.4844476000107561    steps: 541    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1589   score: 7.0   memory length: 360770   epsilon: 0.4836734200107512    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
      "episode: 1590   score: 3.0   memory length: 361017   epsilon: 0.4831843600107481    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.25\n",
      "episode: 1591   score: 6.0   memory length: 361376   epsilon: 0.4824735400107436    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
      "episode: 1592   score: 8.0   memory length: 361826   epsilon: 0.48158254001073797    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 5.33\n",
      "episode: 1593   score: 6.0   memory length: 362162   epsilon: 0.48091726001073376    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1594   score: 6.0   memory length: 362490   epsilon: 0.48026782001072965    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1595   score: 14.0   memory length: 362877   epsilon: 0.4795015600107248    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1596   score: 2.0   memory length: 363058   epsilon: 0.47914318001072254    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1597   score: 7.0   memory length: 363471   epsilon: 0.47832544001071736    steps: 413    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1598   score: 7.0   memory length: 363830   epsilon: 0.47761462001071286    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1599   score: 3.0   memory length: 364076   epsilon: 0.4771275400107098    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1600   score: 3.0   memory length: 364287   epsilon: 0.47670976001070714    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1601   score: 8.0   memory length: 364712   epsilon: 0.4758682600107018    steps: 425    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1602   score: 7.0   memory length: 365112   epsilon: 0.4750762600106968    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1603   score: 5.0   memory length: 365439   epsilon: 0.4744288000106927    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1604   score: 5.0   memory length: 365747   epsilon: 0.47381896001068885    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1605   score: 8.0   memory length: 366217   epsilon: 0.47288836001068296    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1606   score: 3.0   memory length: 366446   epsilon: 0.4724349400106801    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1607   score: 9.0   memory length: 366941   epsilon: 0.4714548400106739    steps: 495    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1608   score: 6.0   memory length: 367256   epsilon: 0.47083114001066995    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1609   score: 5.0   memory length: 367582   epsilon: 0.47018566001066586    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1610   score: 3.0   memory length: 367828   epsilon: 0.4696985800106628    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1611   score: 3.0   memory length: 368053   epsilon: 0.46925308001065996    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1612   score: 7.0   memory length: 368465   epsilon: 0.4684373200106548    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1613   score: 3.0   memory length: 368694   epsilon: 0.46798390001065193    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1614   score: 6.0   memory length: 369023   epsilon: 0.4673324800106478    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1615   score: 6.0   memory length: 369417   epsilon: 0.4665523600106429    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1616   score: 3.0   memory length: 369627   epsilon: 0.46613656001064024    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1617   score: 5.0   memory length: 369914   epsilon: 0.46556830001063665    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1618   score: 5.0   memory length: 370236   epsilon: 0.4649307400106326    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1619   score: 7.0   memory length: 370672   epsilon: 0.46406746001062715    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1620   score: 6.0   memory length: 371007   epsilon: 0.46340416001062296    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1621   score: 9.0   memory length: 371481   epsilon: 0.462465640010617    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1622   score: 7.0   memory length: 371909   epsilon: 0.46161820001061166    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1623   score: 4.0   memory length: 372203   epsilon: 0.461036080010608    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1624   score: 3.0   memory length: 372431   epsilon: 0.4605846400106051    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1625   score: 3.0   memory length: 372644   epsilon: 0.46016290001060245    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1626   score: 4.0   memory length: 372924   epsilon: 0.45960850001059894    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1627   score: 3.0   memory length: 373152   epsilon: 0.4591570600105961    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1628   score: 7.0   memory length: 373530   epsilon: 0.45840862001059135    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1629   score: 3.0   memory length: 373742   epsilon: 0.4579888600105887    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1630   score: 5.0   memory length: 374047   epsilon: 0.4573849600105849    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1631   score: 8.0   memory length: 374496   epsilon: 0.45649594001057925    steps: 449    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1632   score: 5.0   memory length: 374804   epsilon: 0.4558861000105754    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1633   score: 4.0   memory length: 375042   epsilon: 0.4554148600105724    steps: 238    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1634   score: 3.0   memory length: 375270   epsilon: 0.45496342001056955    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1635   score: 5.0   memory length: 375594   epsilon: 0.4543219000105655    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1636   score: 9.0   memory length: 376050   epsilon: 0.4534190200105598    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1637   score: 4.0   memory length: 376323   epsilon: 0.45287848001055636    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1638   score: 6.0   memory length: 376658   epsilon: 0.45221518001055216    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 5.24\n",
      "episode: 1639   score: 6.0   memory length: 377025   epsilon: 0.45148852001054757    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1640   score: 3.0   memory length: 377253   epsilon: 0.4510370800105447    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1641   score: 5.0   memory length: 377543   epsilon: 0.4504628800105411    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1642   score: 4.0   memory length: 377838   epsilon: 0.4498787800105374    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1643   score: 8.0   memory length: 378258   epsilon: 0.4490471800105321    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1644   score: 7.0   memory length: 378623   epsilon: 0.44832448001052755    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1645   score: 3.0   memory length: 378870   epsilon: 0.44783542001052445    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1646   score: 3.0   memory length: 379079   epsilon: 0.44742160001052184    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1647   score: 8.0   memory length: 379523   epsilon: 0.4465424800105163    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1648   score: 3.0   memory length: 379733   epsilon: 0.44612668001051364    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1649   score: 6.0   memory length: 380106   epsilon: 0.44538814001050897    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1650   score: 6.0   memory length: 380452   epsilon: 0.44470306001050464    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
      "episode: 1651   score: 4.0   memory length: 380747   epsilon: 0.44411896001050094    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1652   score: 3.0   memory length: 380973   epsilon: 0.4436714800104981    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1653   score: 3.0   memory length: 381183   epsilon: 0.4432556800104955    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.06\n",
      "episode: 1654   score: 5.0   memory length: 381500   epsilon: 0.4426280200104915    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
      "episode: 1655   score: 4.0   memory length: 381795   epsilon: 0.4420439200104878    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1656   score: 11.0   memory length: 382324   epsilon: 0.4409965000104812    steps: 529    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
      "episode: 1657   score: 3.0   memory length: 382553   epsilon: 0.4405430800104783    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
      "episode: 1658   score: 5.0   memory length: 382898   epsilon: 0.439859980010474    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
      "episode: 1659   score: 10.0   memory length: 383290   epsilon: 0.4390838200104691    steps: 392    lr: 1.6000000000000003e-05     evaluation reward: 5.11\n",
      "episode: 1660   score: 5.0   memory length: 383597   epsilon: 0.43847596001046524    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 5.1\n",
      "episode: 1661   score: 6.0   memory length: 383971   epsilon: 0.43773544001046055    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 5.13\n",
      "episode: 1662   score: 12.0   memory length: 384528   epsilon: 0.4366325800104536    steps: 557    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1663   score: 5.0   memory length: 384853   epsilon: 0.4359890800104495    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1664   score: 6.0   memory length: 385228   epsilon: 0.4352465800104448    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1665   score: 8.0   memory length: 385506   epsilon: 0.4346961400104413    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1666   score: 3.0   memory length: 385731   epsilon: 0.4342506400104385    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1667   score: 9.0   memory length: 386073   epsilon: 0.4335734800104342    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1668   score: 9.0   memory length: 386567   epsilon: 0.43259536001042803    steps: 494    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1669   score: 6.0   memory length: 386940   epsilon: 0.43185682001042336    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1670   score: 6.0   memory length: 387299   epsilon: 0.43114600001041886    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1671   score: 10.0   memory length: 387852   epsilon: 0.43005106001041193    steps: 553    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1672   score: 12.0   memory length: 388325   epsilon: 0.429114520010406    steps: 473    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1673   score: 5.0   memory length: 388636   epsilon: 0.4284987400104021    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1674   score: 9.0   memory length: 389102   epsilon: 0.4275760600103963    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
      "episode: 1675   score: 2.0   memory length: 389288   epsilon: 0.42720778001039394    steps: 186    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
      "episode: 1676   score: 8.0   memory length: 389761   epsilon: 0.426271240010388    steps: 473    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
      "episode: 1677   score: 9.0   memory length: 390232   epsilon: 0.4253386600103821    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 5.68\n",
      "episode: 1678   score: 7.0   memory length: 390593   epsilon: 0.4246238800103776    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
      "episode: 1679   score: 8.0   memory length: 391008   epsilon: 0.4238021800103724    steps: 415    lr: 1.6000000000000003e-05     evaluation reward: 5.69\n",
      "episode: 1680   score: 6.0   memory length: 391348   epsilon: 0.42312898001036814    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
      "episode: 1681   score: 12.0   memory length: 391813   epsilon: 0.4222082800103623    steps: 465    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1682   score: 5.0   memory length: 392124   epsilon: 0.4215925000103584    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1683   score: 9.0   memory length: 392528   epsilon: 0.42079258001035336    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1684   score: 7.0   memory length: 392934   epsilon: 0.41998870001034827    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1685   score: 3.0   memory length: 393181   epsilon: 0.4194996400103452    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1686   score: 9.0   memory length: 393651   epsilon: 0.4185690400103393    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 5.94\n",
      "episode: 1687   score: 5.0   memory length: 393940   epsilon: 0.41799682001033567    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 5.96\n",
      "episode: 1688   score: 8.0   memory length: 394416   epsilon: 0.4170543400103297    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1689   score: 5.0   memory length: 394724   epsilon: 0.41644450001032585    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1690   score: 7.0   memory length: 395089   epsilon: 0.4157218000103213    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1691   score: 6.0   memory length: 395453   epsilon: 0.4150010800103167    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1692   score: 6.0   memory length: 395817   epsilon: 0.41428036001031215    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1693   score: 8.0   memory length: 396217   epsilon: 0.41348836001030714    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1694   score: 6.0   memory length: 396575   epsilon: 0.41277952001030266    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1695   score: 6.0   memory length: 396915   epsilon: 0.4121063200102984    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1696   score: 3.0   memory length: 397127   epsilon: 0.41168656001029574    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1697   score: 7.0   memory length: 397525   epsilon: 0.41089852001029076    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1698   score: 3.0   memory length: 397737   epsilon: 0.4104787600102881    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1699   score: 5.0   memory length: 398059   epsilon: 0.40984120001028407    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1700   score: 6.0   memory length: 398434   epsilon: 0.40909870001027937    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1701   score: 3.0   memory length: 398644   epsilon: 0.40868290001027674    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1702   score: 8.0   memory length: 399056   epsilon: 0.4078671400102716    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1703   score: 9.0   memory length: 399520   epsilon: 0.40694842001026577    steps: 464    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1704   score: 5.0   memory length: 399823   epsilon: 0.40634848001026197    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1705   score: 2.0   memory length: 400041   epsilon: 0.40591684001025924    steps: 218    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
      "episode: 1706   score: 9.0   memory length: 400528   epsilon: 0.40495258001025314    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1707   score: 8.0   memory length: 400955   epsilon: 0.4041071200102478    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 1708   score: 4.0   memory length: 401234   epsilon: 0.4035547000102443    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 5.86\n",
      "episode: 1709   score: 6.0   memory length: 401628   epsilon: 0.40277458001023936    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
      "episode: 1710   score: 5.0   memory length: 401971   epsilon: 0.40209544001023506    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1711   score: 4.0   memory length: 402228   epsilon: 0.40158658001023184    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.9\n",
      "episode: 1712   score: 8.0   memory length: 402666   epsilon: 0.40071934001022635    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1713   score: 5.0   memory length: 402969   epsilon: 0.40011940001022256    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1714   score: 4.0   memory length: 403266   epsilon: 0.39953134001021884    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1715   score: 4.0   memory length: 403544   epsilon: 0.39898090001021536    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 1716   score: 5.0   memory length: 403868   epsilon: 0.3983393800102113    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1717   score: 7.0   memory length: 404256   epsilon: 0.39757114001020644    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1718   score: 5.0   memory length: 404599   epsilon: 0.39689200001020214    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1719   score: 5.0   memory length: 404891   epsilon: 0.3963138400101985    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1720   score: 9.0   memory length: 405362   epsilon: 0.3953812600101926    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
      "episode: 1721   score: 3.0   memory length: 405587   epsilon: 0.39493576001018976    steps: 225    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 1722   score: 2.0   memory length: 405768   epsilon: 0.3945773800101875    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
      "episode: 1723   score: 6.0   memory length: 406097   epsilon: 0.3939259600101834    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 1724   score: 9.0   memory length: 406584   epsilon: 0.3929617000101773    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 5.91\n",
      "episode: 1725   score: 5.0   memory length: 406905   epsilon: 0.39232612001017325    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 1726   score: 6.0   memory length: 407227   epsilon: 0.3916885600101692    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
      "episode: 1727   score: 6.0   memory length: 407626   epsilon: 0.3908985400101642    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
      "episode: 1728   score: 8.0   memory length: 408047   epsilon: 0.39006496001015895    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 1729   score: 3.0   memory length: 408273   epsilon: 0.3896174800101561    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 1730   score: 6.0   memory length: 408646   epsilon: 0.38887894001015144    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.0\n",
      "episode: 1731   score: 9.0   memory length: 409139   epsilon: 0.38790280001014527    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
      "episode: 1732   score: 8.0   memory length: 409548   epsilon: 0.38709298001014014    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 1733   score: 5.0   memory length: 409858   epsilon: 0.38647918001013626    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
      "episode: 1734   score: 3.0   memory length: 410088   epsilon: 0.3860237800101334    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 6.05\n",
      "episode: 1735   score: 6.0   memory length: 410437   epsilon: 0.385332760010129    steps: 349    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 1736   score: 7.0   memory length: 410791   epsilon: 0.38463184001012457    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 1737   score: 7.0   memory length: 411186   epsilon: 0.3838497400101196    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
      "episode: 1738   score: 3.0   memory length: 411432   epsilon: 0.38336266001011654    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 1739   score: 5.0   memory length: 411743   epsilon: 0.38274688001011264    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.03\n",
      "episode: 1740   score: 10.0   memory length: 412211   epsilon: 0.3818202400101068    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 6.1\n",
      "episode: 1741   score: 4.0   memory length: 412472   epsilon: 0.3813034600101035    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 1742   score: 14.0   memory length: 413000   epsilon: 0.3802580200100969    steps: 528    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 1743   score: 3.0   memory length: 413212   epsilon: 0.37983826001009424    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
      "episode: 1744   score: 11.0   memory length: 413625   epsilon: 0.37902052001008907    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 6.18\n",
      "episode: 1745   score: 5.0   memory length: 413952   epsilon: 0.37837306001008497    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.2\n",
      "episode: 1746   score: 7.0   memory length: 414311   epsilon: 0.3776622400100805    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
      "episode: 1747   score: 7.0   memory length: 414651   epsilon: 0.3769890400100762    steps: 340    lr: 6.400000000000001e-06     evaluation reward: 6.23\n",
      "episode: 1748   score: 8.0   memory length: 415101   epsilon: 0.3760980400100706    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 1749   score: 6.0   memory length: 415438   epsilon: 0.37543078001006636    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 1750   score: 11.0   memory length: 415957   epsilon: 0.37440316001005985    steps: 519    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 1751   score: 5.0   memory length: 416232   epsilon: 0.3738586600100564    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1752   score: 6.0   memory length: 416611   epsilon: 0.37310824001005166    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1753   score: 9.0   memory length: 417065   epsilon: 0.372209320010046    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1754   score: 6.0   memory length: 417412   epsilon: 0.3715222600100416    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
      "episode: 1755   score: 9.0   memory length: 417867   epsilon: 0.3706213600100359    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 1756   score: 6.0   memory length: 418238   epsilon: 0.3698867800100313    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
      "episode: 1757   score: 2.0   memory length: 418420   epsilon: 0.369526420010029    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1758   score: 3.0   memory length: 418633   epsilon: 0.36910468001002633    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1759   score: 10.0   memory length: 419184   epsilon: 0.36801370001001943    steps: 551    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1760   score: 4.0   memory length: 419461   epsilon: 0.36746524001001596    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1761   score: 7.0   memory length: 419842   epsilon: 0.3667108600100112    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1762   score: 10.0   memory length: 420302   epsilon: 0.3658000600100054    steps: 460    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 1763   score: 5.0   memory length: 420644   epsilon: 0.36512290001000114    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 1764   score: 12.0   memory length: 421248   epsilon: 0.3639269800099936    steps: 604    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 1765   score: 3.0   memory length: 421478   epsilon: 0.3634715800099907    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1766   score: 3.0   memory length: 421688   epsilon: 0.36305578000998806    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1767   score: 3.0   memory length: 421898   epsilon: 0.36263998000998543    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1768   score: 9.0   memory length: 422389   epsilon: 0.3616678000099793    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1769   score: 10.0   memory length: 422871   epsilon: 0.36071344000997324    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1770   score: 7.0   memory length: 423270   epsilon: 0.35992342000996824    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 1771   score: 8.0   memory length: 423695   epsilon: 0.3590819200099629    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1772   score: 3.0   memory length: 423907   epsilon: 0.35866216000996026    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 1773   score: 11.0   memory length: 424460   epsilon: 0.35756722000995333    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1774   score: 7.0   memory length: 424842   epsilon: 0.35681086000994855    steps: 382    lr: 6.400000000000001e-06     evaluation reward: 6.32\n",
      "episode: 1775   score: 8.0   memory length: 425312   epsilon: 0.35588026000994266    steps: 470    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1776   score: 9.0   memory length: 425767   epsilon: 0.35497936000993696    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 1777   score: 7.0   memory length: 426135   epsilon: 0.35425072000993235    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1778   score: 8.0   memory length: 426557   epsilon: 0.35341516000992707    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1779   score: 11.0   memory length: 426985   epsilon: 0.3525677200099217    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1780   score: 7.0   memory length: 427388   epsilon: 0.35176978000991665    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 1781   score: 8.0   memory length: 427828   epsilon: 0.35089858000991114    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1782   score: 5.0   memory length: 428116   epsilon: 0.35032834000990754    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1783   score: 6.0   memory length: 428457   epsilon: 0.34965316000990326    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 1784   score: 4.0   memory length: 428719   epsilon: 0.3491344000099    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 6.32\n",
      "episode: 1785   score: 6.0   memory length: 429072   epsilon: 0.34843546000989556    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 1786   score: 9.0   memory length: 429567   epsilon: 0.34745536000988936    steps: 495    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 1787   score: 5.0   memory length: 429893   epsilon: 0.3468098800098853    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 1788   score: 5.0   memory length: 430258   epsilon: 0.3460871800098807    steps: 365    lr: 6.400000000000001e-06     evaluation reward: 6.32\n",
      "episode: 1789   score: 4.0   memory length: 430542   epsilon: 0.34552486000987714    steps: 284    lr: 6.400000000000001e-06     evaluation reward: 6.31\n",
      "episode: 1790   score: 3.0   memory length: 430755   epsilon: 0.3451031200098745    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.27\n",
      "episode: 1791   score: 5.0   memory length: 431077   epsilon: 0.34446556000987044    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 6.26\n",
      "episode: 1792   score: 11.0   memory length: 431602   epsilon: 0.34342606000986386    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 6.31\n",
      "episode: 1793   score: 11.0   memory length: 432089   epsilon: 0.34246180000985776    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 1794   score: 9.0   memory length: 432577   epsilon: 0.34149556000985165    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1795   score: 6.0   memory length: 432911   epsilon: 0.34083424000984747    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 1796   score: 7.0   memory length: 433314   epsilon: 0.3400363000098424    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1797   score: 4.0   memory length: 433595   epsilon: 0.3394799200098389    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 1798   score: 6.0   memory length: 433924   epsilon: 0.3388285000098348    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 1799   score: 9.0   memory length: 434383   epsilon: 0.337919680009829    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 1800   score: 3.0   memory length: 434599   epsilon: 0.3374920000098263    steps: 216    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 1801   score: 5.0   memory length: 434922   epsilon: 0.3368524600098223    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
      "episode: 1802   score: 4.0   memory length: 435201   epsilon: 0.3363000400098188    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1803   score: 9.0   memory length: 435684   epsilon: 0.33534370000981273    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 1804   score: 9.0   memory length: 436155   epsilon: 0.33441112000980683    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
      "episode: 1805   score: 5.0   memory length: 436461   epsilon: 0.333805240009803    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
      "episode: 1806   score: 5.0   memory length: 436751   epsilon: 0.33323104000979936    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 1807   score: 13.0   memory length: 437360   epsilon: 0.33202522000979173    steps: 609    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
      "episode: 1808   score: 5.0   memory length: 437671   epsilon: 0.33140944000978784    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 1809   score: 8.0   memory length: 438123   epsilon: 0.3305144800097822    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 6.51\n",
      "episode: 1810   score: 12.0   memory length: 438570   epsilon: 0.3296294200097766    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
      "episode: 1811   score: 9.0   memory length: 438997   epsilon: 0.3287839600097712    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 1812   score: 14.0   memory length: 439509   epsilon: 0.3277702000097648    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 6.69\n",
      "episode: 1813   score: 5.0   memory length: 439798   epsilon: 0.3271979800097612    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 6.69\n",
      "episode: 1814   score: 5.0   memory length: 440112   epsilon: 0.32657626000975726    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 1815   score: 9.0   memory length: 440585   epsilon: 0.32563972000975133    steps: 473    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 1816   score: 8.0   memory length: 440998   epsilon: 0.32482198000974616    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 1817   score: 6.0   memory length: 441336   epsilon: 0.3241527400097419    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1818   score: 5.0   memory length: 441659   epsilon: 0.3235132000097379    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1819   score: 7.0   memory length: 442065   epsilon: 0.3227093200097328    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1820   score: 4.0   memory length: 442343   epsilon: 0.3221588800097293    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 1821   score: 2.0   memory length: 442527   epsilon: 0.321794560009727    steps: 184    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1822   score: 6.0   memory length: 442871   epsilon: 0.3211134400097227    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1823   score: 7.0   memory length: 443277   epsilon: 0.3203095600097176    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 1824   score: 8.0   memory length: 443723   epsilon: 0.319426480009712    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1825   score: 13.0   memory length: 444194   epsilon: 0.3184939000097061    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 6.85\n",
      "episode: 1826   score: 3.0   memory length: 444423   epsilon: 0.31804048000970325    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1827   score: 9.0   memory length: 444909   epsilon: 0.31707820000969716    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 6.85\n",
      "episode: 1828   score: 3.0   memory length: 445157   epsilon: 0.31658716000969406    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1829   score: 6.0   memory length: 445510   epsilon: 0.31588822000968964    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1830   score: 6.0   memory length: 445852   epsilon: 0.31521106000968535    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1831   score: 12.0   memory length: 446408   epsilon: 0.3141101800096784    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1832   score: 5.0   memory length: 446680   epsilon: 0.313571620009675    steps: 272    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1833   score: 6.0   memory length: 447038   epsilon: 0.3128627800096705    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
      "episode: 1834   score: 4.0   memory length: 447319   epsilon: 0.312306400009667    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 6.85\n",
      "episode: 1835   score: 7.0   memory length: 447684   epsilon: 0.3115837000096624    steps: 365    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1836   score: 4.0   memory length: 447966   epsilon: 0.31102534000965887    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1837   score: 8.0   memory length: 448393   epsilon: 0.3101798800096535    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
      "episode: 1838   score: 7.0   memory length: 448742   epsilon: 0.30948886000964915    steps: 349    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1839   score: 7.0   memory length: 449145   epsilon: 0.3086909200096441    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1840   score: 7.0   memory length: 449527   epsilon: 0.3079345600096393    steps: 382    lr: 6.400000000000001e-06     evaluation reward: 6.87\n",
      "episode: 1841   score: 6.0   memory length: 449897   epsilon: 0.3072019600096347    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1842   score: 4.0   memory length: 450178   epsilon: 0.30664558000963116    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1843   score: 6.0   memory length: 450537   epsilon: 0.30593476000962666    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1844   score: 11.0   memory length: 451114   epsilon: 0.30479230000961943    steps: 577    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1845   score: 4.0   memory length: 451392   epsilon: 0.30424186000961595    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1846   score: 15.0   memory length: 451852   epsilon: 0.3033310600096102    steps: 460    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1847   score: 8.0   memory length: 452271   epsilon: 0.30250144000960494    steps: 419    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1848   score: 4.0   memory length: 452514   epsilon: 0.3020203000096019    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1849   score: 3.0   memory length: 452727   epsilon: 0.3015985600095992    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1850   score: 8.0   memory length: 453147   epsilon: 0.30076696000959396    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1851   score: 5.0   memory length: 453456   epsilon: 0.3001551400095901    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1852   score: 5.0   memory length: 453781   epsilon: 0.299511640009586    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1853   score: 5.0   memory length: 454071   epsilon: 0.2989374400095824    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 1854   score: 3.0   memory length: 454302   epsilon: 0.2984800600095795    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1855   score: 10.0   memory length: 454794   epsilon: 0.29750590000957333    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1856   score: 5.0   memory length: 455101   epsilon: 0.2968980400095695    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1857   score: 7.0   memory length: 455503   epsilon: 0.29610208000956445    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1858   score: 6.0   memory length: 455873   epsilon: 0.2953694800095598    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1859   score: 5.0   memory length: 456169   epsilon: 0.2947834000095561    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 1860   score: 9.0   memory length: 456608   epsilon: 0.2939141800095506    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1861   score: 11.0   memory length: 457171   epsilon: 0.29279944000954355    steps: 563    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
      "episode: 1862   score: 8.0   memory length: 457591   epsilon: 0.2919678400095383    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1863   score: 4.0   memory length: 457865   epsilon: 0.29142532000953486    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1864   score: 7.0   memory length: 458270   epsilon: 0.2906234200095298    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1865   score: 4.0   memory length: 458544   epsilon: 0.29008090000952635    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1866   score: 3.0   memory length: 458773   epsilon: 0.2896274800095235    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1867   score: 5.0   memory length: 459076   epsilon: 0.2890275400095197    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1868   score: 6.0   memory length: 459449   epsilon: 0.288289000009515    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1869   score: 8.0   memory length: 459910   epsilon: 0.28737622000950924    steps: 461    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 1870   score: 3.0   memory length: 460143   epsilon: 0.2869148800095063    steps: 233    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 1871   score: 9.0   memory length: 460611   epsilon: 0.28598824000950046    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 1872   score: 8.0   memory length: 461040   epsilon: 0.2851388200094951    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1873   score: 7.0   memory length: 461449   epsilon: 0.28432900000948996    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1874   score: 7.0   memory length: 461857   epsilon: 0.28352116000948485    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1875   score: 10.0   memory length: 462313   epsilon: 0.28261828000947914    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 1876   score: 7.0   memory length: 462718   epsilon: 0.28181638000947407    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1877   score: 5.0   memory length: 463020   epsilon: 0.2812184200094703    steps: 302    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 1878   score: 7.0   memory length: 463438   epsilon: 0.28039078000946505    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 6.69\n",
      "episode: 1879   score: 8.0   memory length: 463836   epsilon: 0.27960274000946006    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1880   score: 8.0   memory length: 464271   epsilon: 0.2787414400094546    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 6.67\n",
      "episode: 1881   score: 3.0   memory length: 464518   epsilon: 0.2782523800094515    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 1882   score: 6.0   memory length: 464861   epsilon: 0.2775732400094472    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 1883   score: 3.0   memory length: 465107   epsilon: 0.27708616000944414    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 1884   score: 15.0   memory length: 465708   epsilon: 0.2758961800094366    steps: 601    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 1885   score: 13.0   memory length: 466202   epsilon: 0.2749180600094304    steps: 494    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 1886   score: 9.0   memory length: 466706   epsilon: 0.2739201400094241    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 1887   score: 15.0   memory length: 467256   epsilon: 0.2728311400094172    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1888   score: 7.0   memory length: 467639   epsilon: 0.2720728000094124    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1889   score: 3.0   memory length: 467852   epsilon: 0.27165106000940975    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1890   score: 5.0   memory length: 468177   epsilon: 0.2710075600094057    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
      "episode: 1891   score: 7.0   memory length: 468549   epsilon: 0.270271000009401    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
      "episode: 1892   score: 9.0   memory length: 468952   epsilon: 0.26947306000939597    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
      "episode: 1893   score: 6.0   memory length: 469281   epsilon: 0.26882164000939185    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1894   score: 4.0   memory length: 469559   epsilon: 0.26827120000938837    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1895   score: 6.0   memory length: 469874   epsilon: 0.2676475000093844    steps: 315    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1896   score: 8.0   memory length: 470298   epsilon: 0.2668079800093791    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1897   score: 6.0   memory length: 470650   epsilon: 0.2661110200093747    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
      "episode: 1898   score: 5.0   memory length: 470923   epsilon: 0.2655704800093713    steps: 273    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1899   score: 12.0   memory length: 471482   epsilon: 0.2644636600093643    steps: 559    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1900   score: 5.0   memory length: 471788   epsilon: 0.26385778000936044    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1901   score: 7.0   memory length: 472152   epsilon: 0.2631370600093559    steps: 364    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1902   score: 6.0   memory length: 472500   epsilon: 0.2624480200093515    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 6.92\n",
      "episode: 1903   score: 7.0   memory length: 472852   epsilon: 0.2617510600093471    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1904   score: 7.0   memory length: 473201   epsilon: 0.26106004000934274    steps: 349    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1905   score: 7.0   memory length: 473586   epsilon: 0.2602977400093379    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1906   score: 5.0   memory length: 473890   epsilon: 0.2596958200093341    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1907   score: 13.0   memory length: 474358   epsilon: 0.25876918000932825    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 6.9\n",
      "episode: 1908   score: 4.0   memory length: 474652   epsilon: 0.25818706000932456    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1909   score: 5.0   memory length: 474978   epsilon: 0.2575415800093205    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.86\n",
      "episode: 1910   score: 6.0   memory length: 475347   epsilon: 0.25681096000931586    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1911   score: 4.0   memory length: 475630   epsilon: 0.2562506200093123    steps: 283    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 1912   score: 7.0   memory length: 476031   epsilon: 0.2554566400093073    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
      "episode: 1913   score: 9.0   memory length: 476480   epsilon: 0.25456762000930167    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 1914   score: 8.0   memory length: 476876   epsilon: 0.2537835400092967    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 1915   score: 4.0   memory length: 477156   epsilon: 0.2532291400092932    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 1916   score: 4.0   memory length: 477430   epsilon: 0.25268662000928976    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1917   score: 6.0   memory length: 477793   epsilon: 0.2519678800092852    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 1918   score: 12.0   memory length: 478344   epsilon: 0.2508769000092783    steps: 551    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1919   score: 9.0   memory length: 478801   epsilon: 0.2499720400092726    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 1920   score: 5.0   memory length: 479108   epsilon: 0.24936418000926874    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 1921   score: 5.0   memory length: 479411   epsilon: 0.24876424000926495    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1922   score: 11.0   memory length: 479842   epsilon: 0.24791086000925955    steps: 431    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
      "episode: 1923   score: 7.0   memory length: 480242   epsilon: 0.24711886000925454    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 6.84\n",
      "episode: 1924   score: 6.0   memory length: 480603   epsilon: 0.24640408000925001    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1925   score: 4.0   memory length: 480901   epsilon: 0.24581404000924628    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 1926   score: 7.0   memory length: 481306   epsilon: 0.2450121400092412    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1927   score: 9.0   memory length: 481755   epsilon: 0.24412312000923558    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 1928   score: 5.0   memory length: 482077   epsilon: 0.24348556000923155    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 1929   score: 7.0   memory length: 482470   epsilon: 0.24270742000922663    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1930   score: 9.0   memory length: 482919   epsilon: 0.241818400009221    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1931   score: 10.0   memory length: 483438   epsilon: 0.2407907800092145    steps: 519    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1932   score: 4.0   memory length: 483680   epsilon: 0.24031162000921147    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 6.8\n",
      "episode: 1933   score: 9.0   memory length: 484143   epsilon: 0.23939488000920567    steps: 463    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1934   score: 6.0   memory length: 484516   epsilon: 0.238656340009201    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.85\n",
      "episode: 1935   score: 3.0   memory length: 484764   epsilon: 0.2381653000091979    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1936   score: 5.0   memory length: 485056   epsilon: 0.23758714000919423    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1937   score: 8.0   memory length: 485514   epsilon: 0.2366803000091885    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 1938   score: 14.0   memory length: 486070   epsilon: 0.23557942000918153    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1939   score: 6.0   memory length: 486423   epsilon: 0.2348804800091771    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.88\n",
      "episode: 1940   score: 8.0   memory length: 486882   epsilon: 0.23397166000917136    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1941   score: 4.0   memory length: 487181   epsilon: 0.2333796400091676    steps: 299    lr: 6.400000000000001e-06     evaluation reward: 6.87\n",
      "episode: 1942   score: 8.0   memory length: 487616   epsilon: 0.23251834000916216    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 6.91\n",
      "episode: 1943   score: 9.0   memory length: 488072   epsilon: 0.23161546000915645    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
      "episode: 1944   score: 10.0   memory length: 488442   epsilon: 0.2308828600091518    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
      "episode: 1945   score: 5.0   memory length: 488714   epsilon: 0.2303443000091484    steps: 272    lr: 6.400000000000001e-06     evaluation reward: 6.94\n",
      "episode: 1946   score: 4.0   memory length: 488957   epsilon: 0.22986316000914536    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 6.83\n",
      "episode: 1947   score: 6.0   memory length: 489329   epsilon: 0.2291266000091407    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 6.81\n",
      "episode: 1948   score: 12.0   memory length: 489808   epsilon: 0.2281781800091347    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 6.89\n",
      "episode: 1949   score: 7.0   memory length: 490196   epsilon: 0.22740994000912984    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 6.93\n",
      "episode: 1950   score: 14.0   memory length: 490789   epsilon: 0.2262358000091224    steps: 593    lr: 6.400000000000001e-06     evaluation reward: 6.99\n",
      "episode: 1951   score: 9.0   memory length: 491218   epsilon: 0.22538638000911704    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 7.03\n",
      "episode: 1952   score: 13.0   memory length: 491818   epsilon: 0.22419838000910952    steps: 600    lr: 6.400000000000001e-06     evaluation reward: 7.11\n",
      "episode: 1953   score: 11.0   memory length: 492351   epsilon: 0.22314304000910284    steps: 533    lr: 6.400000000000001e-06     evaluation reward: 7.17\n",
      "episode: 1954   score: 5.0   memory length: 492636   epsilon: 0.22257874000909927    steps: 285    lr: 6.400000000000001e-06     evaluation reward: 7.19\n",
      "episode: 1955   score: 9.0   memory length: 493105   epsilon: 0.2216501200090934    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 7.18\n",
      "episode: 1956   score: 7.0   memory length: 493494   epsilon: 0.22087990000908853    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 7.2\n",
      "episode: 1957   score: 9.0   memory length: 493931   epsilon: 0.22001464000908305    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 7.22\n",
      "episode: 1958   score: 4.0   memory length: 494208   epsilon: 0.21946618000907958    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 7.2\n",
      "episode: 1959   score: 6.0   memory length: 494583   epsilon: 0.21872368000907488    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 7.21\n",
      "episode: 1960   score: 8.0   memory length: 495041   epsilon: 0.21781684000906915    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 7.2\n",
      "episode: 1961   score: 4.0   memory length: 495319   epsilon: 0.21726640000906566    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 7.13\n",
      "episode: 1962   score: 11.0   memory length: 495856   epsilon: 0.21620314000905894    steps: 537    lr: 6.400000000000001e-06     evaluation reward: 7.16\n",
      "episode: 1963   score: 5.0   memory length: 496144   epsilon: 0.21563290000905533    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 7.17\n",
      "episode: 1964   score: 8.0   memory length: 496589   epsilon: 0.21475180000904975    steps: 445    lr: 6.400000000000001e-06     evaluation reward: 7.18\n",
      "episode: 1965   score: 7.0   memory length: 496992   epsilon: 0.2139538600090447    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 7.21\n",
      "episode: 1966   score: 4.0   memory length: 497269   epsilon: 0.21340540000904124    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 7.22\n",
      "episode: 1967   score: 8.0   memory length: 497685   epsilon: 0.21258172000903602    steps: 416    lr: 6.400000000000001e-06     evaluation reward: 7.25\n",
      "episode: 1968   score: 9.0   memory length: 497997   epsilon: 0.21196396000903212    steps: 312    lr: 6.400000000000001e-06     evaluation reward: 7.28\n",
      "episode: 1969   score: 15.0   memory length: 498593   epsilon: 0.21078388000902465    steps: 596    lr: 6.400000000000001e-06     evaluation reward: 7.35\n",
      "episode: 1970   score: 5.0   memory length: 498877   epsilon: 0.2102215600090211    steps: 284    lr: 6.400000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1971   score: 4.0   memory length: 499120   epsilon: 0.20974042000901805    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1972   score: 7.0   memory length: 499506   epsilon: 0.2089761400090132    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 7.31\n",
      "episode: 1973   score: 5.0   memory length: 499815   epsilon: 0.20836432000900934    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 7.29\n",
      "episode: 1974   score: 17.0   memory length: 500364   epsilon: 0.20727730000900246    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1975   score: 8.0   memory length: 500809   epsilon: 0.2063962000089969    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.37\n",
      "episode: 1976   score: 9.0   memory length: 501289   epsilon: 0.20544580000899088    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1977   score: 12.0   memory length: 501769   epsilon: 0.20449540000898486    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
      "episode: 1978   score: 5.0   memory length: 502058   epsilon: 0.20392318000898124    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1979   score: 8.0   memory length: 502491   epsilon: 0.20306584000897582    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1980   score: 4.0   memory length: 502750   epsilon: 0.20255302000897257    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 7.4\n",
      "episode: 1981   score: 11.0   memory length: 503170   epsilon: 0.2017214200089673    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 1982   score: 5.0   memory length: 503517   epsilon: 0.20103436000896296    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 1983   score: 7.0   memory length: 503906   epsilon: 0.2002641400089581    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
      "episode: 1984   score: 9.0   memory length: 504409   epsilon: 0.1992682000089518    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
      "episode: 1985   score: 6.0   memory length: 504752   epsilon: 0.1985890600089475    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1986   score: 9.0   memory length: 505181   epsilon: 0.19773964000894212    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
      "episode: 1987   score: 9.0   memory length: 505657   epsilon: 0.19679716000893616    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 7.32\n",
      "episode: 1988   score: 9.0   memory length: 505985   epsilon: 0.19614772000893205    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 7.34\n",
      "episode: 1989   score: 8.0   memory length: 506343   epsilon: 0.19543888000892756    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 1990   score: 7.0   memory length: 506718   epsilon: 0.19469638000892286    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 7.41\n",
      "episode: 1991   score: 10.0   memory length: 507231   epsilon: 0.19368064000891644    steps: 513    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 1992   score: 8.0   memory length: 507664   epsilon: 0.192823300008911    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 1993   score: 10.0   memory length: 508130   epsilon: 0.19190062000890518    steps: 466    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 1994   score: 9.0   memory length: 508606   epsilon: 0.1909581400088992    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 7.52\n",
      "episode: 1995   score: 5.0   memory length: 508909   epsilon: 0.19035820000889542    steps: 303    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
      "episode: 1996   score: 4.0   memory length: 509185   epsilon: 0.18981172000889196    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 1997   score: 9.0   memory length: 509635   epsilon: 0.18892072000888632    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
      "episode: 1998   score: 10.0   memory length: 510138   epsilon: 0.18792478000888002    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
      "episode: 1999   score: 10.0   memory length: 510650   epsilon: 0.1869110200088736    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 7.53\n",
      "episode: 2000   score: 7.0   memory length: 511059   epsilon: 0.18610120000886848    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
      "episode: 2001   score: 9.0   memory length: 511507   epsilon: 0.18521416000886287    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 7.57\n",
      "episode: 2002   score: 9.0   memory length: 511980   epsilon: 0.18427762000885695    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2003   score: 7.0   memory length: 512353   epsilon: 0.18353908000885227    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2004   score: 6.0   memory length: 512696   epsilon: 0.18285994000884798    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2005   score: 7.0   memory length: 513120   epsilon: 0.18202042000884266    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2006   score: 6.0   memory length: 513460   epsilon: 0.1813472200088384    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2007   score: 10.0   memory length: 513988   epsilon: 0.1803017800088318    steps: 528    lr: 2.560000000000001e-06     evaluation reward: 7.57\n",
      "episode: 2008   score: 8.0   memory length: 514443   epsilon: 0.1794008800088261    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
      "episode: 2009   score: 11.0   memory length: 515000   epsilon: 0.1782980200088191    steps: 557    lr: 2.560000000000001e-06     evaluation reward: 7.67\n",
      "episode: 2010   score: 11.0   memory length: 515553   epsilon: 0.17720308000881219    steps: 553    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
      "episode: 2011   score: 8.0   memory length: 515941   epsilon: 0.17643484000880733    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2012   score: 6.0   memory length: 516275   epsilon: 0.17577352000880314    steps: 334    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2013   score: 9.0   memory length: 516749   epsilon: 0.1748350000087972    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2014   score: 7.0   memory length: 517111   epsilon: 0.17411824000879267    steps: 362    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
      "episode: 2015   score: 9.0   memory length: 517553   epsilon: 0.17324308000878713    steps: 442    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2016   score: 5.0   memory length: 517875   epsilon: 0.1726055200087831    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2017   score: 6.0   memory length: 518225   epsilon: 0.1719125200087787    steps: 350    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2018   score: 4.0   memory length: 518500   epsilon: 0.17136802000877527    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
      "episode: 2019   score: 8.0   memory length: 518912   epsilon: 0.1705522600087701    steps: 412    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
      "episode: 2020   score: 7.0   memory length: 519270   epsilon: 0.16984342000876562    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
      "episode: 2021   score: 10.0   memory length: 519765   epsilon: 0.16886332000875942    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2022   score: 9.0   memory length: 520245   epsilon: 0.1679129200087534    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2023   score: 5.0   memory length: 520551   epsilon: 0.16730704000874957    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
      "episode: 2024   score: 5.0   memory length: 520880   epsilon: 0.16665562000874545    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
      "episode: 2025   score: 7.0   memory length: 521280   epsilon: 0.16586362000874044    steps: 400    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2026   score: 7.0   memory length: 521708   epsilon: 0.16501618000873508    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2027   score: 8.0   memory length: 522133   epsilon: 0.16417468000872976    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2028   score: 8.0   memory length: 522559   epsilon: 0.16333120000872442    steps: 426    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2029   score: 11.0   memory length: 523134   epsilon: 0.16219270000871722    steps: 575    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2030   score: 6.0   memory length: 523488   epsilon: 0.16149178000871278    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2031   score: 7.0   memory length: 523892   epsilon: 0.16069186000870772    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2032   score: 7.0   memory length: 524284   epsilon: 0.1599157000087028    steps: 392    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2033   score: 4.0   memory length: 524584   epsilon: 0.15932170000869905    steps: 300    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
      "episode: 2034   score: 7.0   memory length: 524952   epsilon: 0.15859306000869444    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2035   score: 9.0   memory length: 525446   epsilon: 0.15761494000868825    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2036   score: 7.0   memory length: 525816   epsilon: 0.15688234000868362    steps: 370    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2037   score: 7.0   memory length: 526238   epsilon: 0.15604678000867833    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2038   score: 9.0   memory length: 526653   epsilon: 0.15522508000867313    steps: 415    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2039   score: 7.0   memory length: 527038   epsilon: 0.1544627800086683    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2040   score: 10.0   memory length: 527609   epsilon: 0.15333220000866116    steps: 571    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2041   score: 9.0   memory length: 528103   epsilon: 0.15235408000865497    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2042   score: 8.0   memory length: 528518   epsilon: 0.15153238000864977    steps: 415    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2043   score: 4.0   memory length: 528795   epsilon: 0.1509839200086463    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2044   score: 8.0   memory length: 529214   epsilon: 0.15015430000864105    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2045   score: 3.0   memory length: 529461   epsilon: 0.14966524000863796    steps: 247    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2046   score: 11.0   memory length: 529985   epsilon: 0.1486277200086314    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2047   score: 3.0   memory length: 530232   epsilon: 0.1481386600086283    steps: 247    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2048   score: 16.0   memory length: 530852   epsilon: 0.14691106000862053    steps: 620    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2049   score: 6.0   memory length: 531168   epsilon: 0.14628538000861657    steps: 316    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2050   score: 10.0   memory length: 531544   epsilon: 0.14554090000861186    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2051   score: 9.0   memory length: 532034   epsilon: 0.14457070000860572    steps: 490    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2052   score: 5.0   memory length: 532341   epsilon: 0.14396284000860188    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
      "episode: 2053   score: 12.0   memory length: 532921   epsilon: 0.1428144400085946    steps: 580    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
      "episode: 2054   score: 4.0   memory length: 533215   epsilon: 0.14223232000859093    steps: 294    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
      "episode: 2055   score: 11.0   memory length: 533766   epsilon: 0.14114134000858403    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
      "episode: 2056   score: 12.0   memory length: 534244   epsilon: 0.14019490000857804    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2057   score: 11.0   memory length: 534766   epsilon: 0.1391613400085715    steps: 522    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2058   score: 4.0   memory length: 535039   epsilon: 0.13862080000856808    steps: 273    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2059   score: 4.0   memory length: 535302   epsilon: 0.13810006000856478    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2060   score: 7.0   memory length: 535704   epsilon: 0.13730410000855975    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2061   score: 7.0   memory length: 536082   epsilon: 0.136555660008555    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2062   score: 6.0   memory length: 536438   epsilon: 0.13585078000855055    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2063   score: 9.0   memory length: 536892   epsilon: 0.13495186000854487    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2064   score: 6.0   memory length: 537254   epsilon: 0.13423510000854033    steps: 362    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2065   score: 9.0   memory length: 537693   epsilon: 0.13336588000853483    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2066   score: 9.0   memory length: 538105   epsilon: 0.13255012000852967    steps: 412    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2067   score: 12.0   memory length: 538767   epsilon: 0.13123936000852138    steps: 662    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2068   score: 9.0   memory length: 539227   epsilon: 0.13032856000851561    steps: 460    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2069   score: 5.0   memory length: 539533   epsilon: 0.12972268000851178    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2070   score: 8.0   memory length: 539977   epsilon: 0.12884356000850622    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2071   score: 9.0   memory length: 540443   epsilon: 0.12792088000850038    steps: 466    lr: 2.560000000000001e-06     evaluation reward: 7.86\n",
      "episode: 2072   score: 7.0   memory length: 540828   epsilon: 0.12715858000849556    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 7.86\n",
      "episode: 2073   score: 6.0   memory length: 541203   epsilon: 0.12641608000849086    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
      "episode: 2074   score: 8.0   memory length: 541657   epsilon: 0.12551716000848517    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2075   score: 8.0   memory length: 542091   epsilon: 0.12465784000848214    steps: 434    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2076   score: 11.0   memory length: 542664   epsilon: 0.12352330000848291    steps: 573    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2077   score: 7.0   memory length: 543019   epsilon: 0.12282040000848339    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2078   score: 8.0   memory length: 543514   epsilon: 0.12184030000848406    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2079   score: 7.0   memory length: 543896   epsilon: 0.12108394000848458    steps: 382    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2080   score: 10.0   memory length: 544399   epsilon: 0.12008800000848525    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2081   score: 7.0   memory length: 544759   epsilon: 0.11937520000848574    steps: 360    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2082   score: 6.0   memory length: 545132   epsilon: 0.11863666000848624    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2083   score: 8.0   memory length: 545553   epsilon: 0.11780308000848681    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2084   score: 6.0   memory length: 545931   epsilon: 0.11705464000848732    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2085   score: 7.0   memory length: 546354   epsilon: 0.1162171000084879    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2086   score: 9.0   memory length: 546792   epsilon: 0.11534986000848849    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2087   score: 14.0   memory length: 547448   epsilon: 0.11405098000848937    steps: 656    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2088   score: 9.0   memory length: 547943   epsilon: 0.11307088000849004    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2089   score: 7.0   memory length: 548349   epsilon: 0.11226700000849059    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2090   score: 16.0   memory length: 548963   epsilon: 0.11105128000849142    steps: 614    lr: 2.560000000000001e-06     evaluation reward: 7.92\n",
      "episode: 2091   score: 5.0   memory length: 549269   epsilon: 0.11044540000849183    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
      "episode: 2092   score: 9.0   memory length: 549757   epsilon: 0.10947916000849249    steps: 488    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2093   score: 8.0   memory length: 550202   epsilon: 0.10859806000849309    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.86\n",
      "episode: 2094   score: 6.0   memory length: 550556   epsilon: 0.10789714000849357    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2095   score: 6.0   memory length: 550912   epsilon: 0.10719226000849405    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2096   score: 5.0   memory length: 551198   epsilon: 0.10662598000849444    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2097   score: 7.0   memory length: 551607   epsilon: 0.10581616000849499    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2098   score: 6.0   memory length: 551923   epsilon: 0.10519048000849542    steps: 316    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2099   score: 8.0   memory length: 552403   epsilon: 0.10424008000849606    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2100   score: 9.0   memory length: 552848   epsilon: 0.10335898000849666    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2101   score: 5.0   memory length: 553140   epsilon: 0.10278082000849706    steps: 292    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2102   score: 5.0   memory length: 553446   epsilon: 0.10217494000849747    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
      "episode: 2103   score: 13.0   memory length: 554013   epsilon: 0.10105228000849824    steps: 567    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2104   score: 8.0   memory length: 554464   epsilon: 0.10015930000849885    steps: 451    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2105   score: 10.0   memory length: 554945   epsilon: 0.0992069200084995    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2106   score: 7.0   memory length: 555316   epsilon: 0.0984723400085    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2107   score: 5.0   memory length: 555622   epsilon: 0.09786646000850041    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2108   score: 5.0   memory length: 555932   epsilon: 0.09725266000850083    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 7.75\n",
      "episode: 2109   score: 3.0   memory length: 556145   epsilon: 0.09683092000850112    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.67\n",
      "episode: 2110   score: 14.0   memory length: 556760   epsilon: 0.09561322000850195    steps: 615    lr: 2.560000000000001e-06     evaluation reward: 7.7\n",
      "episode: 2111   score: 11.0   memory length: 557270   epsilon: 0.09460342000850264    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 7.73\n",
      "episode: 2112   score: 11.0   memory length: 557817   epsilon: 0.09352036000850338    steps: 547    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2113   score: 12.0   memory length: 558301   epsilon: 0.09256204000850403    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2114   score: 8.0   memory length: 558757   epsilon: 0.09165916000850464    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2115   score: 8.0   memory length: 559175   epsilon: 0.09083152000850521    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2116   score: 10.0   memory length: 559717   epsilon: 0.08975836000850594    steps: 542    lr: 2.560000000000001e-06     evaluation reward: 7.86\n",
      "episode: 2117   score: 4.0   memory length: 559993   epsilon: 0.08921188000850631    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2118   score: 7.0   memory length: 560361   epsilon: 0.08848324000850681    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
      "episode: 2119   score: 6.0   memory length: 560677   epsilon: 0.08785756000850724    steps: 316    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2120   score: 8.0   memory length: 561107   epsilon: 0.08700616000850782    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 7.86\n",
      "episode: 2121   score: 7.0   memory length: 561493   epsilon: 0.08624188000850834    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2122   score: 4.0   memory length: 561770   epsilon: 0.08569342000850871    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2123   score: 6.0   memory length: 562144   epsilon: 0.08495290000850922    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2124   score: 8.0   memory length: 562577   epsilon: 0.0840955600085098    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2125   score: 3.0   memory length: 562826   epsilon: 0.08360254000851014    steps: 249    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2126   score: 5.0   memory length: 563131   epsilon: 0.08299864000851055    steps: 305    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2127   score: 11.0   memory length: 563703   epsilon: 0.08186608000851132    steps: 572    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2128   score: 11.0   memory length: 564220   epsilon: 0.08084242000851202    steps: 517    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2129   score: 3.0   memory length: 564447   epsilon: 0.08039296000851233    steps: 227    lr: 2.560000000000001e-06     evaluation reward: 7.74\n",
      "episode: 2130   score: 8.0   memory length: 564832   epsilon: 0.07963066000851285    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2131   score: 8.0   memory length: 565276   epsilon: 0.07875154000851345    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2132   score: 7.0   memory length: 565671   epsilon: 0.07796944000851398    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 7.77\n",
      "episode: 2133   score: 8.0   memory length: 566121   epsilon: 0.07707844000851459    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2134   score: 11.0   memory length: 566669   epsilon: 0.07599340000851533    steps: 548    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2135   score: 8.0   memory length: 567091   epsilon: 0.0751578400085159    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2136   score: 4.0   memory length: 567352   epsilon: 0.07464106000851625    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2137   score: 8.0   memory length: 567788   epsilon: 0.07377778000851684    steps: 436    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2138   score: 14.0   memory length: 568351   epsilon: 0.0726630400085176    steps: 563    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
      "episode: 2139   score: 11.0   memory length: 568903   epsilon: 0.07157008000851835    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
      "episode: 2140   score: 7.0   memory length: 569289   epsilon: 0.07080580000851887    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2141   score: 5.0   memory length: 569579   epsilon: 0.07023160000851926    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2142   score: 11.0   memory length: 570092   epsilon: 0.06921586000851995    steps: 513    lr: 2.560000000000001e-06     evaluation reward: 7.87\n",
      "episode: 2143   score: 10.0   memory length: 570522   epsilon: 0.06836446000852053    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 7.93\n",
      "episode: 2144   score: 10.0   memory length: 570996   epsilon: 0.06742594000852117    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 7.95\n",
      "episode: 2145   score: 14.0   memory length: 571543   epsilon: 0.06634288000852191    steps: 547    lr: 2.560000000000001e-06     evaluation reward: 8.06\n",
      "episode: 2146   score: 12.0   memory length: 571981   epsilon: 0.0654756400085225    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 8.07\n",
      "episode: 2147   score: 9.0   memory length: 572455   epsilon: 0.06453712000852314    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 8.13\n",
      "episode: 2148   score: 3.0   memory length: 572705   epsilon: 0.06404212000852348    steps: 250    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2149   score: 6.0   memory length: 573060   epsilon: 0.06333922000852396    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2150   score: 10.0   memory length: 573537   epsilon: 0.062394760008524605    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2151   score: 9.0   memory length: 574025   epsilon: 0.061428520008525264    steps: 488    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2152   score: 11.0   memory length: 574535   epsilon: 0.06041872000852595    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 8.06\n",
      "episode: 2153   score: 8.0   memory length: 574965   epsilon: 0.059567320008526534    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 8.02\n",
      "episode: 2154   score: 5.0   memory length: 575256   epsilon: 0.05899114000852693    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 8.03\n",
      "episode: 2155   score: 9.0   memory length: 575755   epsilon: 0.0580031200085276    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 8.01\n",
      "episode: 2156   score: 6.0   memory length: 576107   epsilon: 0.057306160008528076    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 7.95\n",
      "episode: 2157   score: 5.0   memory length: 576453   epsilon: 0.05662108000852854    steps: 346    lr: 2.560000000000001e-06     evaluation reward: 7.89\n",
      "episode: 2158   score: 9.0   memory length: 576877   epsilon: 0.055781560008529116    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 7.94\n",
      "episode: 2159   score: 7.0   memory length: 577268   epsilon: 0.055007380008529644    steps: 391    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2160   score: 9.0   memory length: 577741   epsilon: 0.05407084000853028    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2161   score: 9.0   memory length: 578188   epsilon: 0.053185780008530886    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 8.01\n",
      "episode: 2162   score: 9.0   memory length: 578640   epsilon: 0.0522908200085315    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 8.04\n",
      "episode: 2163   score: 9.0   memory length: 579132   epsilon: 0.05131666000853216    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 8.04\n",
      "episode: 2164   score: 8.0   memory length: 579576   epsilon: 0.05043754000853276    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 8.06\n",
      "episode: 2165   score: 9.0   memory length: 580070   epsilon: 0.04945942000853343    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 8.06\n",
      "episode: 2166   score: 15.0   memory length: 580586   epsilon: 0.048437740008534125    steps: 516    lr: 2.560000000000001e-06     evaluation reward: 8.12\n",
      "episode: 2167   score: 11.0   memory length: 581081   epsilon: 0.04745764000853479    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 8.11\n",
      "episode: 2168   score: 12.0   memory length: 581521   epsilon: 0.04658644000853539    steps: 440    lr: 2.560000000000001e-06     evaluation reward: 8.14\n",
      "episode: 2169   score: 9.0   memory length: 581995   epsilon: 0.04564792000853603    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 8.18\n",
      "episode: 2170   score: 10.0   memory length: 582517   epsilon: 0.04461436000853673    steps: 522    lr: 2.560000000000001e-06     evaluation reward: 8.2\n",
      "episode: 2171   score: 9.0   memory length: 582969   epsilon: 0.04371940000853734    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 8.2\n",
      "episode: 2172   score: 6.0   memory length: 583326   epsilon: 0.043012540008537825    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 8.19\n",
      "episode: 2173   score: 14.0   memory length: 583956   epsilon: 0.041765140008538676    steps: 630    lr: 2.560000000000001e-06     evaluation reward: 8.27\n",
      "episode: 2174   score: 8.0   memory length: 584373   epsilon: 0.04093948000853924    steps: 417    lr: 2.560000000000001e-06     evaluation reward: 8.27\n",
      "episode: 2175   score: 15.0   memory length: 584964   epsilon: 0.03976930000854004    steps: 591    lr: 2.560000000000001e-06     evaluation reward: 8.34\n",
      "episode: 2176   score: 6.0   memory length: 585324   epsilon: 0.03905650000854052    steps: 360    lr: 2.560000000000001e-06     evaluation reward: 8.29\n",
      "episode: 2177   score: 11.0   memory length: 585915   epsilon: 0.03788632000854132    steps: 591    lr: 2.560000000000001e-06     evaluation reward: 8.33\n",
      "episode: 2178   score: 9.0   memory length: 586406   epsilon: 0.036914140008541985    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 8.34\n",
      "episode: 2179   score: 6.0   memory length: 586785   epsilon: 0.036163720008542496    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 8.33\n",
      "episode: 2180   score: 7.0   memory length: 587195   epsilon: 0.03535192000854305    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 8.3\n",
      "episode: 2181   score: 13.0   memory length: 587767   epsilon: 0.03421936000854382    steps: 572    lr: 2.560000000000001e-06     evaluation reward: 8.36\n",
      "episode: 2182   score: 9.0   memory length: 588215   epsilon: 0.03333232000854443    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 8.39\n",
      "episode: 2183   score: 9.0   memory length: 588708   epsilon: 0.03235618000854509    steps: 493    lr: 2.560000000000001e-06     evaluation reward: 8.4\n",
      "episode: 2184   score: 7.0   memory length: 589077   epsilon: 0.03162556000854559    steps: 369    lr: 2.560000000000001e-06     evaluation reward: 8.41\n",
      "episode: 2185   score: 4.0   memory length: 589358   epsilon: 0.03106918000854597    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 8.38\n",
      "episode: 2186   score: 7.0   memory length: 589726   epsilon: 0.030340540008546468    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 8.36\n",
      "episode: 2187   score: 14.0   memory length: 590310   epsilon: 0.029184220008547257    steps: 584    lr: 2.560000000000001e-06     evaluation reward: 8.36\n",
      "episode: 2188   score: 7.0   memory length: 590693   epsilon: 0.028425880008547774    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 8.34\n",
      "episode: 2189   score: 8.0   memory length: 591096   epsilon: 0.02762794000854832    steps: 403    lr: 2.560000000000001e-06     evaluation reward: 8.35\n",
      "episode: 2190   score: 10.0   memory length: 591611   epsilon: 0.026608240008549014    steps: 515    lr: 2.560000000000001e-06     evaluation reward: 8.29\n",
      "episode: 2191   score: 6.0   memory length: 591935   epsilon: 0.02596672000854945    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 8.3\n",
      "episode: 2192   score: 12.0   memory length: 592468   epsilon: 0.02491138000855017    steps: 533    lr: 2.560000000000001e-06     evaluation reward: 8.33\n",
      "episode: 2193   score: 8.0   memory length: 592854   epsilon: 0.024147100008550693    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 8.33\n",
      "episode: 2194   score: 5.0   memory length: 593161   epsilon: 0.023539240008551107    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 8.32\n",
      "episode: 2195   score: 11.0   memory length: 593720   epsilon: 0.022432420008551862    steps: 559    lr: 2.560000000000001e-06     evaluation reward: 8.37\n",
      "episode: 2196   score: 5.0   memory length: 594042   epsilon: 0.021794860008552297    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 8.37\n",
      "episode: 2197   score: 8.0   memory length: 594498   epsilon: 0.020891980008552913    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 8.38\n",
      "episode: 2198   score: 11.0   memory length: 595009   epsilon: 0.019880200008553603    steps: 511    lr: 2.560000000000001e-06     evaluation reward: 8.43\n",
      "episode: 2199   score: 3.0   memory length: 595259   epsilon: 0.01938520000855394    steps: 250    lr: 2.560000000000001e-06     evaluation reward: 8.38\n",
      "episode: 2200   score: 5.0   memory length: 595567   epsilon: 0.018775360008554356    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 8.34\n",
      "episode: 2201   score: 6.0   memory length: 595924   epsilon: 0.01806850000855484    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 8.35\n",
      "episode: 2202   score: 9.0   memory length: 596370   epsilon: 0.01718542000855544    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 8.39\n",
      "episode: 2203   score: 10.0   memory length: 596859   epsilon: 0.0162172000085561    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 8.36\n",
      "episode: 2204   score: 9.0   memory length: 597296   epsilon: 0.015351940008556452    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 8.37\n",
      "episode: 2205   score: 10.0   memory length: 597785   epsilon: 0.014383720008556264    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 8.37\n",
      "episode: 2206   score: 8.0   memory length: 598208   epsilon: 0.013546180008556102    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 8.38\n",
      "episode: 2207   score: 11.0   memory length: 598764   epsilon: 0.012445300008555888    steps: 556    lr: 2.560000000000001e-06     evaluation reward: 8.44\n",
      "episode: 2208   score: 8.0   memory length: 599206   epsilon: 0.011570140008555718    steps: 442    lr: 2.560000000000001e-06     evaluation reward: 8.47\n",
      "episode: 2209   score: 10.0   memory length: 599737   epsilon: 0.010518760008555514    steps: 531    lr: 2.560000000000001e-06     evaluation reward: 8.54\n",
      "episode: 2210   score: 8.0   memory length: 600158   epsilon: 0.009998020008555413    steps: 421    lr: 1.0240000000000005e-06     evaluation reward: 8.48\n",
      "episode: 2211   score: 12.0   memory length: 600753   epsilon: 0.009998020008555413    steps: 595    lr: 1.0240000000000005e-06     evaluation reward: 8.49\n",
      "episode: 2212   score: 9.0   memory length: 601234   epsilon: 0.009998020008555413    steps: 481    lr: 1.0240000000000005e-06     evaluation reward: 8.47\n",
      "episode: 2213   score: 8.0   memory length: 601706   epsilon: 0.009998020008555413    steps: 472    lr: 1.0240000000000005e-06     evaluation reward: 8.43\n",
      "episode: 2214   score: 8.0   memory length: 602116   epsilon: 0.009998020008555413    steps: 410    lr: 1.0240000000000005e-06     evaluation reward: 8.43\n",
      "episode: 2215   score: 7.0   memory length: 602522   epsilon: 0.009998020008555413    steps: 406    lr: 1.0240000000000005e-06     evaluation reward: 8.42\n",
      "episode: 2216   score: 7.0   memory length: 602950   epsilon: 0.009998020008555413    steps: 428    lr: 1.0240000000000005e-06     evaluation reward: 8.39\n",
      "episode: 2217   score: 11.0   memory length: 603488   epsilon: 0.009998020008555413    steps: 538    lr: 1.0240000000000005e-06     evaluation reward: 8.46\n",
      "episode: 2218   score: 6.0   memory length: 603863   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 8.45\n",
      "episode: 2219   score: 6.0   memory length: 604238   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 8.45\n",
      "episode: 2220   score: 8.0   memory length: 604673   epsilon: 0.009998020008555413    steps: 435    lr: 1.0240000000000005e-06     evaluation reward: 8.45\n",
      "episode: 2221   score: 9.0   memory length: 605166   epsilon: 0.009998020008555413    steps: 493    lr: 1.0240000000000005e-06     evaluation reward: 8.47\n",
      "episode: 2222   score: 6.0   memory length: 605503   epsilon: 0.009998020008555413    steps: 337    lr: 1.0240000000000005e-06     evaluation reward: 8.49\n",
      "episode: 2223   score: 9.0   memory length: 606005   epsilon: 0.009998020008555413    steps: 502    lr: 1.0240000000000005e-06     evaluation reward: 8.52\n",
      "episode: 2224   score: 5.0   memory length: 606335   epsilon: 0.009998020008555413    steps: 330    lr: 1.0240000000000005e-06     evaluation reward: 8.49\n",
      "episode: 2225   score: 15.0   memory length: 606952   epsilon: 0.009998020008555413    steps: 617    lr: 1.0240000000000005e-06     evaluation reward: 8.61\n",
      "episode: 2226   score: 8.0   memory length: 607371   epsilon: 0.009998020008555413    steps: 419    lr: 1.0240000000000005e-06     evaluation reward: 8.64\n",
      "episode: 2227   score: 10.0   memory length: 607806   epsilon: 0.009998020008555413    steps: 435    lr: 1.0240000000000005e-06     evaluation reward: 8.63\n",
      "episode: 2228   score: 8.0   memory length: 608161   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 8.6\n",
      "episode: 2229   score: 7.0   memory length: 608544   epsilon: 0.009998020008555413    steps: 383    lr: 1.0240000000000005e-06     evaluation reward: 8.64\n",
      "episode: 2230   score: 6.0   memory length: 608906   epsilon: 0.009998020008555413    steps: 362    lr: 1.0240000000000005e-06     evaluation reward: 8.62\n",
      "episode: 2231   score: 13.0   memory length: 609567   epsilon: 0.009998020008555413    steps: 661    lr: 1.0240000000000005e-06     evaluation reward: 8.67\n",
      "episode: 2232   score: 10.0   memory length: 610100   epsilon: 0.009998020008555413    steps: 533    lr: 1.0240000000000005e-06     evaluation reward: 8.7\n",
      "episode: 2233   score: 14.0   memory length: 610726   epsilon: 0.009998020008555413    steps: 626    lr: 1.0240000000000005e-06     evaluation reward: 8.76\n",
      "episode: 2234   score: 9.0   memory length: 611218   epsilon: 0.009998020008555413    steps: 492    lr: 1.0240000000000005e-06     evaluation reward: 8.74\n",
      "episode: 2235   score: 7.0   memory length: 611603   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 8.73\n",
      "episode: 2236   score: 9.0   memory length: 612027   epsilon: 0.009998020008555413    steps: 424    lr: 1.0240000000000005e-06     evaluation reward: 8.78\n",
      "episode: 2237   score: 6.0   memory length: 612381   epsilon: 0.009998020008555413    steps: 354    lr: 1.0240000000000005e-06     evaluation reward: 8.76\n",
      "episode: 2238   score: 8.0   memory length: 612758   epsilon: 0.009998020008555413    steps: 377    lr: 1.0240000000000005e-06     evaluation reward: 8.7\n",
      "episode: 2239   score: 8.0   memory length: 613135   epsilon: 0.009998020008555413    steps: 377    lr: 1.0240000000000005e-06     evaluation reward: 8.67\n",
      "episode: 2240   score: 11.0   memory length: 613669   epsilon: 0.009998020008555413    steps: 534    lr: 1.0240000000000005e-06     evaluation reward: 8.71\n",
      "episode: 2241   score: 7.0   memory length: 614018   epsilon: 0.009998020008555413    steps: 349    lr: 1.0240000000000005e-06     evaluation reward: 8.73\n",
      "episode: 2242   score: 9.0   memory length: 614484   epsilon: 0.009998020008555413    steps: 466    lr: 1.0240000000000005e-06     evaluation reward: 8.71\n",
      "episode: 2243   score: 9.0   memory length: 614967   epsilon: 0.009998020008555413    steps: 483    lr: 1.0240000000000005e-06     evaluation reward: 8.7\n",
      "episode: 2244   score: 9.0   memory length: 615415   epsilon: 0.009998020008555413    steps: 448    lr: 1.0240000000000005e-06     evaluation reward: 8.69\n",
      "episode: 2245   score: 10.0   memory length: 615937   epsilon: 0.009998020008555413    steps: 522    lr: 1.0240000000000005e-06     evaluation reward: 8.65\n",
      "episode: 2246   score: 10.0   memory length: 616473   epsilon: 0.009998020008555413    steps: 536    lr: 1.0240000000000005e-06     evaluation reward: 8.63\n",
      "episode: 2247   score: 13.0   memory length: 617084   epsilon: 0.009998020008555413    steps: 611    lr: 1.0240000000000005e-06     evaluation reward: 8.67\n",
      "episode: 2248   score: 10.0   memory length: 617560   epsilon: 0.009998020008555413    steps: 476    lr: 1.0240000000000005e-06     evaluation reward: 8.74\n",
      "episode: 2249   score: 13.0   memory length: 618159   epsilon: 0.009998020008555413    steps: 599    lr: 1.0240000000000005e-06     evaluation reward: 8.81\n",
      "episode: 2250   score: 10.0   memory length: 618689   epsilon: 0.009998020008555413    steps: 530    lr: 1.0240000000000005e-06     evaluation reward: 8.81\n",
      "episode: 2251   score: 11.0   memory length: 619221   epsilon: 0.009998020008555413    steps: 532    lr: 1.0240000000000005e-06     evaluation reward: 8.83\n",
      "episode: 2252   score: 8.0   memory length: 619628   epsilon: 0.009998020008555413    steps: 407    lr: 1.0240000000000005e-06     evaluation reward: 8.8\n",
      "episode: 2253   score: 6.0   memory length: 619939   epsilon: 0.009998020008555413    steps: 311    lr: 1.0240000000000005e-06     evaluation reward: 8.78\n",
      "episode: 2254   score: 13.0   memory length: 620420   epsilon: 0.009998020008555413    steps: 481    lr: 1.0240000000000005e-06     evaluation reward: 8.86\n",
      "episode: 2255   score: 11.0   memory length: 620966   epsilon: 0.009998020008555413    steps: 546    lr: 1.0240000000000005e-06     evaluation reward: 8.88\n",
      "episode: 2256   score: 6.0   memory length: 621305   epsilon: 0.009998020008555413    steps: 339    lr: 1.0240000000000005e-06     evaluation reward: 8.88\n",
      "episode: 2257   score: 11.0   memory length: 621833   epsilon: 0.009998020008555413    steps: 528    lr: 1.0240000000000005e-06     evaluation reward: 8.94\n",
      "episode: 2258   score: 4.0   memory length: 622110   epsilon: 0.009998020008555413    steps: 277    lr: 1.0240000000000005e-06     evaluation reward: 8.89\n",
      "episode: 2259   score: 8.0   memory length: 622551   epsilon: 0.009998020008555413    steps: 441    lr: 1.0240000000000005e-06     evaluation reward: 8.9\n",
      "episode: 2260   score: 8.0   memory length: 622993   epsilon: 0.009998020008555413    steps: 442    lr: 1.0240000000000005e-06     evaluation reward: 8.89\n",
      "episode: 2261   score: 5.0   memory length: 623319   epsilon: 0.009998020008555413    steps: 326    lr: 1.0240000000000005e-06     evaluation reward: 8.85\n",
      "episode: 2262   score: 6.0   memory length: 623683   epsilon: 0.009998020008555413    steps: 364    lr: 1.0240000000000005e-06     evaluation reward: 8.82\n",
      "episode: 2263   score: 6.0   memory length: 624034   epsilon: 0.009998020008555413    steps: 351    lr: 1.0240000000000005e-06     evaluation reward: 8.79\n",
      "episode: 2264   score: 11.0   memory length: 624542   epsilon: 0.009998020008555413    steps: 508    lr: 1.0240000000000005e-06     evaluation reward: 8.82\n",
      "episode: 2265   score: 8.0   memory length: 625006   epsilon: 0.009998020008555413    steps: 464    lr: 1.0240000000000005e-06     evaluation reward: 8.81\n",
      "episode: 2266   score: 13.0   memory length: 625466   epsilon: 0.009998020008555413    steps: 460    lr: 1.0240000000000005e-06     evaluation reward: 8.79\n",
      "episode: 2267   score: 7.0   memory length: 625872   epsilon: 0.009998020008555413    steps: 406    lr: 1.0240000000000005e-06     evaluation reward: 8.75\n",
      "episode: 2268   score: 9.0   memory length: 626321   epsilon: 0.009998020008555413    steps: 449    lr: 1.0240000000000005e-06     evaluation reward: 8.72\n",
      "episode: 2269   score: 15.0   memory length: 626742   epsilon: 0.009998020008555413    steps: 421    lr: 1.0240000000000005e-06     evaluation reward: 8.78\n",
      "episode: 2270   score: 9.0   memory length: 627210   epsilon: 0.009998020008555413    steps: 468    lr: 1.0240000000000005e-06     evaluation reward: 8.77\n",
      "episode: 2271   score: 12.0   memory length: 627774   epsilon: 0.009998020008555413    steps: 564    lr: 1.0240000000000005e-06     evaluation reward: 8.8\n",
      "episode: 2272   score: 11.0   memory length: 628199   epsilon: 0.009998020008555413    steps: 425    lr: 1.0240000000000005e-06     evaluation reward: 8.85\n",
      "episode: 2273   score: 10.0   memory length: 628703   epsilon: 0.009998020008555413    steps: 504    lr: 1.0240000000000005e-06     evaluation reward: 8.81\n",
      "episode: 2274   score: 5.0   memory length: 629032   epsilon: 0.009998020008555413    steps: 329    lr: 1.0240000000000005e-06     evaluation reward: 8.78\n",
      "episode: 2275   score: 8.0   memory length: 629509   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 8.71\n",
      "episode: 2276   score: 6.0   memory length: 629883   epsilon: 0.009998020008555413    steps: 374    lr: 1.0240000000000005e-06     evaluation reward: 8.71\n",
      "episode: 2277   score: 8.0   memory length: 630348   epsilon: 0.009998020008555413    steps: 465    lr: 1.0240000000000005e-06     evaluation reward: 8.68\n",
      "episode: 2278   score: 10.0   memory length: 630827   epsilon: 0.009998020008555413    steps: 479    lr: 1.0240000000000005e-06     evaluation reward: 8.69\n",
      "episode: 2279   score: 10.0   memory length: 631310   epsilon: 0.009998020008555413    steps: 483    lr: 1.0240000000000005e-06     evaluation reward: 8.73\n",
      "episode: 2280   score: 8.0   memory length: 631761   epsilon: 0.009998020008555413    steps: 451    lr: 1.0240000000000005e-06     evaluation reward: 8.74\n",
      "episode: 2281   score: 13.0   memory length: 632393   epsilon: 0.009998020008555413    steps: 632    lr: 1.0240000000000005e-06     evaluation reward: 8.74\n",
      "episode: 2282   score: 8.0   memory length: 632804   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 8.73\n",
      "episode: 2283   score: 8.0   memory length: 633218   epsilon: 0.009998020008555413    steps: 414    lr: 1.0240000000000005e-06     evaluation reward: 8.72\n",
      "episode: 2284   score: 10.0   memory length: 633718   epsilon: 0.009998020008555413    steps: 500    lr: 1.0240000000000005e-06     evaluation reward: 8.75\n",
      "episode: 2285   score: 7.0   memory length: 634124   epsilon: 0.009998020008555413    steps: 406    lr: 1.0240000000000005e-06     evaluation reward: 8.78\n",
      "episode: 2286   score: 11.0   memory length: 634681   epsilon: 0.009998020008555413    steps: 557    lr: 1.0240000000000005e-06     evaluation reward: 8.82\n",
      "episode: 2287   score: 9.0   memory length: 635175   epsilon: 0.009998020008555413    steps: 494    lr: 1.0240000000000005e-06     evaluation reward: 8.77\n",
      "episode: 2288   score: 7.0   memory length: 635568   epsilon: 0.009998020008555413    steps: 393    lr: 1.0240000000000005e-06     evaluation reward: 8.77\n",
      "episode: 2289   score: 6.0   memory length: 635948   epsilon: 0.009998020008555413    steps: 380    lr: 1.0240000000000005e-06     evaluation reward: 8.75\n",
      "episode: 2290   score: 7.0   memory length: 636346   epsilon: 0.009998020008555413    steps: 398    lr: 1.0240000000000005e-06     evaluation reward: 8.72\n",
      "episode: 2291   score: 6.0   memory length: 636675   epsilon: 0.009998020008555413    steps: 329    lr: 1.0240000000000005e-06     evaluation reward: 8.72\n",
      "episode: 2292   score: 7.0   memory length: 637059   epsilon: 0.009998020008555413    steps: 384    lr: 1.0240000000000005e-06     evaluation reward: 8.67\n",
      "episode: 2293   score: 10.0   memory length: 637603   epsilon: 0.009998020008555413    steps: 544    lr: 1.0240000000000005e-06     evaluation reward: 8.69\n",
      "episode: 2294   score: 7.0   memory length: 637972   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 8.71\n",
      "episode: 2295   score: 9.0   memory length: 638441   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 8.69\n",
      "episode: 2296   score: 16.0   memory length: 639108   epsilon: 0.009998020008555413    steps: 667    lr: 1.0240000000000005e-06     evaluation reward: 8.8\n",
      "episode: 2297   score: 16.0   memory length: 639718   epsilon: 0.009998020008555413    steps: 610    lr: 1.0240000000000005e-06     evaluation reward: 8.88\n",
      "episode: 2298   score: 9.0   memory length: 640196   epsilon: 0.009998020008555413    steps: 478    lr: 1.0240000000000005e-06     evaluation reward: 8.86\n",
      "episode: 2299   score: 6.0   memory length: 640552   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 8.89\n",
      "episode: 2300   score: 11.0   memory length: 641122   epsilon: 0.009998020008555413    steps: 570    lr: 1.0240000000000005e-06     evaluation reward: 8.95\n",
      "episode: 2301   score: 15.0   memory length: 641706   epsilon: 0.009998020008555413    steps: 584    lr: 1.0240000000000005e-06     evaluation reward: 9.04\n",
      "episode: 2302   score: 7.0   memory length: 642111   epsilon: 0.009998020008555413    steps: 405    lr: 1.0240000000000005e-06     evaluation reward: 9.02\n",
      "episode: 2303   score: 5.0   memory length: 642422   epsilon: 0.009998020008555413    steps: 311    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2304   score: 12.0   memory length: 642918   epsilon: 0.009998020008555413    steps: 496    lr: 1.0240000000000005e-06     evaluation reward: 9.0\n",
      "episode: 2305   score: 7.0   memory length: 643301   epsilon: 0.009998020008555413    steps: 383    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2306   score: 11.0   memory length: 643869   epsilon: 0.009998020008555413    steps: 568    lr: 1.0240000000000005e-06     evaluation reward: 9.0\n",
      "episode: 2307   score: 12.0   memory length: 644433   epsilon: 0.009998020008555413    steps: 564    lr: 1.0240000000000005e-06     evaluation reward: 9.01\n",
      "episode: 2308   score: 7.0   memory length: 644841   epsilon: 0.009998020008555413    steps: 408    lr: 1.0240000000000005e-06     evaluation reward: 9.0\n",
      "episode: 2309   score: 11.0   memory length: 645244   epsilon: 0.009998020008555413    steps: 403    lr: 1.0240000000000005e-06     evaluation reward: 9.01\n",
      "episode: 2310   score: 6.0   memory length: 645601   epsilon: 0.009998020008555413    steps: 357    lr: 1.0240000000000005e-06     evaluation reward: 8.99\n",
      "episode: 2311   score: 10.0   memory length: 646055   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2312   score: 7.0   memory length: 646460   epsilon: 0.009998020008555413    steps: 405    lr: 1.0240000000000005e-06     evaluation reward: 8.95\n",
      "episode: 2313   score: 10.0   memory length: 646940   epsilon: 0.009998020008555413    steps: 480    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2314   score: 8.0   memory length: 647373   epsilon: 0.009998020008555413    steps: 433    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2315   score: 7.0   memory length: 647779   epsilon: 0.009998020008555413    steps: 406    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2316   score: 7.0   memory length: 648189   epsilon: 0.009998020008555413    steps: 410    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2317   score: 17.0   memory length: 648840   epsilon: 0.009998020008555413    steps: 651    lr: 1.0240000000000005e-06     evaluation reward: 9.03\n",
      "episode: 2318   score: 12.0   memory length: 649446   epsilon: 0.009998020008555413    steps: 606    lr: 1.0240000000000005e-06     evaluation reward: 9.09\n",
      "episode: 2319   score: 6.0   memory length: 649762   epsilon: 0.009998020008555413    steps: 316    lr: 1.0240000000000005e-06     evaluation reward: 9.09\n",
      "episode: 2320   score: 14.0   memory length: 650377   epsilon: 0.009998020008555413    steps: 615    lr: 1.0240000000000005e-06     evaluation reward: 9.15\n",
      "episode: 2321   score: 13.0   memory length: 650972   epsilon: 0.009998020008555413    steps: 595    lr: 1.0240000000000005e-06     evaluation reward: 9.19\n",
      "episode: 2322   score: 8.0   memory length: 651398   epsilon: 0.009998020008555413    steps: 426    lr: 1.0240000000000005e-06     evaluation reward: 9.21\n",
      "episode: 2323   score: 9.0   memory length: 651880   epsilon: 0.009998020008555413    steps: 482    lr: 1.0240000000000005e-06     evaluation reward: 9.21\n",
      "episode: 2324   score: 5.0   memory length: 652189   epsilon: 0.009998020008555413    steps: 309    lr: 1.0240000000000005e-06     evaluation reward: 9.21\n",
      "episode: 2325   score: 10.0   memory length: 652724   epsilon: 0.009998020008555413    steps: 535    lr: 1.0240000000000005e-06     evaluation reward: 9.16\n",
      "episode: 2326   score: 14.0   memory length: 653352   epsilon: 0.009998020008555413    steps: 628    lr: 1.0240000000000005e-06     evaluation reward: 9.22\n",
      "episode: 2327   score: 9.0   memory length: 653854   epsilon: 0.009998020008555413    steps: 502    lr: 1.0240000000000005e-06     evaluation reward: 9.21\n",
      "episode: 2328   score: 15.0   memory length: 654411   epsilon: 0.009998020008555413    steps: 557    lr: 1.0240000000000005e-06     evaluation reward: 9.28\n",
      "episode: 2329   score: 14.0   memory length: 654942   epsilon: 0.009998020008555413    steps: 531    lr: 1.0240000000000005e-06     evaluation reward: 9.35\n",
      "episode: 2330   score: 9.0   memory length: 655435   epsilon: 0.009998020008555413    steps: 493    lr: 1.0240000000000005e-06     evaluation reward: 9.38\n",
      "episode: 2331   score: 8.0   memory length: 655855   epsilon: 0.009998020008555413    steps: 420    lr: 1.0240000000000005e-06     evaluation reward: 9.33\n",
      "episode: 2332   score: 9.0   memory length: 656303   epsilon: 0.009998020008555413    steps: 448    lr: 1.0240000000000005e-06     evaluation reward: 9.32\n",
      "episode: 2333   score: 7.0   memory length: 656709   epsilon: 0.009998020008555413    steps: 406    lr: 1.0240000000000005e-06     evaluation reward: 9.25\n",
      "episode: 2334   score: 8.0   memory length: 657153   epsilon: 0.009998020008555413    steps: 444    lr: 1.0240000000000005e-06     evaluation reward: 9.24\n",
      "episode: 2335   score: 10.0   memory length: 657677   epsilon: 0.009998020008555413    steps: 524    lr: 1.0240000000000005e-06     evaluation reward: 9.27\n",
      "episode: 2336   score: 13.0   memory length: 658272   epsilon: 0.009998020008555413    steps: 595    lr: 1.0240000000000005e-06     evaluation reward: 9.31\n",
      "episode: 2337   score: 8.0   memory length: 658711   epsilon: 0.009998020008555413    steps: 439    lr: 1.0240000000000005e-06     evaluation reward: 9.33\n",
      "episode: 2338   score: 11.0   memory length: 659252   epsilon: 0.009998020008555413    steps: 541    lr: 1.0240000000000005e-06     evaluation reward: 9.36\n",
      "episode: 2339   score: 8.0   memory length: 659586   epsilon: 0.009998020008555413    steps: 334    lr: 1.0240000000000005e-06     evaluation reward: 9.36\n",
      "episode: 2340   score: 10.0   memory length: 660118   epsilon: 0.009998020008555413    steps: 532    lr: 1.0240000000000005e-06     evaluation reward: 9.35\n",
      "episode: 2341   score: 8.0   memory length: 660533   epsilon: 0.009998020008555413    steps: 415    lr: 1.0240000000000005e-06     evaluation reward: 9.36\n",
      "episode: 2342   score: 5.0   memory length: 660826   epsilon: 0.009998020008555413    steps: 293    lr: 1.0240000000000005e-06     evaluation reward: 9.32\n",
      "episode: 2343   score: 6.0   memory length: 661188   epsilon: 0.009998020008555413    steps: 362    lr: 1.0240000000000005e-06     evaluation reward: 9.29\n",
      "episode: 2344   score: 4.0   memory length: 661467   epsilon: 0.009998020008555413    steps: 279    lr: 1.0240000000000005e-06     evaluation reward: 9.24\n",
      "episode: 2345   score: 6.0   memory length: 661818   epsilon: 0.009998020008555413    steps: 351    lr: 1.0240000000000005e-06     evaluation reward: 9.2\n",
      "episode: 2346   score: 4.0   memory length: 662097   epsilon: 0.009998020008555413    steps: 279    lr: 1.0240000000000005e-06     evaluation reward: 9.14\n",
      "episode: 2347   score: 10.0   memory length: 662613   epsilon: 0.009998020008555413    steps: 516    lr: 1.0240000000000005e-06     evaluation reward: 9.11\n",
      "episode: 2348   score: 8.0   memory length: 663033   epsilon: 0.009998020008555413    steps: 420    lr: 1.0240000000000005e-06     evaluation reward: 9.09\n",
      "episode: 2349   score: 8.0   memory length: 663430   epsilon: 0.009998020008555413    steps: 397    lr: 1.0240000000000005e-06     evaluation reward: 9.04\n",
      "episode: 2350   score: 15.0   memory length: 664053   epsilon: 0.009998020008555413    steps: 623    lr: 1.0240000000000005e-06     evaluation reward: 9.09\n",
      "episode: 2351   score: 6.0   memory length: 664425   epsilon: 0.009998020008555413    steps: 372    lr: 1.0240000000000005e-06     evaluation reward: 9.04\n",
      "episode: 2352   score: 9.0   memory length: 664906   epsilon: 0.009998020008555413    steps: 481    lr: 1.0240000000000005e-06     evaluation reward: 9.05\n",
      "episode: 2353   score: 9.0   memory length: 665365   epsilon: 0.009998020008555413    steps: 459    lr: 1.0240000000000005e-06     evaluation reward: 9.08\n",
      "episode: 2354   score: 7.0   memory length: 665788   epsilon: 0.009998020008555413    steps: 423    lr: 1.0240000000000005e-06     evaluation reward: 9.02\n",
      "episode: 2355   score: 5.0   memory length: 666116   epsilon: 0.009998020008555413    steps: 328    lr: 1.0240000000000005e-06     evaluation reward: 8.96\n",
      "episode: 2356   score: 8.0   memory length: 666530   epsilon: 0.009998020008555413    steps: 414    lr: 1.0240000000000005e-06     evaluation reward: 8.98\n",
      "episode: 2357   score: 13.0   memory length: 667132   epsilon: 0.009998020008555413    steps: 602    lr: 1.0240000000000005e-06     evaluation reward: 9.0\n",
      "episode: 2358   score: 11.0   memory length: 667655   epsilon: 0.009998020008555413    steps: 523    lr: 1.0240000000000005e-06     evaluation reward: 9.07\n",
      "episode: 2359   score: 5.0   memory length: 667945   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 9.04\n",
      "episode: 2360   score: 13.0   memory length: 668455   epsilon: 0.009998020008555413    steps: 510    lr: 1.0240000000000005e-06     evaluation reward: 9.09\n",
      "episode: 2361   score: 11.0   memory length: 669005   epsilon: 0.009998020008555413    steps: 550    lr: 1.0240000000000005e-06     evaluation reward: 9.15\n",
      "episode: 2362   score: 10.0   memory length: 669465   epsilon: 0.009998020008555413    steps: 460    lr: 1.0240000000000005e-06     evaluation reward: 9.19\n",
      "episode: 2363   score: 11.0   memory length: 669983   epsilon: 0.009998020008555413    steps: 518    lr: 1.0240000000000005e-06     evaluation reward: 9.24\n",
      "episode: 2364   score: 9.0   memory length: 670437   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 9.22\n",
      "episode: 2365   score: 11.0   memory length: 670853   epsilon: 0.009998020008555413    steps: 416    lr: 1.0240000000000005e-06     evaluation reward: 9.25\n",
      "episode: 2366   score: 7.0   memory length: 671221   epsilon: 0.009998020008555413    steps: 368    lr: 1.0240000000000005e-06     evaluation reward: 9.19\n",
      "episode: 2367   score: 5.0   memory length: 671492   epsilon: 0.009998020008555413    steps: 271    lr: 1.0240000000000005e-06     evaluation reward: 9.17\n",
      "episode: 2368   score: 9.0   memory length: 671901   epsilon: 0.009998020008555413    steps: 409    lr: 1.0240000000000005e-06     evaluation reward: 9.17\n",
      "episode: 2369   score: 6.0   memory length: 672223   epsilon: 0.009998020008555413    steps: 322    lr: 1.0240000000000005e-06     evaluation reward: 9.08\n",
      "episode: 2370   score: 9.0   memory length: 672661   epsilon: 0.009998020008555413    steps: 438    lr: 1.0240000000000005e-06     evaluation reward: 9.08\n",
      "episode: 2371   score: 11.0   memory length: 673183   epsilon: 0.009998020008555413    steps: 522    lr: 1.0240000000000005e-06     evaluation reward: 9.07\n",
      "episode: 2372   score: 21.0   memory length: 673950   epsilon: 0.009998020008555413    steps: 767    lr: 1.0240000000000005e-06     evaluation reward: 9.17\n",
      "episode: 2373   score: 13.0   memory length: 674484   epsilon: 0.009998020008555413    steps: 534    lr: 1.0240000000000005e-06     evaluation reward: 9.2\n",
      "episode: 2374   score: 4.0   memory length: 674763   epsilon: 0.009998020008555413    steps: 279    lr: 1.0240000000000005e-06     evaluation reward: 9.19\n",
      "episode: 2375   score: 10.0   memory length: 675279   epsilon: 0.009998020008555413    steps: 516    lr: 1.0240000000000005e-06     evaluation reward: 9.21\n",
      "episode: 2376   score: 7.0   memory length: 675671   epsilon: 0.009998020008555413    steps: 392    lr: 1.0240000000000005e-06     evaluation reward: 9.22\n",
      "episode: 2377   score: 12.0   memory length: 676234   epsilon: 0.009998020008555413    steps: 563    lr: 1.0240000000000005e-06     evaluation reward: 9.26\n",
      "episode: 2378   score: 14.0   memory length: 676753   epsilon: 0.009998020008555413    steps: 519    lr: 1.0240000000000005e-06     evaluation reward: 9.3\n",
      "episode: 2379   score: 13.0   memory length: 677243   epsilon: 0.009998020008555413    steps: 490    lr: 1.0240000000000005e-06     evaluation reward: 9.33\n",
      "episode: 2380   score: 10.0   memory length: 677767   epsilon: 0.009998020008555413    steps: 524    lr: 1.0240000000000005e-06     evaluation reward: 9.35\n",
      "episode: 2381   score: 13.0   memory length: 678215   epsilon: 0.009998020008555413    steps: 448    lr: 1.0240000000000005e-06     evaluation reward: 9.35\n",
      "episode: 2382   score: 7.0   memory length: 678629   epsilon: 0.009998020008555413    steps: 414    lr: 1.0240000000000005e-06     evaluation reward: 9.34\n",
      "episode: 2383   score: 12.0   memory length: 679257   epsilon: 0.009998020008555413    steps: 628    lr: 1.0240000000000005e-06     evaluation reward: 9.38\n",
      "episode: 2384   score: 10.0   memory length: 679782   epsilon: 0.009998020008555413    steps: 525    lr: 1.0240000000000005e-06     evaluation reward: 9.38\n",
      "episode: 2385   score: 9.0   memory length: 680213   epsilon: 0.009998020008555413    steps: 431    lr: 1.0240000000000005e-06     evaluation reward: 9.4\n",
      "episode: 2386   score: 8.0   memory length: 680648   epsilon: 0.009998020008555413    steps: 435    lr: 1.0240000000000005e-06     evaluation reward: 9.37\n",
      "episode: 2387   score: 13.0   memory length: 681138   epsilon: 0.009998020008555413    steps: 490    lr: 1.0240000000000005e-06     evaluation reward: 9.41\n",
      "episode: 2388   score: 11.0   memory length: 681649   epsilon: 0.009998020008555413    steps: 511    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2389   score: 7.0   memory length: 682033   epsilon: 0.009998020008555413    steps: 384    lr: 1.0240000000000005e-06     evaluation reward: 9.46\n",
      "episode: 2390   score: 10.0   memory length: 682559   epsilon: 0.009998020008555413    steps: 526    lr: 1.0240000000000005e-06     evaluation reward: 9.49\n",
      "episode: 2391   score: 13.0   memory length: 683121   epsilon: 0.009998020008555413    steps: 562    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2392   score: 6.0   memory length: 683425   epsilon: 0.009998020008555413    steps: 304    lr: 1.0240000000000005e-06     evaluation reward: 9.55\n",
      "episode: 2393   score: 7.0   memory length: 683800   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 9.52\n",
      "episode: 2394   score: 6.0   memory length: 684140   epsilon: 0.009998020008555413    steps: 340    lr: 1.0240000000000005e-06     evaluation reward: 9.51\n",
      "episode: 2395   score: 9.0   memory length: 684618   epsilon: 0.009998020008555413    steps: 478    lr: 1.0240000000000005e-06     evaluation reward: 9.51\n",
      "episode: 2396   score: 14.0   memory length: 685133   epsilon: 0.009998020008555413    steps: 515    lr: 1.0240000000000005e-06     evaluation reward: 9.49\n",
      "episode: 2397   score: 10.0   memory length: 685623   epsilon: 0.009998020008555413    steps: 490    lr: 1.0240000000000005e-06     evaluation reward: 9.43\n",
      "episode: 2398   score: 6.0   memory length: 685976   epsilon: 0.009998020008555413    steps: 353    lr: 1.0240000000000005e-06     evaluation reward: 9.4\n",
      "episode: 2399   score: 9.0   memory length: 686450   epsilon: 0.009998020008555413    steps: 474    lr: 1.0240000000000005e-06     evaluation reward: 9.43\n",
      "episode: 2400   score: 12.0   memory length: 686983   epsilon: 0.009998020008555413    steps: 533    lr: 1.0240000000000005e-06     evaluation reward: 9.44\n",
      "episode: 2401   score: 15.0   memory length: 687576   epsilon: 0.009998020008555413    steps: 593    lr: 1.0240000000000005e-06     evaluation reward: 9.44\n",
      "episode: 2402   score: 10.0   memory length: 688066   epsilon: 0.009998020008555413    steps: 490    lr: 1.0240000000000005e-06     evaluation reward: 9.47\n",
      "episode: 2403   score: 9.0   memory length: 688557   epsilon: 0.009998020008555413    steps: 491    lr: 1.0240000000000005e-06     evaluation reward: 9.51\n",
      "episode: 2404   score: 10.0   memory length: 689077   epsilon: 0.009998020008555413    steps: 520    lr: 1.0240000000000005e-06     evaluation reward: 9.49\n",
      "episode: 2405   score: 8.0   memory length: 689556   epsilon: 0.009998020008555413    steps: 479    lr: 1.0240000000000005e-06     evaluation reward: 9.5\n",
      "episode: 2406   score: 6.0   memory length: 689909   epsilon: 0.009998020008555413    steps: 353    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2407   score: 9.0   memory length: 690366   epsilon: 0.009998020008555413    steps: 457    lr: 1.0240000000000005e-06     evaluation reward: 9.42\n",
      "episode: 2408   score: 11.0   memory length: 690896   epsilon: 0.009998020008555413    steps: 530    lr: 1.0240000000000005e-06     evaluation reward: 9.46\n",
      "episode: 2409   score: 10.0   memory length: 691415   epsilon: 0.009998020008555413    steps: 519    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2410   score: 6.0   memory length: 691771   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2411   score: 9.0   memory length: 692256   epsilon: 0.009998020008555413    steps: 485    lr: 1.0240000000000005e-06     evaluation reward: 9.44\n",
      "episode: 2412   score: 10.0   memory length: 692764   epsilon: 0.009998020008555413    steps: 508    lr: 1.0240000000000005e-06     evaluation reward: 9.47\n",
      "episode: 2413   score: 9.0   memory length: 693220   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 9.46\n",
      "episode: 2414   score: 8.0   memory length: 693645   epsilon: 0.009998020008555413    steps: 425    lr: 1.0240000000000005e-06     evaluation reward: 9.46\n",
      "episode: 2415   score: 7.0   memory length: 694022   epsilon: 0.009998020008555413    steps: 377    lr: 1.0240000000000005e-06     evaluation reward: 9.46\n",
      "episode: 2416   score: 6.0   memory length: 694364   epsilon: 0.009998020008555413    steps: 342    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2417   score: 9.0   memory length: 694791   epsilon: 0.009998020008555413    steps: 427    lr: 1.0240000000000005e-06     evaluation reward: 9.37\n",
      "episode: 2418   score: 5.0   memory length: 695119   epsilon: 0.009998020008555413    steps: 328    lr: 1.0240000000000005e-06     evaluation reward: 9.3\n",
      "episode: 2419   score: 8.0   memory length: 695551   epsilon: 0.009998020008555413    steps: 432    lr: 1.0240000000000005e-06     evaluation reward: 9.32\n",
      "episode: 2420   score: 6.0   memory length: 695906   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 9.24\n",
      "episode: 2421   score: 5.0   memory length: 696218   epsilon: 0.009998020008555413    steps: 312    lr: 1.0240000000000005e-06     evaluation reward: 9.16\n",
      "episode: 2422   score: 8.0   memory length: 696660   epsilon: 0.009998020008555413    steps: 442    lr: 1.0240000000000005e-06     evaluation reward: 9.16\n",
      "episode: 2423   score: 9.0   memory length: 697090   epsilon: 0.009998020008555413    steps: 430    lr: 1.0240000000000005e-06     evaluation reward: 9.16\n",
      "episode: 2424   score: 12.0   memory length: 697574   epsilon: 0.009998020008555413    steps: 484    lr: 1.0240000000000005e-06     evaluation reward: 9.23\n",
      "episode: 2425   score: 13.0   memory length: 698061   epsilon: 0.009998020008555413    steps: 487    lr: 1.0240000000000005e-06     evaluation reward: 9.26\n",
      "episode: 2426   score: 14.0   memory length: 698593   epsilon: 0.009998020008555413    steps: 532    lr: 1.0240000000000005e-06     evaluation reward: 9.26\n",
      "episode: 2427   score: 8.0   memory length: 699024   epsilon: 0.009998020008555413    steps: 431    lr: 1.0240000000000005e-06     evaluation reward: 9.25\n",
      "episode: 2428   score: 10.0   memory length: 699547   epsilon: 0.009998020008555413    steps: 523    lr: 1.0240000000000005e-06     evaluation reward: 9.2\n",
      "episode: 2429   score: 8.0   memory length: 699989   epsilon: 0.009998020008555413    steps: 442    lr: 1.0240000000000005e-06     evaluation reward: 9.14\n",
      "episode: 2430   score: 6.0   memory length: 700332   epsilon: 0.009998020008555413    steps: 343    lr: 4.0960000000000023e-07     evaluation reward: 9.11\n",
      "episode: 2431   score: 9.0   memory length: 700787   epsilon: 0.009998020008555413    steps: 455    lr: 4.0960000000000023e-07     evaluation reward: 9.12\n",
      "episode: 2432   score: 9.0   memory length: 701280   epsilon: 0.009998020008555413    steps: 493    lr: 4.0960000000000023e-07     evaluation reward: 9.12\n",
      "episode: 2433   score: 11.0   memory length: 701810   epsilon: 0.009998020008555413    steps: 530    lr: 4.0960000000000023e-07     evaluation reward: 9.16\n",
      "episode: 2434   score: 8.0   memory length: 702252   epsilon: 0.009998020008555413    steps: 442    lr: 4.0960000000000023e-07     evaluation reward: 9.16\n",
      "episode: 2435   score: 15.0   memory length: 702823   epsilon: 0.009998020008555413    steps: 571    lr: 4.0960000000000023e-07     evaluation reward: 9.21\n",
      "episode: 2436   score: 9.0   memory length: 703304   epsilon: 0.009998020008555413    steps: 481    lr: 4.0960000000000023e-07     evaluation reward: 9.17\n",
      "episode: 2437   score: 5.0   memory length: 703630   epsilon: 0.009998020008555413    steps: 326    lr: 4.0960000000000023e-07     evaluation reward: 9.14\n",
      "episode: 2438   score: 13.0   memory length: 704157   epsilon: 0.009998020008555413    steps: 527    lr: 4.0960000000000023e-07     evaluation reward: 9.16\n",
      "episode: 2439   score: 9.0   memory length: 704612   epsilon: 0.009998020008555413    steps: 455    lr: 4.0960000000000023e-07     evaluation reward: 9.17\n",
      "episode: 2440   score: 8.0   memory length: 705020   epsilon: 0.009998020008555413    steps: 408    lr: 4.0960000000000023e-07     evaluation reward: 9.15\n",
      "episode: 2441   score: 10.0   memory length: 705499   epsilon: 0.009998020008555413    steps: 479    lr: 4.0960000000000023e-07     evaluation reward: 9.17\n",
      "episode: 2442   score: 10.0   memory length: 706045   epsilon: 0.009998020008555413    steps: 546    lr: 4.0960000000000023e-07     evaluation reward: 9.22\n",
      "episode: 2443   score: 8.0   memory length: 706465   epsilon: 0.009998020008555413    steps: 420    lr: 4.0960000000000023e-07     evaluation reward: 9.24\n",
      "episode: 2444   score: 10.0   memory length: 706944   epsilon: 0.009998020008555413    steps: 479    lr: 4.0960000000000023e-07     evaluation reward: 9.3\n",
      "episode: 2445   score: 14.0   memory length: 707453   epsilon: 0.009998020008555413    steps: 509    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2446   score: 6.0   memory length: 707851   epsilon: 0.009998020008555413    steps: 398    lr: 4.0960000000000023e-07     evaluation reward: 9.4\n",
      "episode: 2447   score: 10.0   memory length: 708373   epsilon: 0.009998020008555413    steps: 522    lr: 4.0960000000000023e-07     evaluation reward: 9.4\n",
      "episode: 2448   score: 15.0   memory length: 708920   epsilon: 0.009998020008555413    steps: 547    lr: 4.0960000000000023e-07     evaluation reward: 9.47\n",
      "episode: 2449   score: 9.0   memory length: 709374   epsilon: 0.009998020008555413    steps: 454    lr: 4.0960000000000023e-07     evaluation reward: 9.48\n",
      "episode: 2450   score: 11.0   memory length: 709795   epsilon: 0.009998020008555413    steps: 421    lr: 4.0960000000000023e-07     evaluation reward: 9.44\n",
      "episode: 2451   score: 9.0   memory length: 710289   epsilon: 0.009998020008555413    steps: 494    lr: 4.0960000000000023e-07     evaluation reward: 9.47\n",
      "episode: 2452   score: 13.0   memory length: 710865   epsilon: 0.009998020008555413    steps: 576    lr: 4.0960000000000023e-07     evaluation reward: 9.51\n",
      "episode: 2453   score: 9.0   memory length: 711338   epsilon: 0.009998020008555413    steps: 473    lr: 4.0960000000000023e-07     evaluation reward: 9.51\n",
      "episode: 2454   score: 5.0   memory length: 711650   epsilon: 0.009998020008555413    steps: 312    lr: 4.0960000000000023e-07     evaluation reward: 9.49\n",
      "episode: 2455   score: 13.0   memory length: 712177   epsilon: 0.009998020008555413    steps: 527    lr: 4.0960000000000023e-07     evaluation reward: 9.57\n",
      "episode: 2456   score: 13.0   memory length: 712704   epsilon: 0.009998020008555413    steps: 527    lr: 4.0960000000000023e-07     evaluation reward: 9.62\n",
      "episode: 2457   score: 10.0   memory length: 713198   epsilon: 0.009998020008555413    steps: 494    lr: 4.0960000000000023e-07     evaluation reward: 9.59\n",
      "episode: 2458   score: 7.0   memory length: 713604   epsilon: 0.009998020008555413    steps: 406    lr: 4.0960000000000023e-07     evaluation reward: 9.55\n",
      "episode: 2459   score: 9.0   memory length: 714033   epsilon: 0.009998020008555413    steps: 429    lr: 4.0960000000000023e-07     evaluation reward: 9.59\n",
      "episode: 2460   score: 8.0   memory length: 714438   epsilon: 0.009998020008555413    steps: 405    lr: 4.0960000000000023e-07     evaluation reward: 9.54\n",
      "episode: 2461   score: 15.0   memory length: 715011   epsilon: 0.009998020008555413    steps: 573    lr: 4.0960000000000023e-07     evaluation reward: 9.58\n",
      "episode: 2462   score: 11.0   memory length: 715547   epsilon: 0.009998020008555413    steps: 536    lr: 4.0960000000000023e-07     evaluation reward: 9.59\n",
      "episode: 2463   score: 7.0   memory length: 715933   epsilon: 0.009998020008555413    steps: 386    lr: 4.0960000000000023e-07     evaluation reward: 9.55\n",
      "episode: 2464   score: 9.0   memory length: 716442   epsilon: 0.009998020008555413    steps: 509    lr: 4.0960000000000023e-07     evaluation reward: 9.55\n",
      "episode: 2465   score: 23.0   memory length: 717224   epsilon: 0.009998020008555413    steps: 782    lr: 4.0960000000000023e-07     evaluation reward: 9.67\n",
      "episode: 2466   score: 9.0   memory length: 717698   epsilon: 0.009998020008555413    steps: 474    lr: 4.0960000000000023e-07     evaluation reward: 9.69\n",
      "episode: 2467   score: 6.0   memory length: 718094   epsilon: 0.009998020008555413    steps: 396    lr: 4.0960000000000023e-07     evaluation reward: 9.7\n",
      "episode: 2468   score: 11.0   memory length: 718631   epsilon: 0.009998020008555413    steps: 537    lr: 4.0960000000000023e-07     evaluation reward: 9.72\n",
      "episode: 2469   score: 10.0   memory length: 719052   epsilon: 0.009998020008555413    steps: 421    lr: 4.0960000000000023e-07     evaluation reward: 9.76\n",
      "episode: 2470   score: 8.0   memory length: 719481   epsilon: 0.009998020008555413    steps: 429    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2471   score: 8.0   memory length: 719958   epsilon: 0.009998020008555413    steps: 477    lr: 4.0960000000000023e-07     evaluation reward: 9.72\n",
      "episode: 2472   score: 8.0   memory length: 720400   epsilon: 0.009998020008555413    steps: 442    lr: 4.0960000000000023e-07     evaluation reward: 9.59\n",
      "episode: 2473   score: 9.0   memory length: 720845   epsilon: 0.009998020008555413    steps: 445    lr: 4.0960000000000023e-07     evaluation reward: 9.55\n",
      "episode: 2474   score: 10.0   memory length: 721322   epsilon: 0.009998020008555413    steps: 477    lr: 4.0960000000000023e-07     evaluation reward: 9.61\n",
      "episode: 2475   score: 11.0   memory length: 721850   epsilon: 0.009998020008555413    steps: 528    lr: 4.0960000000000023e-07     evaluation reward: 9.62\n",
      "episode: 2476   score: 10.0   memory length: 722345   epsilon: 0.009998020008555413    steps: 495    lr: 4.0960000000000023e-07     evaluation reward: 9.65\n",
      "episode: 2477   score: 13.0   memory length: 722980   epsilon: 0.009998020008555413    steps: 635    lr: 4.0960000000000023e-07     evaluation reward: 9.66\n",
      "episode: 2478   score: 9.0   memory length: 723473   epsilon: 0.009998020008555413    steps: 493    lr: 4.0960000000000023e-07     evaluation reward: 9.61\n",
      "episode: 2479   score: 6.0   memory length: 723829   epsilon: 0.009998020008555413    steps: 356    lr: 4.0960000000000023e-07     evaluation reward: 9.54\n",
      "episode: 2480   score: 10.0   memory length: 724331   epsilon: 0.009998020008555413    steps: 502    lr: 4.0960000000000023e-07     evaluation reward: 9.54\n",
      "episode: 2481   score: 5.0   memory length: 724642   epsilon: 0.009998020008555413    steps: 311    lr: 4.0960000000000023e-07     evaluation reward: 9.46\n",
      "episode: 2482   score: 8.0   memory length: 725077   epsilon: 0.009998020008555413    steps: 435    lr: 4.0960000000000023e-07     evaluation reward: 9.47\n",
      "episode: 2483   score: 5.0   memory length: 725383   epsilon: 0.009998020008555413    steps: 306    lr: 4.0960000000000023e-07     evaluation reward: 9.4\n",
      "episode: 2484   score: 4.0   memory length: 725662   epsilon: 0.009998020008555413    steps: 279    lr: 4.0960000000000023e-07     evaluation reward: 9.34\n",
      "episode: 2485   score: 9.0   memory length: 726119   epsilon: 0.009998020008555413    steps: 457    lr: 4.0960000000000023e-07     evaluation reward: 9.34\n",
      "episode: 2486   score: 9.0   memory length: 726596   epsilon: 0.009998020008555413    steps: 477    lr: 4.0960000000000023e-07     evaluation reward: 9.35\n",
      "episode: 2487   score: 16.0   memory length: 727217   epsilon: 0.009998020008555413    steps: 621    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2488   score: 8.0   memory length: 727649   epsilon: 0.009998020008555413    steps: 432    lr: 4.0960000000000023e-07     evaluation reward: 9.35\n",
      "episode: 2489   score: 16.0   memory length: 728233   epsilon: 0.009998020008555413    steps: 584    lr: 4.0960000000000023e-07     evaluation reward: 9.44\n",
      "episode: 2490   score: 5.0   memory length: 728545   epsilon: 0.009998020008555413    steps: 312    lr: 4.0960000000000023e-07     evaluation reward: 9.39\n",
      "episode: 2491   score: 9.0   memory length: 729028   epsilon: 0.009998020008555413    steps: 483    lr: 4.0960000000000023e-07     evaluation reward: 9.35\n",
      "episode: 2492   score: 11.0   memory length: 729558   epsilon: 0.009998020008555413    steps: 530    lr: 4.0960000000000023e-07     evaluation reward: 9.4\n",
      "episode: 2493   score: 6.0   memory length: 729947   epsilon: 0.009998020008555413    steps: 389    lr: 4.0960000000000023e-07     evaluation reward: 9.39\n",
      "episode: 2494   score: 5.0   memory length: 730251   epsilon: 0.009998020008555413    steps: 304    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2495   score: 9.0   memory length: 730728   epsilon: 0.009998020008555413    steps: 477    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2496   score: 13.0   memory length: 731315   epsilon: 0.009998020008555413    steps: 587    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2497   score: 11.0   memory length: 731829   epsilon: 0.009998020008555413    steps: 514    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2498   score: 8.0   memory length: 732243   epsilon: 0.009998020008555413    steps: 414    lr: 4.0960000000000023e-07     evaluation reward: 9.4\n",
      "episode: 2499   score: 6.0   memory length: 732586   epsilon: 0.009998020008555413    steps: 343    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2500   score: 7.0   memory length: 732973   epsilon: 0.009998020008555413    steps: 387    lr: 4.0960000000000023e-07     evaluation reward: 9.32\n",
      "episode: 2501   score: 8.0   memory length: 733427   epsilon: 0.009998020008555413    steps: 454    lr: 4.0960000000000023e-07     evaluation reward: 9.25\n",
      "episode: 2502   score: 4.0   memory length: 733700   epsilon: 0.009998020008555413    steps: 273    lr: 4.0960000000000023e-07     evaluation reward: 9.19\n",
      "episode: 2503   score: 7.0   memory length: 734069   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.17\n",
      "episode: 2504   score: 7.0   memory length: 734450   epsilon: 0.009998020008555413    steps: 381    lr: 4.0960000000000023e-07     evaluation reward: 9.14\n",
      "episode: 2505   score: 5.0   memory length: 734758   epsilon: 0.009998020008555413    steps: 308    lr: 4.0960000000000023e-07     evaluation reward: 9.11\n",
      "episode: 2506   score: 7.0   memory length: 735143   epsilon: 0.009998020008555413    steps: 385    lr: 4.0960000000000023e-07     evaluation reward: 9.12\n",
      "episode: 2507   score: 14.0   memory length: 735696   epsilon: 0.009998020008555413    steps: 553    lr: 4.0960000000000023e-07     evaluation reward: 9.17\n",
      "episode: 2508   score: 17.0   memory length: 736257   epsilon: 0.009998020008555413    steps: 561    lr: 4.0960000000000023e-07     evaluation reward: 9.23\n",
      "episode: 2509   score: 8.0   memory length: 736691   epsilon: 0.009998020008555413    steps: 434    lr: 4.0960000000000023e-07     evaluation reward: 9.21\n",
      "episode: 2510   score: 8.0   memory length: 737111   epsilon: 0.009998020008555413    steps: 420    lr: 4.0960000000000023e-07     evaluation reward: 9.23\n",
      "episode: 2511   score: 5.0   memory length: 737422   epsilon: 0.009998020008555413    steps: 311    lr: 4.0960000000000023e-07     evaluation reward: 9.19\n",
      "episode: 2512   score: 7.0   memory length: 737793   epsilon: 0.009998020008555413    steps: 371    lr: 4.0960000000000023e-07     evaluation reward: 9.16\n",
      "episode: 2513   score: 7.0   memory length: 738199   epsilon: 0.009998020008555413    steps: 406    lr: 4.0960000000000023e-07     evaluation reward: 9.14\n",
      "episode: 2514   score: 8.0   memory length: 738616   epsilon: 0.009998020008555413    steps: 417    lr: 4.0960000000000023e-07     evaluation reward: 9.14\n",
      "episode: 2515   score: 10.0   memory length: 739192   epsilon: 0.009998020008555413    steps: 576    lr: 4.0960000000000023e-07     evaluation reward: 9.17\n",
      "episode: 2516   score: 7.0   memory length: 739581   epsilon: 0.009998020008555413    steps: 389    lr: 4.0960000000000023e-07     evaluation reward: 9.18\n",
      "episode: 2517   score: 9.0   memory length: 740034   epsilon: 0.009998020008555413    steps: 453    lr: 4.0960000000000023e-07     evaluation reward: 9.18\n",
      "episode: 2518   score: 12.0   memory length: 740602   epsilon: 0.009998020008555413    steps: 568    lr: 4.0960000000000023e-07     evaluation reward: 9.25\n",
      "episode: 2519   score: 11.0   memory length: 741096   epsilon: 0.009998020008555413    steps: 494    lr: 4.0960000000000023e-07     evaluation reward: 9.28\n",
      "episode: 2520   score: 7.0   memory length: 741487   epsilon: 0.009998020008555413    steps: 391    lr: 4.0960000000000023e-07     evaluation reward: 9.29\n",
      "episode: 2521   score: 6.0   memory length: 741838   epsilon: 0.009998020008555413    steps: 351    lr: 4.0960000000000023e-07     evaluation reward: 9.3\n",
      "episode: 2522   score: 16.0   memory length: 742543   epsilon: 0.009998020008555413    steps: 705    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2523   score: 9.0   memory length: 743017   epsilon: 0.009998020008555413    steps: 474    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2524   score: 12.0   memory length: 743514   epsilon: 0.009998020008555413    steps: 497    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2525   score: 11.0   memory length: 744110   epsilon: 0.009998020008555413    steps: 596    lr: 4.0960000000000023e-07     evaluation reward: 9.36\n",
      "episode: 2526   score: 8.0   memory length: 744560   epsilon: 0.009998020008555413    steps: 450    lr: 4.0960000000000023e-07     evaluation reward: 9.3\n",
      "episode: 2527   score: 9.0   memory length: 745016   epsilon: 0.009998020008555413    steps: 456    lr: 4.0960000000000023e-07     evaluation reward: 9.31\n",
      "episode: 2528   score: 11.0   memory length: 745455   epsilon: 0.009998020008555413    steps: 439    lr: 4.0960000000000023e-07     evaluation reward: 9.32\n",
      "episode: 2529   score: 9.0   memory length: 745930   epsilon: 0.009998020008555413    steps: 475    lr: 4.0960000000000023e-07     evaluation reward: 9.33\n",
      "episode: 2530   score: 17.0   memory length: 746397   epsilon: 0.009998020008555413    steps: 467    lr: 4.0960000000000023e-07     evaluation reward: 9.44\n",
      "episode: 2531   score: 8.0   memory length: 746852   epsilon: 0.009998020008555413    steps: 455    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2532   score: 12.0   memory length: 747474   epsilon: 0.009998020008555413    steps: 622    lr: 4.0960000000000023e-07     evaluation reward: 9.46\n",
      "episode: 2533   score: 8.0   memory length: 747903   epsilon: 0.009998020008555413    steps: 429    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2534   score: 12.0   memory length: 748341   epsilon: 0.009998020008555413    steps: 438    lr: 4.0960000000000023e-07     evaluation reward: 9.47\n",
      "episode: 2535   score: 9.0   memory length: 748859   epsilon: 0.009998020008555413    steps: 518    lr: 4.0960000000000023e-07     evaluation reward: 9.41\n",
      "episode: 2536   score: 11.0   memory length: 749342   epsilon: 0.009998020008555413    steps: 483    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2537   score: 9.0   memory length: 749781   epsilon: 0.009998020008555413    steps: 439    lr: 4.0960000000000023e-07     evaluation reward: 9.47\n",
      "episode: 2538   score: 8.0   memory length: 750186   epsilon: 0.009998020008555413    steps: 405    lr: 4.0960000000000023e-07     evaluation reward: 9.42\n",
      "episode: 2539   score: 6.0   memory length: 750521   epsilon: 0.009998020008555413    steps: 335    lr: 4.0960000000000023e-07     evaluation reward: 9.39\n",
      "episode: 2540   score: 11.0   memory length: 751096   epsilon: 0.009998020008555413    steps: 575    lr: 4.0960000000000023e-07     evaluation reward: 9.42\n",
      "episode: 2541   score: 12.0   memory length: 751659   epsilon: 0.009998020008555413    steps: 563    lr: 4.0960000000000023e-07     evaluation reward: 9.44\n",
      "episode: 2542   score: 9.0   memory length: 752133   epsilon: 0.009998020008555413    steps: 474    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2543   score: 6.0   memory length: 752508   epsilon: 0.009998020008555413    steps: 375    lr: 4.0960000000000023e-07     evaluation reward: 9.41\n",
      "episode: 2544   score: 10.0   memory length: 753026   epsilon: 0.009998020008555413    steps: 518    lr: 4.0960000000000023e-07     evaluation reward: 9.41\n",
      "episode: 2545   score: 6.0   memory length: 753404   epsilon: 0.009998020008555413    steps: 378    lr: 4.0960000000000023e-07     evaluation reward: 9.33\n",
      "episode: 2546   score: 16.0   memory length: 753955   epsilon: 0.009998020008555413    steps: 551    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2547   score: 10.0   memory length: 754420   epsilon: 0.009998020008555413    steps: 465    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2548   score: 9.0   memory length: 754861   epsilon: 0.009998020008555413    steps: 441    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2549   score: 9.0   memory length: 755285   epsilon: 0.009998020008555413    steps: 424    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2550   score: 12.0   memory length: 755817   epsilon: 0.009998020008555413    steps: 532    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2551   score: 8.0   memory length: 756203   epsilon: 0.009998020008555413    steps: 386    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2552   score: 5.0   memory length: 756534   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 9.29\n",
      "episode: 2553   score: 8.0   memory length: 756940   epsilon: 0.009998020008555413    steps: 406    lr: 4.0960000000000023e-07     evaluation reward: 9.28\n",
      "episode: 2554   score: 8.0   memory length: 757347   epsilon: 0.009998020008555413    steps: 407    lr: 4.0960000000000023e-07     evaluation reward: 9.31\n",
      "episode: 2555   score: 7.0   memory length: 757720   epsilon: 0.009998020008555413    steps: 373    lr: 4.0960000000000023e-07     evaluation reward: 9.25\n",
      "episode: 2556   score: 6.0   memory length: 758078   epsilon: 0.009998020008555413    steps: 358    lr: 4.0960000000000023e-07     evaluation reward: 9.18\n",
      "episode: 2557   score: 15.0   memory length: 758624   epsilon: 0.009998020008555413    steps: 546    lr: 4.0960000000000023e-07     evaluation reward: 9.23\n",
      "episode: 2558   score: 24.0   memory length: 759264   epsilon: 0.009998020008555413    steps: 640    lr: 4.0960000000000023e-07     evaluation reward: 9.4\n",
      "episode: 2559   score: 12.0   memory length: 759787   epsilon: 0.009998020008555413    steps: 523    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2560   score: 12.0   memory length: 760216   epsilon: 0.009998020008555413    steps: 429    lr: 4.0960000000000023e-07     evaluation reward: 9.47\n",
      "episode: 2561   score: 7.0   memory length: 760604   epsilon: 0.009998020008555413    steps: 388    lr: 4.0960000000000023e-07     evaluation reward: 9.39\n",
      "episode: 2562   score: 11.0   memory length: 761124   epsilon: 0.009998020008555413    steps: 520    lr: 4.0960000000000023e-07     evaluation reward: 9.39\n",
      "episode: 2563   score: 10.0   memory length: 761669   epsilon: 0.009998020008555413    steps: 545    lr: 4.0960000000000023e-07     evaluation reward: 9.42\n",
      "episode: 2564   score: 11.0   memory length: 762212   epsilon: 0.009998020008555413    steps: 543    lr: 4.0960000000000023e-07     evaluation reward: 9.44\n",
      "episode: 2565   score: 6.0   memory length: 762544   epsilon: 0.009998020008555413    steps: 332    lr: 4.0960000000000023e-07     evaluation reward: 9.27\n",
      "episode: 2566   score: 6.0   memory length: 762876   epsilon: 0.009998020008555413    steps: 332    lr: 4.0960000000000023e-07     evaluation reward: 9.24\n",
      "episode: 2567   score: 15.0   memory length: 763462   epsilon: 0.009998020008555413    steps: 586    lr: 4.0960000000000023e-07     evaluation reward: 9.33\n",
      "episode: 2568   score: 4.0   memory length: 763741   epsilon: 0.009998020008555413    steps: 279    lr: 4.0960000000000023e-07     evaluation reward: 9.26\n",
      "episode: 2569   score: 8.0   memory length: 764101   epsilon: 0.009998020008555413    steps: 360    lr: 4.0960000000000023e-07     evaluation reward: 9.24\n",
      "episode: 2570   score: 17.0   memory length: 764711   epsilon: 0.009998020008555413    steps: 610    lr: 4.0960000000000023e-07     evaluation reward: 9.33\n",
      "episode: 2571   score: 13.0   memory length: 765188   epsilon: 0.009998020008555413    steps: 477    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2572   score: 8.0   memory length: 765619   epsilon: 0.009998020008555413    steps: 431    lr: 4.0960000000000023e-07     evaluation reward: 9.38\n",
      "episode: 2573   score: 7.0   memory length: 766011   epsilon: 0.009998020008555413    steps: 392    lr: 4.0960000000000023e-07     evaluation reward: 9.36\n",
      "episode: 2574   score: 7.0   memory length: 766422   epsilon: 0.009998020008555413    steps: 411    lr: 4.0960000000000023e-07     evaluation reward: 9.33\n",
      "episode: 2575   score: 9.0   memory length: 766877   epsilon: 0.009998020008555413    steps: 455    lr: 4.0960000000000023e-07     evaluation reward: 9.31\n",
      "episode: 2576   score: 7.0   memory length: 767268   epsilon: 0.009998020008555413    steps: 391    lr: 4.0960000000000023e-07     evaluation reward: 9.28\n",
      "episode: 2577   score: 9.0   memory length: 767710   epsilon: 0.009998020008555413    steps: 442    lr: 4.0960000000000023e-07     evaluation reward: 9.24\n",
      "episode: 2578   score: 12.0   memory length: 768267   epsilon: 0.009998020008555413    steps: 557    lr: 4.0960000000000023e-07     evaluation reward: 9.27\n",
      "episode: 2579   score: 16.0   memory length: 769011   epsilon: 0.009998020008555413    steps: 744    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2580   score: 5.0   memory length: 769339   epsilon: 0.009998020008555413    steps: 328    lr: 4.0960000000000023e-07     evaluation reward: 9.32\n",
      "episode: 2581   score: 5.0   memory length: 769667   epsilon: 0.009998020008555413    steps: 328    lr: 4.0960000000000023e-07     evaluation reward: 9.32\n",
      "episode: 2582   score: 11.0   memory length: 770217   epsilon: 0.009998020008555413    steps: 550    lr: 4.0960000000000023e-07     evaluation reward: 9.35\n",
      "episode: 2583   score: 7.0   memory length: 770592   epsilon: 0.009998020008555413    steps: 375    lr: 4.0960000000000023e-07     evaluation reward: 9.37\n",
      "episode: 2584   score: 9.0   memory length: 771095   epsilon: 0.009998020008555413    steps: 503    lr: 4.0960000000000023e-07     evaluation reward: 9.42\n",
      "episode: 2585   score: 10.0   memory length: 771609   epsilon: 0.009998020008555413    steps: 514    lr: 4.0960000000000023e-07     evaluation reward: 9.43\n",
      "episode: 2586   score: 15.0   memory length: 772317   epsilon: 0.009998020008555413    steps: 708    lr: 4.0960000000000023e-07     evaluation reward: 9.49\n",
      "episode: 2587   score: 11.0   memory length: 772851   epsilon: 0.009998020008555413    steps: 534    lr: 4.0960000000000023e-07     evaluation reward: 9.44\n",
      "episode: 2588   score: 15.0   memory length: 773354   epsilon: 0.009998020008555413    steps: 503    lr: 4.0960000000000023e-07     evaluation reward: 9.51\n",
      "episode: 2589   score: 24.0   memory length: 774001   epsilon: 0.009998020008555413    steps: 647    lr: 4.0960000000000023e-07     evaluation reward: 9.59\n",
      "episode: 2590   score: 10.0   memory length: 774473   epsilon: 0.009998020008555413    steps: 472    lr: 4.0960000000000023e-07     evaluation reward: 9.64\n",
      "episode: 2591   score: 13.0   memory length: 774950   epsilon: 0.009998020008555413    steps: 477    lr: 4.0960000000000023e-07     evaluation reward: 9.68\n",
      "episode: 2592   score: 8.0   memory length: 775392   epsilon: 0.009998020008555413    steps: 442    lr: 4.0960000000000023e-07     evaluation reward: 9.65\n",
      "episode: 2593   score: 8.0   memory length: 775821   epsilon: 0.009998020008555413    steps: 429    lr: 4.0960000000000023e-07     evaluation reward: 9.67\n",
      "episode: 2594   score: 13.0   memory length: 776333   epsilon: 0.009998020008555413    steps: 512    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2595   score: 8.0   memory length: 776728   epsilon: 0.009998020008555413    steps: 395    lr: 4.0960000000000023e-07     evaluation reward: 9.74\n",
      "episode: 2596   score: 10.0   memory length: 777149   epsilon: 0.009998020008555413    steps: 421    lr: 4.0960000000000023e-07     evaluation reward: 9.71\n",
      "episode: 2597   score: 8.0   memory length: 777603   epsilon: 0.009998020008555413    steps: 454    lr: 4.0960000000000023e-07     evaluation reward: 9.68\n",
      "episode: 2598   score: 6.0   memory length: 777959   epsilon: 0.009998020008555413    steps: 356    lr: 4.0960000000000023e-07     evaluation reward: 9.66\n",
      "episode: 2599   score: 7.0   memory length: 778345   epsilon: 0.009998020008555413    steps: 386    lr: 4.0960000000000023e-07     evaluation reward: 9.67\n",
      "episode: 2600   score: 13.0   memory length: 778827   epsilon: 0.009998020008555413    steps: 482    lr: 4.0960000000000023e-07     evaluation reward: 9.73\n",
      "episode: 2601   score: 11.0   memory length: 779355   epsilon: 0.009998020008555413    steps: 528    lr: 4.0960000000000023e-07     evaluation reward: 9.76\n",
      "episode: 2602   score: 10.0   memory length: 779885   epsilon: 0.009998020008555413    steps: 530    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2603   score: 16.0   memory length: 780602   epsilon: 0.009998020008555413    steps: 717    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2604   score: 7.0   memory length: 780987   epsilon: 0.009998020008555413    steps: 385    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2605   score: 11.0   memory length: 781505   epsilon: 0.009998020008555413    steps: 518    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2606   score: 12.0   memory length: 782052   epsilon: 0.009998020008555413    steps: 547    lr: 4.0960000000000023e-07     evaluation reward: 10.02\n",
      "episode: 2607   score: 6.0   memory length: 782404   epsilon: 0.009998020008555413    steps: 352    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2608   score: 21.0   memory length: 782966   epsilon: 0.009998020008555413    steps: 562    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2609   score: 9.0   memory length: 783390   epsilon: 0.009998020008555413    steps: 424    lr: 4.0960000000000023e-07     evaluation reward: 9.99\n",
      "episode: 2610   score: 7.0   memory length: 783759   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2611   score: 9.0   memory length: 784285   epsilon: 0.009998020008555413    steps: 526    lr: 4.0960000000000023e-07     evaluation reward: 10.02\n",
      "episode: 2612   score: 12.0   memory length: 784886   epsilon: 0.009998020008555413    steps: 601    lr: 4.0960000000000023e-07     evaluation reward: 10.07\n",
      "episode: 2613   score: 5.0   memory length: 785179   epsilon: 0.009998020008555413    steps: 293    lr: 4.0960000000000023e-07     evaluation reward: 10.05\n",
      "episode: 2614   score: 6.0   memory length: 785541   epsilon: 0.009998020008555413    steps: 362    lr: 4.0960000000000023e-07     evaluation reward: 10.03\n",
      "episode: 2615   score: 9.0   memory length: 785981   epsilon: 0.009998020008555413    steps: 440    lr: 4.0960000000000023e-07     evaluation reward: 10.02\n",
      "episode: 2616   score: 6.0   memory length: 786308   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 10.01\n",
      "episode: 2617   score: 5.0   memory length: 786619   epsilon: 0.009998020008555413    steps: 311    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2618   score: 9.0   memory length: 787088   epsilon: 0.009998020008555413    steps: 469    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2619   score: 8.0   memory length: 787507   epsilon: 0.009998020008555413    steps: 419    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2620   score: 15.0   memory length: 788034   epsilon: 0.009998020008555413    steps: 527    lr: 4.0960000000000023e-07     evaluation reward: 9.99\n",
      "episode: 2621   score: 8.0   memory length: 788444   epsilon: 0.009998020008555413    steps: 410    lr: 4.0960000000000023e-07     evaluation reward: 10.01\n",
      "episode: 2622   score: 6.0   memory length: 788773   epsilon: 0.009998020008555413    steps: 329    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2623   score: 12.0   memory length: 789331   epsilon: 0.009998020008555413    steps: 558    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2624   score: 11.0   memory length: 789863   epsilon: 0.009998020008555413    steps: 532    lr: 4.0960000000000023e-07     evaluation reward: 9.93\n",
      "episode: 2625   score: 13.0   memory length: 790396   epsilon: 0.009998020008555413    steps: 533    lr: 4.0960000000000023e-07     evaluation reward: 9.95\n",
      "episode: 2626   score: 17.0   memory length: 790986   epsilon: 0.009998020008555413    steps: 590    lr: 4.0960000000000023e-07     evaluation reward: 10.04\n",
      "episode: 2627   score: 6.0   memory length: 791303   epsilon: 0.009998020008555413    steps: 317    lr: 4.0960000000000023e-07     evaluation reward: 10.01\n",
      "episode: 2628   score: 6.0   memory length: 791620   epsilon: 0.009998020008555413    steps: 317    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2629   score: 9.0   memory length: 792114   epsilon: 0.009998020008555413    steps: 494    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2630   score: 13.0   memory length: 792618   epsilon: 0.009998020008555413    steps: 504    lr: 4.0960000000000023e-07     evaluation reward: 9.92\n",
      "episode: 2631   score: 6.0   memory length: 792958   epsilon: 0.009998020008555413    steps: 340    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2632   score: 7.0   memory length: 793346   epsilon: 0.009998020008555413    steps: 388    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2633   score: 9.0   memory length: 793801   epsilon: 0.009998020008555413    steps: 455    lr: 4.0960000000000023e-07     evaluation reward: 9.86\n",
      "episode: 2634   score: 8.0   memory length: 794232   epsilon: 0.009998020008555413    steps: 431    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2635   score: 10.0   memory length: 794736   epsilon: 0.009998020008555413    steps: 504    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2636   score: 6.0   memory length: 795071   epsilon: 0.009998020008555413    steps: 335    lr: 4.0960000000000023e-07     evaluation reward: 9.78\n",
      "episode: 2637   score: 10.0   memory length: 795627   epsilon: 0.009998020008555413    steps: 556    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2638   score: 5.0   memory length: 795949   epsilon: 0.009998020008555413    steps: 322    lr: 4.0960000000000023e-07     evaluation reward: 9.76\n",
      "episode: 2639   score: 7.0   memory length: 796340   epsilon: 0.009998020008555413    steps: 391    lr: 4.0960000000000023e-07     evaluation reward: 9.77\n",
      "episode: 2640   score: 6.0   memory length: 796695   epsilon: 0.009998020008555413    steps: 355    lr: 4.0960000000000023e-07     evaluation reward: 9.72\n",
      "episode: 2641   score: 14.0   memory length: 797279   epsilon: 0.009998020008555413    steps: 584    lr: 4.0960000000000023e-07     evaluation reward: 9.74\n",
      "episode: 2642   score: 5.0   memory length: 797601   epsilon: 0.009998020008555413    steps: 322    lr: 4.0960000000000023e-07     evaluation reward: 9.7\n",
      "episode: 2643   score: 13.0   memory length: 798129   epsilon: 0.009998020008555413    steps: 528    lr: 4.0960000000000023e-07     evaluation reward: 9.77\n",
      "episode: 2644   score: 4.0   memory length: 798405   epsilon: 0.009998020008555413    steps: 276    lr: 4.0960000000000023e-07     evaluation reward: 9.71\n",
      "episode: 2645   score: 9.0   memory length: 798887   epsilon: 0.009998020008555413    steps: 482    lr: 4.0960000000000023e-07     evaluation reward: 9.74\n",
      "episode: 2646   score: 12.0   memory length: 799471   epsilon: 0.009998020008555413    steps: 584    lr: 4.0960000000000023e-07     evaluation reward: 9.7\n",
      "episode: 2647   score: 8.0   memory length: 799877   epsilon: 0.009998020008555413    steps: 406    lr: 4.0960000000000023e-07     evaluation reward: 9.68\n",
      "episode: 2648   score: 5.0   memory length: 800199   epsilon: 0.009998020008555413    steps: 322    lr: 1.638400000000001e-07     evaluation reward: 9.64\n",
      "episode: 2649   score: 7.0   memory length: 800639   epsilon: 0.009998020008555413    steps: 440    lr: 1.638400000000001e-07     evaluation reward: 9.62\n",
      "episode: 2650   score: 6.0   memory length: 800994   epsilon: 0.009998020008555413    steps: 355    lr: 1.638400000000001e-07     evaluation reward: 9.56\n",
      "episode: 2651   score: 9.0   memory length: 801462   epsilon: 0.009998020008555413    steps: 468    lr: 1.638400000000001e-07     evaluation reward: 9.57\n",
      "episode: 2652   score: 12.0   memory length: 802029   epsilon: 0.009998020008555413    steps: 567    lr: 1.638400000000001e-07     evaluation reward: 9.64\n",
      "episode: 2653   score: 7.0   memory length: 802478   epsilon: 0.009998020008555413    steps: 449    lr: 1.638400000000001e-07     evaluation reward: 9.63\n",
      "episode: 2654   score: 11.0   memory length: 802978   epsilon: 0.009998020008555413    steps: 500    lr: 1.638400000000001e-07     evaluation reward: 9.66\n",
      "episode: 2655   score: 9.0   memory length: 803408   epsilon: 0.009998020008555413    steps: 430    lr: 1.638400000000001e-07     evaluation reward: 9.68\n",
      "episode: 2656   score: 3.0   memory length: 803637   epsilon: 0.009998020008555413    steps: 229    lr: 1.638400000000001e-07     evaluation reward: 9.65\n",
      "episode: 2657   score: 8.0   memory length: 804047   epsilon: 0.009998020008555413    steps: 410    lr: 1.638400000000001e-07     evaluation reward: 9.58\n",
      "episode: 2658   score: 4.0   memory length: 804324   epsilon: 0.009998020008555413    steps: 277    lr: 1.638400000000001e-07     evaluation reward: 9.38\n",
      "episode: 2659   score: 6.0   memory length: 804684   epsilon: 0.009998020008555413    steps: 360    lr: 1.638400000000001e-07     evaluation reward: 9.32\n",
      "episode: 2660   score: 6.0   memory length: 805032   epsilon: 0.009998020008555413    steps: 348    lr: 1.638400000000001e-07     evaluation reward: 9.26\n",
      "episode: 2661   score: 15.0   memory length: 805610   epsilon: 0.009998020008555413    steps: 578    lr: 1.638400000000001e-07     evaluation reward: 9.34\n",
      "episode: 2662   score: 13.0   memory length: 806116   epsilon: 0.009998020008555413    steps: 506    lr: 1.638400000000001e-07     evaluation reward: 9.36\n",
      "episode: 2663   score: 9.0   memory length: 806522   epsilon: 0.009998020008555413    steps: 406    lr: 1.638400000000001e-07     evaluation reward: 9.35\n",
      "episode: 2664   score: 11.0   memory length: 807080   epsilon: 0.009998020008555413    steps: 558    lr: 1.638400000000001e-07     evaluation reward: 9.35\n",
      "episode: 2665   score: 11.0   memory length: 807522   epsilon: 0.009998020008555413    steps: 442    lr: 1.638400000000001e-07     evaluation reward: 9.4\n",
      "episode: 2666   score: 4.0   memory length: 807802   epsilon: 0.009998020008555413    steps: 280    lr: 1.638400000000001e-07     evaluation reward: 9.38\n",
      "episode: 2667   score: 5.0   memory length: 808129   epsilon: 0.009998020008555413    steps: 327    lr: 1.638400000000001e-07     evaluation reward: 9.28\n",
      "episode: 2668   score: 5.0   memory length: 808457   epsilon: 0.009998020008555413    steps: 328    lr: 1.638400000000001e-07     evaluation reward: 9.29\n",
      "episode: 2669   score: 8.0   memory length: 808882   epsilon: 0.009998020008555413    steps: 425    lr: 1.638400000000001e-07     evaluation reward: 9.29\n",
      "episode: 2670   score: 6.0   memory length: 809238   epsilon: 0.009998020008555413    steps: 356    lr: 1.638400000000001e-07     evaluation reward: 9.18\n",
      "episode: 2671   score: 10.0   memory length: 809793   epsilon: 0.009998020008555413    steps: 555    lr: 1.638400000000001e-07     evaluation reward: 9.15\n",
      "episode: 2672   score: 8.0   memory length: 810179   epsilon: 0.009998020008555413    steps: 386    lr: 1.638400000000001e-07     evaluation reward: 9.15\n",
      "episode: 2673   score: 9.0   memory length: 810689   epsilon: 0.009998020008555413    steps: 510    lr: 1.638400000000001e-07     evaluation reward: 9.17\n",
      "episode: 2674   score: 11.0   memory length: 811214   epsilon: 0.009998020008555413    steps: 525    lr: 1.638400000000001e-07     evaluation reward: 9.21\n",
      "episode: 2675   score: 8.0   memory length: 811677   epsilon: 0.009998020008555413    steps: 463    lr: 1.638400000000001e-07     evaluation reward: 9.2\n",
      "episode: 2676   score: 8.0   memory length: 812097   epsilon: 0.009998020008555413    steps: 420    lr: 1.638400000000001e-07     evaluation reward: 9.21\n",
      "episode: 2677   score: 6.0   memory length: 812451   epsilon: 0.009998020008555413    steps: 354    lr: 1.638400000000001e-07     evaluation reward: 9.18\n",
      "episode: 2678   score: 4.0   memory length: 812731   epsilon: 0.009998020008555413    steps: 280    lr: 1.638400000000001e-07     evaluation reward: 9.1\n",
      "episode: 2679   score: 7.0   memory length: 813121   epsilon: 0.009998020008555413    steps: 390    lr: 1.638400000000001e-07     evaluation reward: 9.01\n",
      "episode: 2680   score: 11.0   memory length: 813648   epsilon: 0.009998020008555413    steps: 527    lr: 1.638400000000001e-07     evaluation reward: 9.07\n",
      "episode: 2681   score: 6.0   memory length: 814015   epsilon: 0.009998020008555413    steps: 367    lr: 1.638400000000001e-07     evaluation reward: 9.08\n",
      "episode: 2682   score: 6.0   memory length: 814357   epsilon: 0.009998020008555413    steps: 342    lr: 1.638400000000001e-07     evaluation reward: 9.03\n",
      "episode: 2683   score: 3.0   memory length: 814588   epsilon: 0.009998020008555413    steps: 231    lr: 1.638400000000001e-07     evaluation reward: 8.99\n",
      "episode: 2684   score: 4.0   memory length: 814866   epsilon: 0.009998020008555413    steps: 278    lr: 1.638400000000001e-07     evaluation reward: 8.94\n",
      "episode: 2685   score: 5.0   memory length: 815145   epsilon: 0.009998020008555413    steps: 279    lr: 1.638400000000001e-07     evaluation reward: 8.89\n",
      "episode: 2686   score: 9.0   memory length: 815654   epsilon: 0.009998020008555413    steps: 509    lr: 1.638400000000001e-07     evaluation reward: 8.83\n",
      "episode: 2687   score: 6.0   memory length: 816020   epsilon: 0.009998020008555413    steps: 366    lr: 1.638400000000001e-07     evaluation reward: 8.78\n",
      "episode: 2688   score: 12.0   memory length: 816597   epsilon: 0.009998020008555413    steps: 577    lr: 1.638400000000001e-07     evaluation reward: 8.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m pylab\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRewards\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     55\u001b[0m pylab\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisodes vs Reward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mpylab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./save_graph/breakout_dqn.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# save graph for training visualization\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# every episode, plot the play time\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  memory length:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m       \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  epsilon:\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mepsilon, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   steps:\u001b[39m\u001b[38;5;124m\"\u001b[39m, step,\n\u001b[1;32m     61\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   lr:\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    evaluation reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(evaluation_reward))\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/pyplot.py:1023\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1022\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m-> 1023\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/figure.py:3378\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3375\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   3376\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m-> 3378\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:457\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    459\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    460\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    398\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    399\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/lines.py:797\u001b[0m, in \u001b[0;36mLine2D.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    794\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_foreground(lc_rgba, isRGBA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    796\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_dashes(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dash_pattern)\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m         gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markersize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:146\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action + 1)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 6 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                save_dir = \"./save_model/breakout_ddqn_\"+str(e)+\".pth\"\n",
    "                torch.save(agent.policy_net, save_dir)\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_2226.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/wxzhang/assignment5/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/wxzhang/assignment5/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/wxzhang/assignment5/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/wxzhang/assignment5/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f567330d220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
    "state = env.reset()\n",
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env\n",
    "\n",
    "from agent import Agent\n",
    "action_size = 3 \n",
    "\n",
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/breakout_dqn_2226.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state, _ = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "frame = 0\n",
    "while not done:\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "        \n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video for double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/wxzhang/assignment5/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/wxzhang/assignment5/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/wxzhang/assignment5/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/wxzhang/assignment5/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7fc25c2ebaf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_ddqn_2612.pth\")\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
    "state = env.reset()\n",
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env\n",
    "\n",
    "from agent import Agent\n",
    "action_size = 3 \n",
    "\n",
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/breakout_dqn_2226.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state, _ = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "frame = 0\n",
    "while not done:\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "        \n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
