{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DQN LSTM Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxzhang/anaconda3/envs/cv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next state) tuples rather than just one (state, action, reward, next state) tuple so it can work with LSTMs. Use the previous tuples to generate the current hidden and context vector for LSTM. \n",
    "Esentially, when you get a sample from replay buffer during training, start with the first tuple and generate hidden and context vector from this and pass it onto the next tuple. Do so consequitively till you reach the last tuple, where you will make Q value predictions.\n",
    "Training loop remains nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_lstm import Agent_LSTM\n",
    "agent = Agent_LSTM(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.0   memory length: 122   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.0\n",
      "episode: 1   score: 1.0   memory length: 290   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 0.5\n",
      "episode: 2   score: 1.0   memory length: 462   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 0.6666666666666666\n",
      "episode: 3   score: 0.0   memory length: 585   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.5\n",
      "episode: 4   score: 1.0   memory length: 753   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 0.6\n",
      "episode: 5   score: 2.0   memory length: 951   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 0.8333333333333334\n",
      "episode: 6   score: 1.0   memory length: 1120   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 0.8571428571428571\n",
      "episode: 7   score: 1.0   memory length: 1271   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.875\n",
      "episode: 8   score: 1.0   memory length: 1442   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 0.8888888888888888\n",
      "episode: 9   score: 1.0   memory length: 1593   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.9\n",
      "episode: 10   score: 4.0   memory length: 1910   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.1818181818181819\n",
      "episode: 11   score: 2.0   memory length: 2107   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 12   score: 2.0   memory length: 2324   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.3076923076923077\n",
      "episode: 13   score: 0.0   memory length: 2446   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2142857142857142\n",
      "episode: 14   score: 2.0   memory length: 2662   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.2666666666666666\n",
      "episode: 15   score: 3.0   memory length: 2887   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 16   score: 0.0   memory length: 3009   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2941176470588236\n",
      "episode: 17   score: 2.0   memory length: 3226   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 18   score: 0.0   memory length: 3349   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.263157894736842\n",
      "episode: 19   score: 0.0   memory length: 3472   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 20   score: 3.0   memory length: 3718   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 21   score: 0.0   memory length: 3840   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2272727272727273\n",
      "episode: 22   score: 4.0   memory length: 4136   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.3478260869565217\n",
      "episode: 23   score: 2.0   memory length: 4333   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 24   score: 2.0   memory length: 4549   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 25   score: 4.0   memory length: 4859   epsilon: 1.0    steps: 310    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 26   score: 1.0   memory length: 5010   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4814814814814814\n",
      "episode: 27   score: 3.0   memory length: 5256   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5357142857142858\n",
      "episode: 28   score: 3.0   memory length: 5502   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5862068965517242\n",
      "episode: 29   score: 2.0   memory length: 5720   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 30   score: 5.0   memory length: 6059   epsilon: 1.0    steps: 339    lr: 0.0001     evaluation reward: 1.7096774193548387\n",
      "episode: 31   score: 1.0   memory length: 6210   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6875\n",
      "episode: 32   score: 0.0   memory length: 6333   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 33   score: 1.0   memory length: 6503   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6176470588235294\n",
      "episode: 34   score: 0.0   memory length: 6626   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 35   score: 3.0   memory length: 6891   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
      "episode: 36   score: 2.0   memory length: 7072   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.6216216216216217\n",
      "episode: 37   score: 1.0   memory length: 7244   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.605263157894737\n",
      "episode: 38   score: 3.0   memory length: 7454   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.641025641025641\n",
      "episode: 39   score: 3.0   memory length: 7681   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.675\n",
      "episode: 40   score: 2.0   memory length: 7878   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6829268292682926\n",
      "episode: 41   score: 1.0   memory length: 8050   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 42   score: 3.0   memory length: 8277   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.697674418604651\n",
      "episode: 43   score: 0.0   memory length: 8399   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6590909090909092\n",
      "episode: 44   score: 2.0   memory length: 8597   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 45   score: 1.0   memory length: 8767   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6521739130434783\n",
      "episode: 46   score: 3.0   memory length: 9028   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.6808510638297873\n",
      "episode: 47   score: 0.0   memory length: 9151   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6458333333333333\n",
      "episode: 48   score: 2.0   memory length: 9348   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.653061224489796\n",
      "episode: 49   score: 1.0   memory length: 9517   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 50   score: 2.0   memory length: 9733   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6470588235294117\n",
      "episode: 51   score: 0.0   memory length: 9856   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 52   score: 5.0   memory length: 10199   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.679245283018868\n",
      "episode: 53   score: 0.0   memory length: 10322   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6481481481481481\n",
      "episode: 54   score: 1.0   memory length: 10472   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 55   score: 1.0   memory length: 10642   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 56   score: 3.0   memory length: 10890   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.6491228070175439\n",
      "episode: 57   score: 1.0   memory length: 11059   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6379310344827587\n",
      "episode: 58   score: 3.0   memory length: 11305   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6610169491525424\n",
      "episode: 59   score: 0.0   memory length: 11428   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6333333333333333\n",
      "episode: 60   score: 1.0   memory length: 11597   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6229508196721312\n",
      "episode: 61   score: 2.0   memory length: 11794   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6290322580645162\n",
      "episode: 62   score: 1.0   memory length: 11945   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 63   score: 2.0   memory length: 12143   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 64   score: 1.0   memory length: 12312   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 65   score: 0.0   memory length: 12434   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5909090909090908\n",
      "episode: 66   score: 3.0   memory length: 12660   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.6119402985074627\n",
      "episode: 67   score: 4.0   memory length: 12918   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.6470588235294117\n",
      "episode: 68   score: 2.0   memory length: 13136   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6521739130434783\n",
      "episode: 69   score: 2.0   memory length: 13352   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6571428571428573\n",
      "episode: 70   score: 3.0   memory length: 13578   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.676056338028169\n",
      "episode: 71   score: 2.0   memory length: 13779   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.6805555555555556\n",
      "episode: 72   score: 2.0   memory length: 14001   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.6849315068493151\n",
      "episode: 73   score: 2.0   memory length: 14222   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.6891891891891893\n",
      "episode: 74   score: 0.0   memory length: 14344   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 75   score: 0.0   memory length: 14467   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.644736842105263\n",
      "episode: 76   score: 1.0   memory length: 14617   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 77   score: 1.0   memory length: 14787   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6282051282051282\n",
      "episode: 78   score: 1.0   memory length: 14938   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.620253164556962\n",
      "episode: 79   score: 0.0   memory length: 15061   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 80   score: 2.0   memory length: 15259   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6049382716049383\n",
      "episode: 81   score: 4.0   memory length: 15527   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.6341463414634145\n",
      "episode: 82   score: 2.0   memory length: 15726   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6385542168674698\n",
      "episode: 83   score: 2.0   memory length: 15944   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 84   score: 0.0   memory length: 16067   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6235294117647059\n",
      "episode: 85   score: 0.0   memory length: 16190   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6046511627906976\n",
      "episode: 86   score: 0.0   memory length: 16313   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5862068965517242\n",
      "episode: 87   score: 0.0   memory length: 16436   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5681818181818181\n",
      "episode: 88   score: 2.0   memory length: 16654   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5730337078651686\n",
      "episode: 89   score: 1.0   memory length: 16823   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5666666666666667\n",
      "episode: 90   score: 0.0   memory length: 16946   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5494505494505495\n",
      "episode: 91   score: 1.0   memory length: 17116   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5434782608695652\n",
      "episode: 92   score: 1.0   memory length: 17285   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5376344086021505\n",
      "episode: 93   score: 0.0   memory length: 17408   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5212765957446808\n",
      "episode: 94   score: 2.0   memory length: 17623   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.5263157894736843\n",
      "episode: 95   score: 2.0   memory length: 17840   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.53125\n",
      "episode: 96   score: 1.0   memory length: 17990   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5257731958762886\n",
      "episode: 97   score: 1.0   memory length: 18161   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5204081632653061\n",
      "episode: 98   score: 1.0   memory length: 18330   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5151515151515151\n",
      "episode: 99   score: 2.0   memory length: 18527   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 100   score: 1.0   memory length: 18697   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 101   score: 3.0   memory length: 18943   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 102   score: 2.0   memory length: 19161   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 103   score: 0.0   memory length: 19283   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 104   score: 2.0   memory length: 19480   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 105   score: 0.0   memory length: 19603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 106   score: 2.0   memory length: 19801   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 107   score: 0.0   memory length: 19923   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 108   score: 1.0   memory length: 20074   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 109   score: 3.0   memory length: 20346   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 110   score: 1.0   memory length: 20497   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 111   score: 2.0   memory length: 20694   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 112   score: 2.0   memory length: 20912   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 113   score: 2.0   memory length: 21130   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 114   score: 1.0   memory length: 21299   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 115   score: 3.0   memory length: 21525   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 116   score: 2.0   memory length: 21723   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 117   score: 1.0   memory length: 21892   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 118   score: 10.0   memory length: 22416   epsilon: 1.0    steps: 524    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 119   score: 0.0   memory length: 22538   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 120   score: 3.0   memory length: 22765   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 121   score: 2.0   memory length: 22967   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 122   score: 2.0   memory length: 23186   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 123   score: 0.0   memory length: 23308   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 124   score: 4.0   memory length: 23602   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 125   score: 1.0   memory length: 23773   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 126   score: 2.0   memory length: 23971   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 127   score: 1.0   memory length: 24143   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 128   score: 0.0   memory length: 24266   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 129   score: 1.0   memory length: 24418   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 130   score: 3.0   memory length: 24664   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 131   score: 0.0   memory length: 24787   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 132   score: 4.0   memory length: 25048   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 133   score: 0.0   memory length: 25171   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 134   score: 0.0   memory length: 25294   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 135   score: 2.0   memory length: 25492   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 136   score: 0.0   memory length: 25614   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 137   score: 2.0   memory length: 25796   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 138   score: 1.0   memory length: 25968   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 139   score: 1.0   memory length: 26137   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 140   score: 0.0   memory length: 26260   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 141   score: 3.0   memory length: 26486   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 142   score: 0.0   memory length: 26609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 143   score: 1.0   memory length: 26777   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 144   score: 2.0   memory length: 26995   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 145   score: 1.0   memory length: 27167   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 146   score: 0.0   memory length: 27290   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 147   score: 2.0   memory length: 27488   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 148   score: 1.0   memory length: 27656   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 149   score: 0.0   memory length: 27779   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 150   score: 1.0   memory length: 27949   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 151   score: 3.0   memory length: 28196   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 152   score: 1.0   memory length: 28368   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 153   score: 0.0   memory length: 28491   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 154   score: 0.0   memory length: 28614   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 155   score: 1.0   memory length: 28783   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 156   score: 3.0   memory length: 29011   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 157   score: 2.0   memory length: 29233   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 158   score: 4.0   memory length: 29550   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 159   score: 4.0   memory length: 29828   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 160   score: 0.0   memory length: 29951   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 161   score: 1.0   memory length: 30122   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 162   score: 2.0   memory length: 30320   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 163   score: 2.0   memory length: 30541   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 164   score: 3.0   memory length: 30754   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 165   score: 1.0   memory length: 30922   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 166   score: 0.0   memory length: 31044   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 167   score: 0.0   memory length: 31166   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 168   score: 1.0   memory length: 31317   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 169   score: 1.0   memory length: 31468   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 170   score: 1.0   memory length: 31637   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 171   score: 2.0   memory length: 31836   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 172   score: 0.0   memory length: 31959   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 173   score: 1.0   memory length: 32109   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 174   score: 2.0   memory length: 32330   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 175   score: 3.0   memory length: 32576   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 176   score: 1.0   memory length: 32747   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 177   score: 0.0   memory length: 32870   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 178   score: 0.0   memory length: 32993   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 179   score: 1.0   memory length: 33161   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 180   score: 1.0   memory length: 33331   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 181   score: 2.0   memory length: 33528   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 182   score: 0.0   memory length: 33651   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 183   score: 0.0   memory length: 33774   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 184   score: 2.0   memory length: 33992   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 185   score: 0.0   memory length: 34115   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 186   score: 2.0   memory length: 34313   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 187   score: 3.0   memory length: 34543   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 188   score: 1.0   memory length: 34712   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 189   score: 1.0   memory length: 34881   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 190   score: 2.0   memory length: 35078   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 191   score: 2.0   memory length: 35279   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 192   score: 1.0   memory length: 35449   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 193   score: 2.0   memory length: 35666   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 194   score: 0.0   memory length: 35788   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 195   score: 10.0   memory length: 36266   epsilon: 1.0    steps: 478    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 196   score: 0.0   memory length: 36389   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 197   score: 0.0   memory length: 36512   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 198   score: 2.0   memory length: 36711   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 199   score: 1.0   memory length: 36862   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 200   score: 1.0   memory length: 37013   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 201   score: 1.0   memory length: 37184   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 202   score: 2.0   memory length: 37402   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 203   score: 1.0   memory length: 37553   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 204   score: 0.0   memory length: 37675   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 205   score: 2.0   memory length: 37872   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 206   score: 2.0   memory length: 38073   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 207   score: 2.0   memory length: 38271   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 208   score: 1.0   memory length: 38443   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 209   score: 0.0   memory length: 38565   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 210   score: 0.0   memory length: 38688   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 211   score: 1.0   memory length: 38856   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 212   score: 4.0   memory length: 39171   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 213   score: 3.0   memory length: 39401   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 214   score: 2.0   memory length: 39616   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 215   score: 3.0   memory length: 39841   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 216   score: 2.0   memory length: 40038   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 217   score: 0.0   memory length: 40160   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 218   score: 1.0   memory length: 40311   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 219   score: 0.0   memory length: 40433   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 220   score: 3.0   memory length: 40661   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 221   score: 1.0   memory length: 40829   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 222   score: 1.0   memory length: 41000   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 223   score: 0.0   memory length: 41123   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 224   score: 0.0   memory length: 41245   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 225   score: 0.0   memory length: 41367   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 226   score: 2.0   memory length: 41565   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 227   score: 4.0   memory length: 41861   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 228   score: 0.0   memory length: 41984   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 229   score: 0.0   memory length: 42106   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 230   score: 2.0   memory length: 42303   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 231   score: 2.0   memory length: 42520   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 232   score: 4.0   memory length: 42794   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 233   score: 1.0   memory length: 42962   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 234   score: 1.0   memory length: 43113   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 235   score: 1.0   memory length: 43285   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 236   score: 0.0   memory length: 43407   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 237   score: 1.0   memory length: 43576   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 238   score: 2.0   memory length: 43777   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 239   score: 0.0   memory length: 43900   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 240   score: 0.0   memory length: 44023   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 241   score: 2.0   memory length: 44221   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 242   score: 0.0   memory length: 44343   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 243   score: 4.0   memory length: 44617   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 244   score: 0.0   memory length: 44740   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 245   score: 2.0   memory length: 44939   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 246   score: 2.0   memory length: 45155   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 247   score: 1.0   memory length: 45326   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 248   score: 1.0   memory length: 45476   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 249   score: 4.0   memory length: 45772   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 250   score: 1.0   memory length: 45923   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 251   score: 2.0   memory length: 46143   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 252   score: 0.0   memory length: 46266   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 253   score: 1.0   memory length: 46416   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 254   score: 4.0   memory length: 46710   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 255   score: 2.0   memory length: 46910   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 256   score: 1.0   memory length: 47079   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 257   score: 0.0   memory length: 47202   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 258   score: 2.0   memory length: 47423   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 259   score: 4.0   memory length: 47718   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 260   score: 1.0   memory length: 47887   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 261   score: 1.0   memory length: 48057   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 262   score: 2.0   memory length: 48272   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 263   score: 2.0   memory length: 48453   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 264   score: 4.0   memory length: 48751   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 265   score: 2.0   memory length: 48950   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 266   score: 2.0   memory length: 49148   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 267   score: 3.0   memory length: 49395   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 268   score: 3.0   memory length: 49638   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 269   score: 1.0   memory length: 49789   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 270   score: 3.0   memory length: 50036   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 271   score: 1.0   memory length: 50207   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 272   score: 2.0   memory length: 50405   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 273   score: 2.0   memory length: 50627   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 274   score: 2.0   memory length: 50825   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 275   score: 0.0   memory length: 50948   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 276   score: 2.0   memory length: 51128   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 277   score: 0.0   memory length: 51250   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 278   score: 3.0   memory length: 51515   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 279   score: 1.0   memory length: 51686   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 280   score: 2.0   memory length: 51883   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 281   score: 2.0   memory length: 52100   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 282   score: 1.0   memory length: 52269   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 283   score: 0.0   memory length: 52391   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 284   score: 2.0   memory length: 52589   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 285   score: 4.0   memory length: 52907   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 286   score: 1.0   memory length: 53076   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 287   score: 1.0   memory length: 53244   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 288   score: 1.0   memory length: 53415   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 289   score: 2.0   memory length: 53614   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 290   score: 0.0   memory length: 53736   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 291   score: 0.0   memory length: 53858   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 292   score: 3.0   memory length: 54083   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 293   score: 1.0   memory length: 54233   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 294   score: 1.0   memory length: 54402   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 295   score: 1.0   memory length: 54572   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 296   score: 2.0   memory length: 54771   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 297   score: 4.0   memory length: 55066   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 298   score: 2.0   memory length: 55285   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 299   score: 1.0   memory length: 55457   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 300   score: 0.0   memory length: 55580   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 301   score: 0.0   memory length: 55703   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 302   score: 1.0   memory length: 55854   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 303   score: 2.0   memory length: 56052   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 304   score: 2.0   memory length: 56268   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 305   score: 8.0   memory length: 56679   epsilon: 1.0    steps: 411    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 306   score: 1.0   memory length: 56830   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 307   score: 2.0   memory length: 57029   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 308   score: 2.0   memory length: 57227   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 309   score: 2.0   memory length: 57425   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 310   score: 1.0   memory length: 57594   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 311   score: 0.0   memory length: 57717   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 312   score: 2.0   memory length: 57915   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 313   score: 1.0   memory length: 58085   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 314   score: 2.0   memory length: 58286   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 315   score: 3.0   memory length: 58553   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 316   score: 2.0   memory length: 58771   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 317   score: 2.0   memory length: 58969   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 318   score: 0.0   memory length: 59091   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 319   score: 3.0   memory length: 59317   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 320   score: 2.0   memory length: 59534   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 321   score: 1.0   memory length: 59702   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 322   score: 2.0   memory length: 59919   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 323   score: 1.0   memory length: 60089   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 324   score: 1.0   memory length: 60259   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 325   score: 4.0   memory length: 60552   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 326   score: 2.0   memory length: 60750   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 327   score: 0.0   memory length: 60873   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 328   score: 2.0   memory length: 61090   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 329   score: 3.0   memory length: 61356   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 330   score: 1.0   memory length: 61507   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 331   score: 0.0   memory length: 61629   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 332   score: 3.0   memory length: 61855   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 333   score: 3.0   memory length: 62101   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 334   score: 1.0   memory length: 62273   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 335   score: 1.0   memory length: 62424   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 336   score: 2.0   memory length: 62622   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 337   score: 0.0   memory length: 62744   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 338   score: 0.0   memory length: 62867   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 339   score: 0.0   memory length: 62990   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 340   score: 4.0   memory length: 63285   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 341   score: 1.0   memory length: 63456   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 342   score: 0.0   memory length: 63579   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 343   score: 0.0   memory length: 63702   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 344   score: 2.0   memory length: 63900   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 345   score: 3.0   memory length: 64147   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 346   score: 1.0   memory length: 64298   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 347   score: 3.0   memory length: 64565   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 348   score: 0.0   memory length: 64688   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 349   score: 1.0   memory length: 64859   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 350   score: 3.0   memory length: 65085   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 351   score: 3.0   memory length: 65311   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 352   score: 0.0   memory length: 65434   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 353   score: 0.0   memory length: 65557   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 354   score: 0.0   memory length: 65680   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 355   score: 1.0   memory length: 65849   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 356   score: 0.0   memory length: 65971   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 357   score: 1.0   memory length: 66142   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 358   score: 1.0   memory length: 66312   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 359   score: 1.0   memory length: 66480   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 360   score: 0.0   memory length: 66603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 361   score: 0.0   memory length: 66726   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 362   score: 0.0   memory length: 66848   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 363   score: 2.0   memory length: 67045   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 364   score: 3.0   memory length: 67291   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 365   score: 3.0   memory length: 67523   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 366   score: 2.0   memory length: 67740   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 367   score: 4.0   memory length: 68055   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 368   score: 2.0   memory length: 68252   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 369   score: 4.0   memory length: 68525   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 370   score: 1.0   memory length: 68677   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 371   score: 0.0   memory length: 68800   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 372   score: 0.0   memory length: 68922   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 373   score: 0.0   memory length: 69044   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 374   score: 2.0   memory length: 69242   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 375   score: 0.0   memory length: 69365   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 376   score: 4.0   memory length: 69640   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 377   score: 0.0   memory length: 69763   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 378   score: 2.0   memory length: 69943   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 379   score: 0.0   memory length: 70066   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 380   score: 2.0   memory length: 70266   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 381   score: 2.0   memory length: 70464   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 382   score: 3.0   memory length: 70709   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 383   score: 3.0   memory length: 70976   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 384   score: 2.0   memory length: 71173   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 385   score: 0.0   memory length: 71296   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 386   score: 0.0   memory length: 71419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 387   score: 0.0   memory length: 71542   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 388   score: 3.0   memory length: 71786   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 389   score: 2.0   memory length: 72009   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 390   score: 1.0   memory length: 72177   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 391   score: 0.0   memory length: 72299   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 392   score: 0.0   memory length: 72421   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 393   score: 2.0   memory length: 72640   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 394   score: 1.0   memory length: 72809   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 395   score: 1.0   memory length: 72978   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 396   score: 4.0   memory length: 73274   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 397   score: 6.0   memory length: 73637   epsilon: 1.0    steps: 363    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 398   score: 1.0   memory length: 73808   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 399   score: 2.0   memory length: 74007   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 400   score: 1.0   memory length: 74176   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 401   score: 2.0   memory length: 74395   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 402   score: 1.0   memory length: 74566   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 403   score: 2.0   memory length: 74787   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 404   score: 3.0   memory length: 75034   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 405   score: 2.0   memory length: 75233   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 406   score: 1.0   memory length: 75401   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 407   score: 2.0   memory length: 75620   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 408   score: 3.0   memory length: 75869   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 409   score: 0.0   memory length: 75991   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 410   score: 0.0   memory length: 76113   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 411   score: 2.0   memory length: 76310   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 412   score: 0.0   memory length: 76432   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 413   score: 0.0   memory length: 76554   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 414   score: 3.0   memory length: 76801   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 415   score: 4.0   memory length: 77066   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 416   score: 3.0   memory length: 77294   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 417   score: 1.0   memory length: 77445   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 418   score: 2.0   memory length: 77663   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 419   score: 0.0   memory length: 77786   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 420   score: 0.0   memory length: 77909   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 421   score: 0.0   memory length: 78032   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 422   score: 1.0   memory length: 78202   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 423   score: 1.0   memory length: 78371   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 424   score: 0.0   memory length: 78493   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 425   score: 0.0   memory length: 78616   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 426   score: 4.0   memory length: 78878   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 427   score: 2.0   memory length: 79076   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 428   score: 2.0   memory length: 79292   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 429   score: 0.0   memory length: 79414   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 430   score: 2.0   memory length: 79611   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 431   score: 0.0   memory length: 79734   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 432   score: 2.0   memory length: 79956   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 433   score: 1.0   memory length: 80107   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 434   score: 2.0   memory length: 80326   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 435   score: 1.0   memory length: 80496   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 436   score: 3.0   memory length: 80748   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 437   score: 0.0   memory length: 80870   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 438   score: 0.0   memory length: 80993   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 439   score: 0.0   memory length: 81116   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 440   score: 0.0   memory length: 81239   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 441   score: 1.0   memory length: 81408   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 442   score: 2.0   memory length: 81623   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 443   score: 3.0   memory length: 81870   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 444   score: 1.0   memory length: 82039   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 445   score: 1.0   memory length: 82208   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 446   score: 2.0   memory length: 82425   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 447   score: 6.0   memory length: 82780   epsilon: 1.0    steps: 355    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 448   score: 3.0   memory length: 83046   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 449   score: 1.0   memory length: 83215   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 450   score: 0.0   memory length: 83338   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 451   score: 0.0   memory length: 83460   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 452   score: 0.0   memory length: 83582   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 453   score: 0.0   memory length: 83704   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 454   score: 1.0   memory length: 83872   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 455   score: 0.0   memory length: 83995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 456   score: 0.0   memory length: 84118   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 457   score: 3.0   memory length: 84367   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 458   score: 2.0   memory length: 84564   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 459   score: 2.0   memory length: 84782   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 460   score: 2.0   memory length: 84980   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 461   score: 0.0   memory length: 85102   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 462   score: 0.0   memory length: 85225   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 463   score: 0.0   memory length: 85348   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 464   score: 5.0   memory length: 85656   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 465   score: 2.0   memory length: 85854   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 466   score: 2.0   memory length: 86052   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 467   score: 1.0   memory length: 86202   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 468   score: 0.0   memory length: 86325   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 469   score: 6.0   memory length: 86698   epsilon: 1.0    steps: 373    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 470   score: 1.0   memory length: 86870   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 471   score: 1.0   memory length: 87038   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 472   score: 2.0   memory length: 87256   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 473   score: 0.0   memory length: 87379   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 474   score: 0.0   memory length: 87502   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 475   score: 2.0   memory length: 87699   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 476   score: 0.0   memory length: 87822   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 477   score: 0.0   memory length: 87944   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 478   score: 0.0   memory length: 88066   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 479   score: 1.0   memory length: 88216   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 480   score: 1.0   memory length: 88367   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 481   score: 2.0   memory length: 88566   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 482   score: 2.0   memory length: 88764   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 483   score: 0.0   memory length: 88887   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 484   score: 4.0   memory length: 89143   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 485   score: 0.0   memory length: 89265   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 486   score: 0.0   memory length: 89387   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 487   score: 1.0   memory length: 89557   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 488   score: 2.0   memory length: 89755   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 489   score: 1.0   memory length: 89924   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 490   score: 1.0   memory length: 90076   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 491   score: 0.0   memory length: 90199   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 492   score: 3.0   memory length: 90431   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 493   score: 2.0   memory length: 90628   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 494   score: 2.0   memory length: 90825   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 495   score: 0.0   memory length: 90948   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 496   score: 1.0   memory length: 91117   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 497   score: 3.0   memory length: 91384   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 498   score: 0.0   memory length: 91507   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 499   score: 0.0   memory length: 91630   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 500   score: 1.0   memory length: 91799   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 501   score: 3.0   memory length: 92064   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 502   score: 3.0   memory length: 92331   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 503   score: 3.0   memory length: 92575   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 504   score: 3.0   memory length: 92822   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 505   score: 1.0   memory length: 92993   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 506   score: 1.0   memory length: 93145   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 507   score: 0.0   memory length: 93268   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 508   score: 2.0   memory length: 93468   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 509   score: 2.0   memory length: 93688   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 510   score: 0.0   memory length: 93810   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 511   score: 1.0   memory length: 93960   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 512   score: 2.0   memory length: 94157   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 513   score: 3.0   memory length: 94402   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 514   score: 0.0   memory length: 94525   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 515   score: 1.0   memory length: 94696   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 516   score: 0.0   memory length: 94819   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 517   score: 0.0   memory length: 94941   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 518   score: 1.0   memory length: 95110   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 519   score: 3.0   memory length: 95336   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 520   score: 1.0   memory length: 95487   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 521   score: 5.0   memory length: 95791   epsilon: 1.0    steps: 304    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 522   score: 0.0   memory length: 95914   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 523   score: 1.0   memory length: 96083   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 524   score: 3.0   memory length: 96328   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 525   score: 0.0   memory length: 96451   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 526   score: 0.0   memory length: 96574   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 527   score: 1.0   memory length: 96725   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 528   score: 3.0   memory length: 96973   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 529   score: 1.0   memory length: 97123   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 530   score: 0.0   memory length: 97246   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 531   score: 0.0   memory length: 97369   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 532   score: 1.0   memory length: 97538   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 533   score: 2.0   memory length: 97736   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 534   score: 2.0   memory length: 97953   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 535   score: 3.0   memory length: 98199   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 536   score: 7.0   memory length: 98612   epsilon: 1.0    steps: 413    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 537   score: 2.0   memory length: 98810   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 538   score: 3.0   memory length: 99036   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 539   score: 3.0   memory length: 99285   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 540   score: 1.0   memory length: 99435   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 541   score: 2.0   memory length: 99655   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 542   score: 3.0   memory length: 99883   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 543   score: 1.0   memory length: 100052   epsilon: 0.9998950600000023    steps: 169    lr: 0.0001     evaluation reward: 1.47\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 5, 512]' is invalid for input of size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     action, hidden \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     27\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/assignment5/agent_lstm.py:39\u001b[0m, in \u001b[0;36mAgent_LSTM.get_action\u001b[0;34m(self, state, hidden)\u001b[0m\n\u001b[1;32m     37\u001b[0m         state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m         Q, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m         a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(Q)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(a), hidden\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/assignment5/model.py:49\u001b[0m, in \u001b[0;36mDQN_LSTM.forward\u001b[0;34m(self, x, hidden, train)\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# We will reshape the output to match the original shape. So first dimension will be extended back to (batch_size, lstm_seq_length)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# input to lstm: 512\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Pass the state through an LSTM\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m### CODE ###\u001b[39;00m\n\u001b[1;32m     52\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_layer(x, hidden)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 5, 512]' is invalid for input of size 512"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HISTORY_SIZE = 1\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    hidden = None\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n",
    "        state = next_state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action + 1)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[1, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "        score += reward\n",
    "        history[:1, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_lstm_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
